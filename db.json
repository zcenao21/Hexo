{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/light/source/js/gallery.js","path":"js/gallery.js","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/light/source/js/jquery.imagesloaded.min.js","path":"js/jquery.imagesloaded.min.js","modified":0,"renderable":1},{"_id":"themes/light/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/light/source/js/jquery-3.4.1.min.js","path":"js/jquery-3.4.1.min.js","modified":0,"renderable":1},{"_id":"themes/light/source/css/font/fontawesome-webfont.eot","path":"css/font/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/light/source/css/font/fontawesome-webfont.woff","path":"css/font/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/light/source/css/font/fontawesome-webfont.ttf","path":"css/font/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/light/source/css/font/fontawesome-webfont.svg","path":"css/font/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0}],"Cache":[{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1582123571055},{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1582123571055},{"_id":"themes/landscape/README.md","hash":"37fae88639ef60d63bd0de22314d7cc4c5d94b07","modified":1582123571055},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1582123571055},{"_id":"themes/landscape/package.json","hash":"544f21a0b2c7034998b36ae94dba6e3e0f39f228","modified":1582123571055},{"_id":"source/_posts/Linux目录.md","hash":"ef2bee05f6aeef532b1e90a99cff220a36bc5c5e","modified":1581439124000},{"_id":"source/_posts/RDD转换.md","hash":"55b8c29e43542b7112c2188d21b3020a67d031d8","modified":1581265249000},{"_id":"source/_posts/hadoop-hadoop安装.md","hash":"816afd002ce28abd0d336adaa2fb03fd6b93b1ee","modified":1582136473883},{"_id":"source/_posts/linux-基础知识.md","hash":"da91f293be717c20ee34cbfced14cbe48df3b3a0","modified":1582173295133},{"_id":"source/_posts/spark.md","hash":"ec3708b8b70b81ab7fb489b64fa5a8a1f2d9b4f2","modified":1581265302000},{"_id":"source/_posts/spark架构.md","hash":"2f61badc97b0d81f9a0202a4d510641feaefc112","modified":1581265282000},{"_id":"source/_posts/spark目录.md","hash":"2419ab643e5c697aafdd5a0523a3a70d8deceba8","modified":1572706482000},{"_id":"themes/landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1582123571055},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1582123571055},{"_id":"themes/landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1582123571055},{"_id":"themes/landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1582123571055},{"_id":"themes/landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1582123571055},{"_id":"themes/landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1582123571055},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1582123571055},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1582123571055},{"_id":"themes/landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1582123571055},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1582123571055},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1582123571055},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1582123571055},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1582123571055},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1582123571055},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1582123571055},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1582123571055},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1582123571055},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1582123571055},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1582123571055},{"_id":"themes/landscape/scripts/fancybox.js","hash":"19a5a140cc3aaa7c1d962d8e9f664ca9b2e2c41d","modified":1582131261927},{"_id":"themes/landscape/_config.yml","hash":"0919fd2eb51d2332c5c01a418ab18c57c4070f8f","modified":1582132044441},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"d0d753d39038284d52b10e5075979cc97db9cd20","modified":1582123571055},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"950ddd91db8718153b329b96dc14439ab8463ba5","modified":1582123571055},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1582123571055},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"c4c835615d96a950d51fa2c3b5d64d0596534fed","modified":1582123571055},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"d1a5e035b08df1625dbd0640f0599e2bc424a9f9","modified":1582131541712},{"_id":"themes/landscape/layout/_partial/gauges-analytics.ejs","hash":"aad6312ac197d6c5aaf2104ac863d7eba46b772a","modified":1582123571055},{"_id":"themes/landscape/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1582123571055},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"5abf77aec957d9445fc71a8310252f0013c84578","modified":1582123571055},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"9e116d2bcec3d92f4db6fdf2a486658a80fc584f","modified":1582131751628},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1582123571055},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1582123571055},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1582123571055},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1582123571055},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1582123571055},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1582123571055},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1582123571055},{"_id":"themes/landscape/source/css/_variables.styl","hash":"628e307579ea46b5928424313993f17b8d729e92","modified":1582123571055},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1582123571055},{"_id":"themes/landscape/source/css/style.styl","hash":"1787c73d74372adab64076e28e6c1086e1b48351","modified":1582130484939},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1582123571059},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1582123571059},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1582123571063},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1582123571063},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1582123571063},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1582123571063},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1582123571063},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1582123571063},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1582123571063},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1582123571063},{"_id":"themes/landscape/source/js/search.js","hash":"ab474680562eb8ca55faa72efc8cd02ab06889ec","modified":1582131118176},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1582123571055},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1582123571055},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1582123571055},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1582123571055},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1582123571055},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1582123571055},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1582123571055},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1582123571055},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1582123571055},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1582123571055},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1582123571055},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1582123571055},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"7e832d1cbd42ed002e8097f3bb0b4122932d4588","modified":1582125729863},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1582123571055},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1582123571055},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1582123571055},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1582123571055},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1582123571055},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1582123571059},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1582123571059},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1582123571063},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1582123571063},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1582123571063},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1582123571063},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1582123571063},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1582123571063},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1582123571059},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1582123571059},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1582123571059},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1582123571059},{"_id":"themes/light/.gitignore","hash":"eaa3d84cb77d92a21b111fd1e37f53edc1ff9de0","modified":1582133550455},{"_id":"themes/light/LICENSE","hash":"c6f301bc722f0af3a55267a36c1c147aeddc6e46","modified":1582133550455},{"_id":"themes/light/README.md","hash":"42cce00e360ae6c8bd1019776ff3b0b974128fa1","modified":1582133550455},{"_id":"themes/light/_config.yml","hash":"ed57a520521f0ba12b1b43167a472e114e1ae649","modified":1582175177031},{"_id":"themes/light/layout/category.ejs","hash":"9b740fc33f6f028df60f0bc4312bf3ebd03aa8ea","modified":1582133550459},{"_id":"themes/light/layout/archive.ejs","hash":"a18842e3d719fe3ca9b977a6995f8facc75c8673","modified":1582133550455},{"_id":"themes/light/layout/index.ejs","hash":"e569d8fe0741a24efb89e44781f9e616da17e036","modified":1582133550459},{"_id":"themes/light/layout/layout.ejs","hash":"1b4ee853dcd80892ba971954641c0e283ddb2e6e","modified":1582133550459},{"_id":"themes/light/layout/page.ejs","hash":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1582133550459},{"_id":"themes/light/layout/post.ejs","hash":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1582133550459},{"_id":"themes/light/layout/tag.ejs","hash":"45150a2365768b6b67880193c9264ad2bb4814db","modified":1582133550459},{"_id":"themes/light/languages/de.yml","hash":"e076c7f2eb29ebcfb04d94861bf3063c4b08078c","modified":1582133550455},{"_id":"themes/light/languages/default.yml","hash":"fd7397be7789b43c1c163ab4faf106318811c2a8","modified":1582133550455},{"_id":"themes/light/languages/es.yml","hash":"de273af604b27812cfd4195e7b7f28ceff2734b3","modified":1582133550455},{"_id":"themes/light/languages/ja.yml","hash":"1511143393fb86819a9d8685ee81c3bbf1e10b23","modified":1582133550455},{"_id":"themes/light/languages/lt.yml","hash":"8826ef5b3911e094f8a118d8db981532d0919bb6","modified":1582133550455},{"_id":"themes/light/languages/no.yml","hash":"bf11017d77f64fbafb9c99ac219d076b20d53afc","modified":1582133550455},{"_id":"themes/light/languages/pl.yml","hash":"3f36d08e84a85651bf777cec0752193057c08430","modified":1582133550455},{"_id":"themes/light/languages/ru.yml","hash":"35aadf8fdd28aaff8a1c8f50e80201dcf8ce0604","modified":1582133550455},{"_id":"themes/light/languages/zh-CN.yml","hash":"ca0118e9081b54cc0fca8596660bd6acf4c0308f","modified":1582133550455},{"_id":"themes/light/languages/zh-TW.yml","hash":"6141b4c7a094c74bd9df7c08908d92b561c1a0c0","modified":1582133550455},{"_id":"themes/light/layout/_partial/after_footer.ejs","hash":"ac0876ae4ec09e2f16969fea196adc66a29ea3b3","modified":1582133550455},{"_id":"themes/light/layout/_partial/archive.ejs","hash":"7e4f7c2909b1b90241424ea2ff8e7b4761d8360f","modified":1582133550455},{"_id":"themes/light/layout/_partial/article.ejs","hash":"b4479954cdaa33420e06ffce6ea8e05f48da2928","modified":1582133550455},{"_id":"themes/light/layout/_partial/facebook_comment.ejs","hash":"3fdc1d0ce9177825e7417635fbc545a35d528d04","modified":1582133550455},{"_id":"themes/light/layout/_partial/comment.ejs","hash":"9dcd65da6bf61b143ffdcd154fb5431b4ecc10fa","modified":1582175839826},{"_id":"themes/light/layout/_partial/footer.ejs","hash":"f1ddfeb508eedea3a30875e42a2024e5c1d75105","modified":1582163361798},{"_id":"themes/light/layout/_partial/google_analytics.ejs","hash":"7cf0d1f93051bda510d49dab7f684b9d7c6ba58f","modified":1582133550455},{"_id":"themes/light/layout/_partial/head.ejs","hash":"bdb65794b84b8242f0e4711059a2737f317587e9","modified":1582133550455},{"_id":"themes/light/layout/_partial/header.ejs","hash":"224ea7f0fccc29418583a5c59497a8ece0073301","modified":1582133550455},{"_id":"themes/light/layout/_partial/pagination.ejs","hash":"1206b630a07444e8744365f14ddb26095c925ae1","modified":1582133550455},{"_id":"themes/light/layout/_widget/search.ejs","hash":"f3600ade452f70bd49cb8cf53f3b6e59015f2ce3","modified":1582133550455},{"_id":"themes/light/layout/_partial/sidebar.ejs","hash":"08672eb8a781356ae55124b03d0cc6a0c650f728","modified":1582162765732},{"_id":"themes/light/layout/_widget/recent_posts.ejs","hash":"f17d2cb69034acabea4df54f301f80812e7b84a8","modified":1582133550455},{"_id":"themes/light/layout/_widget/category.ejs","hash":"8a2b90dc29661371f060f710668929c3588e15e4","modified":1582133550455},{"_id":"themes/light/layout/_widget/tag.ejs","hash":"1914db78bea49c333067d79fe7ad9567d2b08d00","modified":1582133550455},{"_id":"themes/light/layout/_widget/tagcloud.ejs","hash":"a236c86481196ae43206e056ba78cec14f1ac014","modified":1582133550455},{"_id":"themes/light/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1582133550459},{"_id":"themes/light/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1582133550459},{"_id":"themes/light/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1582133550459},{"_id":"themes/light/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1582133550459},{"_id":"themes/light/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1582133550459},{"_id":"themes/light/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1582133550459},{"_id":"themes/light/source/js/gallery.js","hash":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1582133550459},{"_id":"themes/light/source/fancybox/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1582133550459},{"_id":"themes/light/source/fancybox/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1582133550459},{"_id":"themes/light/source/js/jquery.imagesloaded.min.js","hash":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1582133550459},{"_id":"themes/light/source/css/style.styl","hash":"c03b2520e4a85b981e29516cadc0a365e6500e3d","modified":1582133550459},{"_id":"themes/light/source/js/jquery-3.4.1.min.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1582133550459},{"_id":"themes/light/layout/_partial/post/category.ejs","hash":"be740939c5c2d4ffdbed9557b4e63a590058b476","modified":1582133550455},{"_id":"themes/light/layout/_partial/post/gallery.ejs","hash":"fafc2501d7e65983b0f5c2b58151ca12e57c0574","modified":1582133550455},{"_id":"themes/light/layout/_partial/post/share.ejs","hash":"24c04b319f1b19e887c42db961b90a7e0ab26fdc","modified":1582133550455},{"_id":"themes/light/layout/_partial/post/title.ejs","hash":"5c9ccaf7fc87d2f8a5af764da962760e11902bde","modified":1582162162114},{"_id":"themes/light/layout/_partial/post/tag.ejs","hash":"095418df66a27a28cbab16d7cb0d16001b0e23f1","modified":1582133550455},{"_id":"themes/light/source/css/_base/utils.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1582133550459},{"_id":"themes/light/source/css/font/fontawesome-webfont.eot","hash":"d775f599ff3f23be082e6a9604b4898718923a37","modified":1582133550459},{"_id":"themes/light/source/css/font/fontawesome-webfont.woff","hash":"0612cddf2f835cceffccc88fd194f97367d0b024","modified":1582133550459},{"_id":"themes/light/source/css/_partial/archive.styl","hash":"072e9b8c5ee9acf95ac7cce9c34706d41e412229","modified":1582133550459},{"_id":"themes/light/source/css/_partial/article.styl","hash":"c40dea0a6035628dd299cee299d8a6d2abf20d8b","modified":1582133550459},{"_id":"themes/light/source/css/_partial/comment.styl","hash":"e7f8c085bfa8c26afc4b2fbc9f2092f4f07aef34","modified":1582133550459},{"_id":"themes/light/source/css/_partial/footer.styl","hash":"1757872dbdbd09295a625f13e356aa798a8bb308","modified":1582133550459},{"_id":"themes/light/source/css/_partial/header.styl","hash":"50d36fe0c803cbba69dd57493611466e4d72156e","modified":1582133550459},{"_id":"themes/light/source/css/_partial/index.styl","hash":"7a8c0ec6ab99a9f8e00c9687aca29d31752424a2","modified":1582133550459},{"_id":"themes/light/source/css/_partial/sidebar.styl","hash":"a8bf5237d7d2fba66988cfb85a3ae218be8709ae","modified":1582133550459},{"_id":"themes/light/source/css/_partial/syntax.styl","hash":"400335f01229ed02e62110ba90312adb78b84ff5","modified":1582133550459},{"_id":"themes/light/source/css/_base/layout.styl","hash":"1b58c21aa48a8f9f7f811af681ac182dd058e23d","modified":1582133550459},{"_id":"themes/light/source/css/_base/variable.styl","hash":"6f3ad13e49634dae8cd992bbd598f5ff0b39a816","modified":1582133550459},{"_id":"themes/light/source/css/font/fontawesome-webfont.ttf","hash":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c","modified":1582133550459},{"_id":"themes/light/source/css/font/fontawesome-webfont.svg","hash":"d162419c91b8bab3a4fd327c933a0fcf3799c251","modified":1582133550459},{"_id":"public/search.xml","hash":"62595cbac7ab57b951a4e92d417cecce422123a6","modified":1582548116825},{"_id":"public/2020/02/12/linux-基础知识/index.html","hash":"1f1d8ae95de335a97f0cbc8d4c087b61860e14db","modified":1582547092281},{"_id":"public/2020/02/09/Linux目录/index.html","hash":"1879a14cd531702ee643439029b3e7742a1839cf","modified":1582547092281},{"_id":"public/2019/11/02/RDD转换/index.html","hash":"e9f9393e1050972c961691f9094ee64e8da362a8","modified":1582547092281},{"_id":"public/2019/11/02/spark目录/index.html","hash":"56348ca3e025b06925b7d131c557f94d5c5ec68d","modified":1582547092281},{"_id":"public/2019/11/02/spark架构/index.html","hash":"3da616ba93a894b2f43e6182baea4ceee9034e3f","modified":1582547092281},{"_id":"public/2019/10/24/spark/index.html","hash":"0e184128d5a0752acdc708fbddb6f57f8c12b7fd","modified":1582547092281},{"_id":"public/archives/index.html","hash":"c47d66a7681a717500539fd4456c775f7ef71f05","modified":1582548044051},{"_id":"public/archives/page/2/index.html","hash":"96f4544aab9b0e19cbe04501ef52072273a0fcc5","modified":1582548044051},{"_id":"public/archives/2019/index.html","hash":"a6d25405b77c3ea52ceceb62a79ae9f649a6a987","modified":1582548044051},{"_id":"public/archives/2019/10/index.html","hash":"66a0957b02ef5c5c8aac04b9709230ecc698aa96","modified":1582548044051},{"_id":"public/archives/2019/11/index.html","hash":"b8cfd51fa5f0c16140735795599c37d1f0bc4671","modified":1582548044051},{"_id":"public/archives/2020/index.html","hash":"00447bf1421c1ef8c1b462b40b7c5e043f59dc91","modified":1582548044051},{"_id":"public/archives/2020/02/index.html","hash":"e84540d63d2b9a0aefbc24637fc0a6b97a443e20","modified":1582548044051},{"_id":"public/categories/Spark/index.html","hash":"cc3ccc9997da300adb2bad295df3be2c17abee42","modified":1582548044051},{"_id":"public/categories/hadoop/index.html","hash":"0633f94dbf23597a8a98ffba1a2363738d80e3b0","modified":1582548044051},{"_id":"public/categories/目录/index.html","hash":"0ab8eebc0f151ff2ea3992df1f7f18130c860347","modified":1582548044051},{"_id":"public/categories/Linux/index.html","hash":"43a857c3d8a0b9be59463f48890bc5722831c78d","modified":1582548044051},{"_id":"public/index.html","hash":"fe218d8cbb166b06fa16ba171ae855e0fa668482","modified":1582548116825},{"_id":"public/page/2/index.html","hash":"684221177a904857403d74ca2e823fd3d1fc75ab","modified":1582548044051},{"_id":"public/tags/Spark/index.html","hash":"4e6caae186680bd7b4ed7cad9c89098f1192655e","modified":1582548044051},{"_id":"public/tags/High-Performance-Spark/index.html","hash":"7b47bdbc4c4a9ae72f53793378e137c381002c4a","modified":1582548044051},{"_id":"public/tags/RDD/index.html","hash":"150a4bc70fe9e233ac08e77c06047b33f2e14e71","modified":1582548044051},{"_id":"public/tags/hadoop/index.html","hash":"2e4a98f283b6d2bc506a0f35d66df9e7eb490d73","modified":1582548044051},{"_id":"public/tags/hadoop安装/index.html","hash":"c66af730325f59925f751e6f4c1e731537d2d618","modified":1582548044051},{"_id":"public/tags/windows-10/index.html","hash":"73bd5caf6e46706bee63daf89a145738588cc19f","modified":1582548044051},{"_id":"public/tags/linux目录/index.html","hash":"9f9e4149f546c7ffc35557d33875f6dd678fba90","modified":1582548044051},{"_id":"public/tags/Linux/index.html","hash":"eaf2ee972c2d2ca48af2a59010f2b126bb915df8","modified":1582548044051},{"_id":"public/tags/基础知识/index.html","hash":"99b28f13ce9916495f805e5a34d2137f68cb5184","modified":1582548044051},{"_id":"public/tags/大数据工具/index.html","hash":"492ff5155d7c3ac64ad6b855b74ba63ae869fc09","modified":1582548044051},{"_id":"public/tags/spark架构/index.html","hash":"85448ef6eadcf57b0088fb50e099ae1c0bdc0a6a","modified":1582548044051},{"_id":"public/tags/spark目录/index.html","hash":"b4e1a33c3eaaf2d1204e2f1fa09659fd93b8d118","modified":1582548044051},{"_id":"public/2020/02/15/hadoop-hadoop安装/index.html","hash":"cfb5a2731b1efa17d65073d77bb9ebae0b7fb812","modified":1582547092281},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1582133577967},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1582133577967},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1582133577967},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1582133577967},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1582133577967},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1582133577967},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1582133577967},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1582133577967},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1582133577967},{"_id":"public/css/font/fontawesome-webfont.eot","hash":"d775f599ff3f23be082e6a9604b4898718923a37","modified":1582133577967},{"_id":"public/css/font/fontawesome-webfont.woff","hash":"0612cddf2f835cceffccc88fd194f97367d0b024","modified":1582133577967},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1582133577967},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1582133577967},{"_id":"public/css/font/fontawesome-webfont.ttf","hash":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c","modified":1582133577967},{"_id":"public/css/style.css","hash":"d9cc77a18d5014e515cfc3debac6aa8c484a1e85","modified":1582133577967},{"_id":"public/fancybox/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1582133577967},{"_id":"public/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1582133577967},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1582133577967},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1582133577967},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1582133577967},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1582133577967},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1582133577967},{"_id":"public/js/gallery.js","hash":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1582133577967},{"_id":"public/js/jquery.imagesloaded.min.js","hash":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1582133577967},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1582133577967},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1582133577967},{"_id":"public/js/search.js","hash":"ab474680562eb8ca55faa72efc8cd02ab06889ec","modified":1582133577967},{"_id":"public/js/jquery-3.4.1.min.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1582133577967},{"_id":"public/css/font/fontawesome-webfont.svg","hash":"d162419c91b8bab3a4fd327c933a0fcf3799c251","modified":1582133577967},{"_id":"public/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1582133577967},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1582133577967},{"_id":"source/_posts/hive-source-modification.md","hash":"cbcf0b334a29c87ff0369d3ed85e555488046ad2","modified":1582280341555},{"_id":"public/2020/02/21/hive-source-modification/index.html","hash":"2548675b059b4ecffbdd93c820764d978a329853","modified":1582547092281},{"_id":"public/categories/Hive/index.html","hash":"31dea91395402549bb8ca55a5a55bbac62b1aad5","modified":1582548044051},{"_id":"public/tags/hive/index.html","hash":"6d8ac85ed778b8ac6484e20e2ca54c8b83ba7df9","modified":1582548044051},{"_id":"public/tags/源码/index.html","hash":"dbf354e040de495f0f1b1301a14751ff1368c38f","modified":1582548044051},{"_id":"public/tags/开发/index.html","hash":"d2a7a1204b8e5df6c6acc5df8d88ecd75066cef0","modified":1582548044051},{"_id":"source/CNAME","hash":"9e50bb7377d3775cf2cac9f84f26e5baa4aedca4","modified":1582543112592},{"_id":"source/_posts/hive-udf.md","hash":"a4e0427b715f4666dc9a8bc7007e5b12ad62d77b","modified":1582435767440},{"_id":"public/2020/02/21/hive-udf/index.html","hash":"939c83b45e2adecf3a9931b6aa4e994f992d8bdb","modified":1582547092281},{"_id":"public/tags/udf/index.html","hash":"cd39324fc599e3ecc42559b4fd301159ab89e643","modified":1582548044051},{"_id":"public/tags/udaf/index.html","hash":"93de073e698786949137847c2424193cc5746e30","modified":1582548044051},{"_id":"public/CNAME","hash":"9e50bb7377d3775cf2cac9f84f26e5baa4aedca4","modified":1582543115900},{"_id":"source/_posts/github个人网站更改域名.md","hash":"b4e06d16cc2e7aea67a34cf3c4e5f3237faeb3b3","modified":1582547693103},{"_id":"public/2020/02/15/github个人网站更改域名/index.html","hash":"f43e88a1ca25a87ff03de329b483a27acafdc473","modified":1582547092281},{"_id":"public/archives/2020/page/2/index.html","hash":"1c027d9a3158aaadbb631abd3ceeca8207208cca","modified":1582548044051},{"_id":"public/archives/2020/02/page/2/index.html","hash":"afaa7ab51f7c7810060069d5610b846fa4af90a0","modified":1582548044051},{"_id":"public/categories/other/index.html","hash":"7a62284e49e349236c73a2e6a00e7c4537dca9f6","modified":1582548044051},{"_id":"public/tags/github个人网站/index.html","hash":"25e1fd2c521f949a2f80617a3b527c37d20f875d","modified":1582548044051},{"_id":"public/tags/域名修改/index.html","hash":"599d57561602c87de34331b6c335cbba1da67ba6","modified":1582548044051},{"_id":"public/2020/02/24/github个人网站更改域名/index.html","hash":"c60a04f7e5f0989e2c133192e8dda79bdd6d6f4e","modified":1582548044051},{"_id":"source/_posts/hive/hive-source-modification.md","hash":"cbcf0b334a29c87ff0369d3ed85e555488046ad2","modified":1582280341555},{"_id":"source/_posts/hive/hive-udf.md","hash":"a4e0427b715f4666dc9a8bc7007e5b12ad62d77b","modified":1582435767440},{"_id":"public/2020/02/21/hive/hive-source-modification/index.html","hash":"bd683327ab18c3fab12decf90793d26062a71bcd","modified":1582548044051},{"_id":"public/2020/02/21/hive/hive-udf/index.html","hash":"2254418bc317618ff35a460b76477a1fa412bfc9","modified":1582548044051},{"_id":"source/_posts/hadoop/hadoop-hadoop安装.md","hash":"816afd002ce28abd0d336adaa2fb03fd6b93b1ee","modified":1582136473883},{"_id":"source/_posts/linux/Linux目录.md","hash":"19183269d620874f1652f2d96f18f8bb1474707d","modified":1582548038851},{"_id":"source/_posts/linux/linux-基础知识.md","hash":"da91f293be717c20ee34cbfced14cbe48df3b3a0","modified":1582173295133},{"_id":"source/_posts/other/github个人网站更改域名.md","hash":"b4e06d16cc2e7aea67a34cf3c4e5f3237faeb3b3","modified":1582547693103},{"_id":"source/_posts/spark/RDD转换.md","hash":"55b8c29e43542b7112c2188d21b3020a67d031d8","modified":1582135256045},{"_id":"source/_posts/spark/spark架构.md","hash":"2f61badc97b0d81f9a0202a4d510641feaefc112","modified":1582135256045},{"_id":"source/_posts/spark/spark.md","hash":"ec3708b8b70b81ab7fb489b64fa5a8a1f2d9b4f2","modified":1582135256045},{"_id":"source/_posts/spark/spark目录.md","hash":"4c412c768392e9cfc39702af6d6492ffa0806192","modified":1582548100095},{"_id":"public/2020/02/24/other/github个人网站更改域名/index.html","hash":"ddb39c03643f6514bcc36c4eb28ff8b6a8509935","modified":1582548044051},{"_id":"public/2020/02/12/linux/linux-基础知识/index.html","hash":"f83b24b776f797fec5399422ae1564792273a32d","modified":1582548044051},{"_id":"public/2020/02/09/linux/Linux目录/index.html","hash":"15f729091e58a0ad3dc4d6754ca1a82f88ce2525","modified":1582548044051},{"_id":"public/2019/11/02/spark/RDD转换/index.html","hash":"2c1aa42228450a5e18994cc29e1af2556c151da7","modified":1582548044051},{"_id":"public/2019/11/02/spark/spark目录/index.html","hash":"78ad05ca28ccae6765a534be7abffdc5a206c3cf","modified":1582548116825},{"_id":"public/2019/11/02/spark/spark架构/index.html","hash":"28a976154a853e916f198c2420b2ae2d81fd0321","modified":1582548044051},{"_id":"public/2019/10/24/spark/spark/index.html","hash":"673e29c8d397cc374e494a486cd4ecf8730edfbf","modified":1582548044051},{"_id":"public/2020/02/15/hadoop/hadoop-hadoop安装/index.html","hash":"febde9ed6586a0d4c265eba0c357c5f6cae60996","modified":1582548044051},{"_id":"public/archives/page/3/index.html","hash":"25359a9bddc0f5044b26c03537dd0630a3c309a3","modified":1582548044051},{"_id":"public/page/3/index.html","hash":"8714d1090c28d949f14985e343c6fc3ab20db79d","modified":1582548044051}],"Category":[{"name":"Spark","_id":"ck6tkirfv000291zkegcu7gf8"},{"name":"hadoop","_id":"ck6tkirg0000791zkcru027tj"},{"name":"目录","_id":"ck6tkirg4000c91zk95undmh3"},{"name":"Linux","_id":"ck6tkirg6000g91zk9rlscwa5"},{"name":"Hive","_id":"ck6vpalj60001auzk3ywm64b7"},{"name":"other","_id":"ck70fofqw0001plzk4u30dzsf"}],"Data":[],"Page":[],"Post":[{"title":"hive源码调试入门","date":"2020-02-21T04:45:21.000Z","_content":"\n\n\n在windows上折腾安装好了hadoop，hive因为文件路径太长不支持等各种奇奇怪怪的问题始终运行不起来，下了大决心放弃windows系统，unbuntu走起！重装系统还算顺利，几个小时搞定。然后就是ubuntu上运行hadoop,成功; ubuntu上运行hive，成功！下一步hive源码走起。<!--more-->\n\nhive依赖hadoop，看hive启动脚本，最终是调用hadoop启动，而hadoop最终还是执行的java -jar xxx.jar形式，所以决定先从hive java源码入口看。先试一试改动源码打印个信息。\n\n首先在hive的main函数入口增加一行\n\n```\nSystem.out.println(\"Will's first hive source code modification: test err print info\");\n```\n\n<img src=\"https://i.loli.net/2020/02/21/bQOyidqXgsMR3Ca.png\" alt=\"启动\" style=\"zoom:80%;\" />\n\n打包，然后替换lib目录下hive-cli-xxx.jar。\n\n运行hive\n\n![选区_001.png](https://i.loli.net/2020/02/21/LZ16gVt4rfdXqyu.png)\n\n\n\n好啦，正式开启与hive源码的斗争！\n\n","source":"_posts/hive/hive-source-modification.md","raw":"---\ntitle: hive源码调试入门\ndate: 2020-02-21 12:45:21\ntags:\n    - hive\n    - 源码\n    - 开发\ncategories: \n    - Hive\n---\n\n\n\n在windows上折腾安装好了hadoop，hive因为文件路径太长不支持等各种奇奇怪怪的问题始终运行不起来，下了大决心放弃windows系统，unbuntu走起！重装系统还算顺利，几个小时搞定。然后就是ubuntu上运行hadoop,成功; ubuntu上运行hive，成功！下一步hive源码走起。<!--more-->\n\nhive依赖hadoop，看hive启动脚本，最终是调用hadoop启动，而hadoop最终还是执行的java -jar xxx.jar形式，所以决定先从hive java源码入口看。先试一试改动源码打印个信息。\n\n首先在hive的main函数入口增加一行\n\n```\nSystem.out.println(\"Will's first hive source code modification: test err print info\");\n```\n\n<img src=\"https://i.loli.net/2020/02/21/bQOyidqXgsMR3Ca.png\" alt=\"启动\" style=\"zoom:80%;\" />\n\n打包，然后替换lib目录下hive-cli-xxx.jar。\n\n运行hive\n\n![选区_001.png](https://i.loli.net/2020/02/21/LZ16gVt4rfdXqyu.png)\n\n\n\n好啦，正式开启与hive源码的斗争！\n\n","slug":"hive/hive-source-modification","published":1,"updated":"2020-02-21T10:19:01.555Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck70g2rlw00003ozk8v1i3nrh","content":"<p>在windows上折腾安装好了hadoop，hive因为文件路径太长不支持等各种奇奇怪怪的问题始终运行不起来，下了大决心放弃windows系统，unbuntu走起！重装系统还算顺利，几个小时搞定。然后就是ubuntu上运行hadoop,成功; ubuntu上运行hive，成功！下一步hive源码走起。<a id=\"more\"></a></p>\n<p>hive依赖hadoop，看hive启动脚本，最终是调用hadoop启动，而hadoop最终还是执行的java -jar xxx.jar形式，所以决定先从hive java源码入口看。先试一试改动源码打印个信息。</p>\n<p>首先在hive的main函数入口增加一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">System.out.println(&quot;Will&#39;s first hive source code modification: test err print info&quot;);</span><br></pre></td></tr></table></figure>\n\n<img src=\"https://i.loli.net/2020/02/21/bQOyidqXgsMR3Ca.png\" alt=\"启动\" style=\"zoom:80%;\" />\n\n<p>打包，然后替换lib目录下hive-cli-xxx.jar。</p>\n<p>运行hive</p>\n<p><img src=\"https://i.loli.net/2020/02/21/LZ16gVt4rfdXqyu.png\" alt=\"选区_001.png\"></p>\n<p>好啦，正式开启与hive源码的斗争！</p>\n","site":{"data":{}},"excerpt":"<p>在windows上折腾安装好了hadoop，hive因为文件路径太长不支持等各种奇奇怪怪的问题始终运行不起来，下了大决心放弃windows系统，unbuntu走起！重装系统还算顺利，几个小时搞定。然后就是ubuntu上运行hadoop,成功; ubuntu上运行hive，成功！下一步hive源码走起。","more":"</p>\n<p>hive依赖hadoop，看hive启动脚本，最终是调用hadoop启动，而hadoop最终还是执行的java -jar xxx.jar形式，所以决定先从hive java源码入口看。先试一试改动源码打印个信息。</p>\n<p>首先在hive的main函数入口增加一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">System.out.println(&quot;Will&#39;s first hive source code modification: test err print info&quot;);</span><br></pre></td></tr></table></figure>\n\n<img src=\"https://i.loli.net/2020/02/21/bQOyidqXgsMR3Ca.png\" alt=\"启动\" style=\"zoom:80%;\" />\n\n<p>打包，然后替换lib目录下hive-cli-xxx.jar。</p>\n<p>运行hive</p>\n<p><img src=\"https://i.loli.net/2020/02/21/LZ16gVt4rfdXqyu.png\" alt=\"选区_001.png\"></p>\n<p>好啦，正式开启与hive源码的斗争！</p>"},{"title":"hive udf&udaf","date":"2020-02-21T10:20:21.000Z","_content":"\n\n\nhive为我们定义了很多函数，大多数情况下是能够满足我们的需求的。但是在有些情况下，很有必要自己定义一些函数，这样使用起来非常方便。这就是自定义函数(udf)和自定义聚合函数(udaf,user defined aggregation function)。udf是每行返回一个结果，而udaf则是聚合的结果，多行产生一个结果。另外还有udtf，是一行产生多个结果。目前我还没有用到过udtf，就先不介绍它了。下面分别用两个示例介绍udf和udaf<!--more-->\n\n\n\n# udf\n\n### 功能\n\n查找array中是否包含被查询值\n\n\n\n### 步骤\n\n- 测试数据准备\n\n  ```\n  zhangsan        beijing,shanghai,tianjin,hangzhou\n  lisi    changchu,chengdu,wuhan\n  ```\n\n- hive建表与导入\n\n  ```\n  Create table users(name string, worklocations array<string> ) row format delimited fields terminated by '\\t' collection items terminated by ','; \n  \n  load data local inpath '/root/person.txt ' OVERWRITE INTO TABLE users; \n  ```\n\n- udf包生成与导入\n\n  ```\n  package com.will;\n  \n  import org.apache.hadoop.hive.ql.exec.UDF;\n  import java.util.ArrayList;\n  \n  public class FindInArray extends UDF {\n      public ArrayList<String> evaluate(String keywords, ArrayList<String> column){\n          //参数类型使用arraylist<String>对应hive中的array<string>,而不是String[]\n          if(column.contains(keywords)){\n              return column;\n          }else{\n              return null;\n          }\n      }\n      \n      public String evaluate(String keywords,ArrayList<String> column,String name){\n          //重载evaluate，另一种查询方式，返回name值\n          if(column.contains(keywords)){\n              return name;\n          }else{\n              return null;\n          }\n      }\n  }\n  ```\n\n  使用mvn 打包\n\n  ```\n  mvn clean package\n  ```\n\n- 导入hive\n\n  ```\n  add jar /home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar;\n  create temporary function find_in_array as 'com.will.FindInArray';\n  ```\n\n- 使用\n\n  ```\n  hive> select find_in_array('beijing',worklocations) from users;\n  OK\n  [\"beijing\",\"shanghai\",\"tianjin\",\"hangzhou\"]\n  NULL\n  Time taken: 0.424 seconds, Fetched: 2 row(s)\n  ```\n\n> 参考：https://blog.csdn.net/Nougats/article/details/71158318\n\n\n\n# udaf\n\n> 参考： \n>\n> https://blog.51cto.com/xiaolanlan/2397771\n>\n> https://www.cnblogs.com/Rudd/p/5137612.html\n\n\n\n### 基础知识\n\nhive的udtf有两种，Simple和Generic。这是由于历史问题，Simple先出现，但是因为反射问题效率较低，所以现在推荐使用Generic的写法。\n\n- Simple。即继承`org.apache.hadoop.hive.ql.exec.UDAF`类，并在派生类中以静态内部类的方式实现`org.apache.hadoop.hive.ql.exec.UDAFEvaluator`接口。在Hive源码包`org.apache.hadoop.hive.contrib.udaf.example`中包含几个示例。可以直接参阅。但是这些接口已经被注解为Deprecated，建议不要使用这种方式开发新的UDAF函数。\n- Generic。这是Hive社区推荐的新的写法，以抽象类代替原有的接口。新的抽象类`org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver`替代老的UDAF接口，新的抽象类`org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator`替代老的UDAFEvaluator接口。\n\n\n\nhive是立足于hadoop之上，也就是hive基于mapreduce，hive sql最终还是会转化为mapreduce执行。为了实现mapreduce，udaf中用model来表示mapreduce各个阶段。\n\n```\npublic static enum Mode {\n        PARTIAL1,\n        PARTIAL2,\n        FINAL,\n        COMPLETE;\n\n        private Mode() {}\n    }\n```\n\n- PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合，将会调用**iterate()**和**terminatePartial() **\n-  PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合，将会调用**merge()** 和 **terminatePartial()** \n- FINAL: mapreduce的reduce阶段，从部分数据的聚合到完全聚合，将会调用**merge()**和**terminate() **\n\n---\n\n- COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了，从原始数据直接到完全聚合，将会调用 **iterate()**和**terminate()**\n\n ![image.png](https://i.loli.net/2020/02/23/DlNqFRHnZ6zLuoA.png)\n\n![image.png](https://i.loli.net/2020/02/23/IKn3Vv57jPScxs8.png)\n\n\n\nudaf骨架示例：\n\n```\npublic class GenericUDAFHistogramNumeric extends AbstractGenericUDAFResolver {\n  static final Log LOG = LogFactory.getLog(GenericUDAFHistogramNumeric.class.getName());\n \n  @Override\n  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException {\n    // 这里主要做类型检查\n \n    return new GenericUDAFHistogramNumericEvaluator();\n  }\n \n  public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator {\n         // 确定各个阶段输入输出参数的数据格式ObjectInspectors\n         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException{\n             return  null;\n         }\n\n         // 保存数据聚集结果的类\n         public AggregationBuffer getNewAggregationBuffer() throws HiveException {\n             return null;\n         }\n\n\t\t // 重置聚集结果\n         public void reset(AggregationBuffer aggregationBuffer) throws HiveException {}\n\n         // map阶段，迭代处理输入sql传过来的列数据 \n         public void iterate(AggregationBuffer aggregationBuffer, Object[] objects) throws HiveException {}\n\n         // map与combiner结束返回结果，得到部分数据聚集结果\n         public Object terminatePartial(AggregationBuffer aggregationBuffer) throws HiveException {\n             return null;\n         }\n\n         // combiner合并map返回的结果，还有reducer合并mapper或combiner返回的结果。\n         public void merge(AggregationBuffer aggregationBuffer, Object o) throws HiveException {}\n\n\t\t // reducer阶段，输出最终结果 \n         public Object terminate(AggregationBuffer aggregationBuffer) throws HiveException {\n             return null;\n         }\n  }\n}\n```\n\n\n\n\n\n### 功能\n\n统计字符数\n\n\n\n### 代码\n\n```\npackage com.will;\n\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.parse.SemanticException;\nimport org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;\nimport org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\nimport org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\nimport org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\nimport org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n\npublic class TotalNumOfLetttersGenericUDAF extends AbstractGenericUDAFResolver {\n    @Override\n    public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException {\n\n        if (parameters.length != 1) {\n            throw new UDFArgumentTypeException(parameters.length - 1,\"Exactly one argument is expected.\");\n        }\n\n        ObjectInspector oi = TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(parameters[0]);\n\n        if (oi.getCategory() != ObjectInspector.Category.PRIMITIVE){\n            throw new UDFArgumentTypeException(0,\n                    \"Argument must be PRIMITIVE, but \"\n                    + oi.getCategory().name()\n                    + \" was passed.\");\n        }\n\n        PrimitiveObjectInspector inputOI = (PrimitiveObjectInspector) oi;\n        if (inputOI.getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.STRING){\n            throw new UDFArgumentTypeException(0, \"Argument must be String, but \"\n                     + inputOI.getPrimitiveCategory().name()\n                     + \" was passed.\");\n        }\n\n        return new TotalNumOfLettersEvaluator();\n    }\n\n    public static class TotalNumOfLettersEvaluator extends GenericUDAFEvaluator{\n        PrimitiveObjectInspector inputOI;\n        ObjectInspector outputOI;\n        PrimitiveObjectInspector integerOI;\n\n        int total = 0;\n        private boolean warned = false;\n\n        @Override\n         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException{\n            assert (parameters.length == 1);\n            super.init(m, parameters);\n\n            //map阶段读取sql列，输入为String基础数据格式\n            if (m == Mode.PARTIAL1 || m == Mode.COMPLETE) {\n                inputOI = (PrimitiveObjectInspector) parameters[0];\n            } else {\n                //其余阶段，输入为Integer基础数据格式\n                integerOI = (PrimitiveObjectInspector) parameters[0];\n            }\n\n            // 指定各个阶段输出数据格式都为Integer类型\n            outputOI = ObjectInspectorFactory.getReflectionObjectInspector(Integer.class,\n                                        ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n\n            return outputOI;\n        }\n\n        //存储当前字符总数的类\n        static class LetterSumAgg implements AggregationBuffer {\n            int sum = 0;\n            void add(int num){\n                sum += num;\n            }\n        }\n\n         public AggregationBuffer getNewAggregationBuffer() throws HiveException {\n             LetterSumAgg result = new LetterSumAgg();\n             return result;\n         }\n\n         public void reset(AggregationBuffer aggregationBuffer) throws HiveException {\n             LetterSumAgg myagg = new LetterSumAgg();\n         }\n\n         public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {\n             assert (parameters.length == 1);\n             if (parameters[0] != null) {\n                 LetterSumAgg myagg = (LetterSumAgg) agg;\n                 Object p1 = ((PrimitiveObjectInspector) inputOI).getPrimitiveJavaObject(parameters[0]);\n                 myagg.add(String.valueOf(p1).length());\n             }\n         }\n\n         public Object terminatePartial(AggregationBuffer agg) throws HiveException {\n             LetterSumAgg myagg = (LetterSumAgg) agg;\n             total += myagg.sum;\n             return total;\n         }\n\n         public void merge(AggregationBuffer agg, Object partial) throws HiveException {\n             if (partial != null) {\n                 LetterSumAgg myagg1 = (LetterSumAgg) agg;\n                 Integer partialSum = (Integer) integerOI.getPrimitiveJavaObject(partial);\n                 LetterSumAgg myagg2 = new LetterSumAgg();\n                 myagg2.add(partialSum);\n                 myagg1.add(myagg2.sum);\n             }\n         }\n\n         public Object terminate(AggregationBuffer agg) throws HiveException {\n             LetterSumAgg myagg = (LetterSumAgg) agg;\n             total = myagg.sum;\n             return myagg.sum;\n         }\n    }\n}\n\n```\n\n\n\n### 验证\n\n首先准备数据\n\n```\nhive> select * from users;\nOK\nzhangsan\t[\"beijing\",\"shanghai\",\"tianjin\",\"hangzhou\"]\nlisi\t[\"changchu\",\"chengdu\",\"wuhan\"]\n```\n\n\n\n然后添加jar包\n\n```\n> ADD JAR /home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar;\nAdded [/home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar] to class path\nAdded resources: [/home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar]\n```\n\n\n\n定义函数\n\n```\nhive>  CREATE TEMPORARY FUNCTION letters as 'com.will.TotalNumOfLetttersGenericUDAF';\nOK\nTime taken: 0.049 seconds\n```\n\n\n\n执行\n\n```\nhive> select letters(name) from users;\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n2020-02-23 13:25:06,087 Stage-1 map = 0%,  reduce = 0%\n2020-02-23 13:25:11,426 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.03 sec\n2020-02-23 13:25:16,607 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.01 sec\nMapReduce Total cumulative CPU time: 4 seconds 10 msec\nTotal MapReduce CPU Time Spent: 4 seconds 10 msec\nOK\n12\nTime taken: 23.819 seconds, Fetched: 1 row(s)\n```\n\n","source":"_posts/hive/hive-udf.md","raw":"---\ntitle: hive udf&udaf\ndate: 2020-02-21 18:20:21\ntags:\n    - hive\n    - udf\n    - udaf\ncategories: \n    - Hive\n---\n\n\n\nhive为我们定义了很多函数，大多数情况下是能够满足我们的需求的。但是在有些情况下，很有必要自己定义一些函数，这样使用起来非常方便。这就是自定义函数(udf)和自定义聚合函数(udaf,user defined aggregation function)。udf是每行返回一个结果，而udaf则是聚合的结果，多行产生一个结果。另外还有udtf，是一行产生多个结果。目前我还没有用到过udtf，就先不介绍它了。下面分别用两个示例介绍udf和udaf<!--more-->\n\n\n\n# udf\n\n### 功能\n\n查找array中是否包含被查询值\n\n\n\n### 步骤\n\n- 测试数据准备\n\n  ```\n  zhangsan        beijing,shanghai,tianjin,hangzhou\n  lisi    changchu,chengdu,wuhan\n  ```\n\n- hive建表与导入\n\n  ```\n  Create table users(name string, worklocations array<string> ) row format delimited fields terminated by '\\t' collection items terminated by ','; \n  \n  load data local inpath '/root/person.txt ' OVERWRITE INTO TABLE users; \n  ```\n\n- udf包生成与导入\n\n  ```\n  package com.will;\n  \n  import org.apache.hadoop.hive.ql.exec.UDF;\n  import java.util.ArrayList;\n  \n  public class FindInArray extends UDF {\n      public ArrayList<String> evaluate(String keywords, ArrayList<String> column){\n          //参数类型使用arraylist<String>对应hive中的array<string>,而不是String[]\n          if(column.contains(keywords)){\n              return column;\n          }else{\n              return null;\n          }\n      }\n      \n      public String evaluate(String keywords,ArrayList<String> column,String name){\n          //重载evaluate，另一种查询方式，返回name值\n          if(column.contains(keywords)){\n              return name;\n          }else{\n              return null;\n          }\n      }\n  }\n  ```\n\n  使用mvn 打包\n\n  ```\n  mvn clean package\n  ```\n\n- 导入hive\n\n  ```\n  add jar /home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar;\n  create temporary function find_in_array as 'com.will.FindInArray';\n  ```\n\n- 使用\n\n  ```\n  hive> select find_in_array('beijing',worklocations) from users;\n  OK\n  [\"beijing\",\"shanghai\",\"tianjin\",\"hangzhou\"]\n  NULL\n  Time taken: 0.424 seconds, Fetched: 2 row(s)\n  ```\n\n> 参考：https://blog.csdn.net/Nougats/article/details/71158318\n\n\n\n# udaf\n\n> 参考： \n>\n> https://blog.51cto.com/xiaolanlan/2397771\n>\n> https://www.cnblogs.com/Rudd/p/5137612.html\n\n\n\n### 基础知识\n\nhive的udtf有两种，Simple和Generic。这是由于历史问题，Simple先出现，但是因为反射问题效率较低，所以现在推荐使用Generic的写法。\n\n- Simple。即继承`org.apache.hadoop.hive.ql.exec.UDAF`类，并在派生类中以静态内部类的方式实现`org.apache.hadoop.hive.ql.exec.UDAFEvaluator`接口。在Hive源码包`org.apache.hadoop.hive.contrib.udaf.example`中包含几个示例。可以直接参阅。但是这些接口已经被注解为Deprecated，建议不要使用这种方式开发新的UDAF函数。\n- Generic。这是Hive社区推荐的新的写法，以抽象类代替原有的接口。新的抽象类`org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver`替代老的UDAF接口，新的抽象类`org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator`替代老的UDAFEvaluator接口。\n\n\n\nhive是立足于hadoop之上，也就是hive基于mapreduce，hive sql最终还是会转化为mapreduce执行。为了实现mapreduce，udaf中用model来表示mapreduce各个阶段。\n\n```\npublic static enum Mode {\n        PARTIAL1,\n        PARTIAL2,\n        FINAL,\n        COMPLETE;\n\n        private Mode() {}\n    }\n```\n\n- PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合，将会调用**iterate()**和**terminatePartial() **\n-  PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合，将会调用**merge()** 和 **terminatePartial()** \n- FINAL: mapreduce的reduce阶段，从部分数据的聚合到完全聚合，将会调用**merge()**和**terminate() **\n\n---\n\n- COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了，从原始数据直接到完全聚合，将会调用 **iterate()**和**terminate()**\n\n ![image.png](https://i.loli.net/2020/02/23/DlNqFRHnZ6zLuoA.png)\n\n![image.png](https://i.loli.net/2020/02/23/IKn3Vv57jPScxs8.png)\n\n\n\nudaf骨架示例：\n\n```\npublic class GenericUDAFHistogramNumeric extends AbstractGenericUDAFResolver {\n  static final Log LOG = LogFactory.getLog(GenericUDAFHistogramNumeric.class.getName());\n \n  @Override\n  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException {\n    // 这里主要做类型检查\n \n    return new GenericUDAFHistogramNumericEvaluator();\n  }\n \n  public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator {\n         // 确定各个阶段输入输出参数的数据格式ObjectInspectors\n         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException{\n             return  null;\n         }\n\n         // 保存数据聚集结果的类\n         public AggregationBuffer getNewAggregationBuffer() throws HiveException {\n             return null;\n         }\n\n\t\t // 重置聚集结果\n         public void reset(AggregationBuffer aggregationBuffer) throws HiveException {}\n\n         // map阶段，迭代处理输入sql传过来的列数据 \n         public void iterate(AggregationBuffer aggregationBuffer, Object[] objects) throws HiveException {}\n\n         // map与combiner结束返回结果，得到部分数据聚集结果\n         public Object terminatePartial(AggregationBuffer aggregationBuffer) throws HiveException {\n             return null;\n         }\n\n         // combiner合并map返回的结果，还有reducer合并mapper或combiner返回的结果。\n         public void merge(AggregationBuffer aggregationBuffer, Object o) throws HiveException {}\n\n\t\t // reducer阶段，输出最终结果 \n         public Object terminate(AggregationBuffer aggregationBuffer) throws HiveException {\n             return null;\n         }\n  }\n}\n```\n\n\n\n\n\n### 功能\n\n统计字符数\n\n\n\n### 代码\n\n```\npackage com.will;\n\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.parse.SemanticException;\nimport org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;\nimport org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\nimport org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\nimport org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\nimport org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n\npublic class TotalNumOfLetttersGenericUDAF extends AbstractGenericUDAFResolver {\n    @Override\n    public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException {\n\n        if (parameters.length != 1) {\n            throw new UDFArgumentTypeException(parameters.length - 1,\"Exactly one argument is expected.\");\n        }\n\n        ObjectInspector oi = TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(parameters[0]);\n\n        if (oi.getCategory() != ObjectInspector.Category.PRIMITIVE){\n            throw new UDFArgumentTypeException(0,\n                    \"Argument must be PRIMITIVE, but \"\n                    + oi.getCategory().name()\n                    + \" was passed.\");\n        }\n\n        PrimitiveObjectInspector inputOI = (PrimitiveObjectInspector) oi;\n        if (inputOI.getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.STRING){\n            throw new UDFArgumentTypeException(0, \"Argument must be String, but \"\n                     + inputOI.getPrimitiveCategory().name()\n                     + \" was passed.\");\n        }\n\n        return new TotalNumOfLettersEvaluator();\n    }\n\n    public static class TotalNumOfLettersEvaluator extends GenericUDAFEvaluator{\n        PrimitiveObjectInspector inputOI;\n        ObjectInspector outputOI;\n        PrimitiveObjectInspector integerOI;\n\n        int total = 0;\n        private boolean warned = false;\n\n        @Override\n         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException{\n            assert (parameters.length == 1);\n            super.init(m, parameters);\n\n            //map阶段读取sql列，输入为String基础数据格式\n            if (m == Mode.PARTIAL1 || m == Mode.COMPLETE) {\n                inputOI = (PrimitiveObjectInspector) parameters[0];\n            } else {\n                //其余阶段，输入为Integer基础数据格式\n                integerOI = (PrimitiveObjectInspector) parameters[0];\n            }\n\n            // 指定各个阶段输出数据格式都为Integer类型\n            outputOI = ObjectInspectorFactory.getReflectionObjectInspector(Integer.class,\n                                        ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n\n            return outputOI;\n        }\n\n        //存储当前字符总数的类\n        static class LetterSumAgg implements AggregationBuffer {\n            int sum = 0;\n            void add(int num){\n                sum += num;\n            }\n        }\n\n         public AggregationBuffer getNewAggregationBuffer() throws HiveException {\n             LetterSumAgg result = new LetterSumAgg();\n             return result;\n         }\n\n         public void reset(AggregationBuffer aggregationBuffer) throws HiveException {\n             LetterSumAgg myagg = new LetterSumAgg();\n         }\n\n         public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {\n             assert (parameters.length == 1);\n             if (parameters[0] != null) {\n                 LetterSumAgg myagg = (LetterSumAgg) agg;\n                 Object p1 = ((PrimitiveObjectInspector) inputOI).getPrimitiveJavaObject(parameters[0]);\n                 myagg.add(String.valueOf(p1).length());\n             }\n         }\n\n         public Object terminatePartial(AggregationBuffer agg) throws HiveException {\n             LetterSumAgg myagg = (LetterSumAgg) agg;\n             total += myagg.sum;\n             return total;\n         }\n\n         public void merge(AggregationBuffer agg, Object partial) throws HiveException {\n             if (partial != null) {\n                 LetterSumAgg myagg1 = (LetterSumAgg) agg;\n                 Integer partialSum = (Integer) integerOI.getPrimitiveJavaObject(partial);\n                 LetterSumAgg myagg2 = new LetterSumAgg();\n                 myagg2.add(partialSum);\n                 myagg1.add(myagg2.sum);\n             }\n         }\n\n         public Object terminate(AggregationBuffer agg) throws HiveException {\n             LetterSumAgg myagg = (LetterSumAgg) agg;\n             total = myagg.sum;\n             return myagg.sum;\n         }\n    }\n}\n\n```\n\n\n\n### 验证\n\n首先准备数据\n\n```\nhive> select * from users;\nOK\nzhangsan\t[\"beijing\",\"shanghai\",\"tianjin\",\"hangzhou\"]\nlisi\t[\"changchu\",\"chengdu\",\"wuhan\"]\n```\n\n\n\n然后添加jar包\n\n```\n> ADD JAR /home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar;\nAdded [/home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar] to class path\nAdded resources: [/home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar]\n```\n\n\n\n定义函数\n\n```\nhive>  CREATE TEMPORARY FUNCTION letters as 'com.will.TotalNumOfLetttersGenericUDAF';\nOK\nTime taken: 0.049 seconds\n```\n\n\n\n执行\n\n```\nhive> select letters(name) from users;\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n2020-02-23 13:25:06,087 Stage-1 map = 0%,  reduce = 0%\n2020-02-23 13:25:11,426 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.03 sec\n2020-02-23 13:25:16,607 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.01 sec\nMapReduce Total cumulative CPU time: 4 seconds 10 msec\nTotal MapReduce CPU Time Spent: 4 seconds 10 msec\nOK\n12\nTime taken: 23.819 seconds, Fetched: 1 row(s)\n```\n\n","slug":"hive/hive-udf","published":1,"updated":"2020-02-23T05:29:27.440Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck70g2rm000013ozkfd9napoe","content":"<p>hive为我们定义了很多函数，大多数情况下是能够满足我们的需求的。但是在有些情况下，很有必要自己定义一些函数，这样使用起来非常方便。这就是自定义函数(udf)和自定义聚合函数(udaf,user defined aggregation function)。udf是每行返回一个结果，而udaf则是聚合的结果，多行产生一个结果。另外还有udtf，是一行产生多个结果。目前我还没有用到过udtf，就先不介绍它了。下面分别用两个示例介绍udf和udaf<a id=\"more\"></a></p>\n<h1 id=\"udf\"><a href=\"#udf\" class=\"headerlink\" title=\"udf\"></a>udf</h1><h3 id=\"功能\"><a href=\"#功能\" class=\"headerlink\" title=\"功能\"></a>功能</h3><p>查找array中是否包含被查询值</p>\n<h3 id=\"步骤\"><a href=\"#步骤\" class=\"headerlink\" title=\"步骤\"></a>步骤</h3><ul>\n<li><p>测试数据准备</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zhangsan        beijing,shanghai,tianjin,hangzhou</span><br><span class=\"line\">lisi    changchu,chengdu,wuhan</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>hive建表与导入</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Create table users(name string, worklocations array&lt;string&gt; ) row format delimited fields terminated by &#39;\\t&#39; collection items terminated by &#39;,&#39;; </span><br><span class=\"line\"></span><br><span class=\"line\">load data local inpath &#39;&#x2F;root&#x2F;person.txt &#39; OVERWRITE INTO TABLE users;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>udf包生成与导入</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.will;</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class=\"line\">import java.util.ArrayList;</span><br><span class=\"line\"></span><br><span class=\"line\">public class FindInArray extends UDF &#123;</span><br><span class=\"line\">    public ArrayList&lt;String&gt; evaluate(String keywords, ArrayList&lt;String&gt; column)&#123;</span><br><span class=\"line\">        &#x2F;&#x2F;参数类型使用arraylist&lt;String&gt;对应hive中的array&lt;string&gt;,而不是String[]</span><br><span class=\"line\">        if(column.contains(keywords))&#123;</span><br><span class=\"line\">            return column;</span><br><span class=\"line\">        &#125;else&#123;</span><br><span class=\"line\">            return null;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    public String evaluate(String keywords,ArrayList&lt;String&gt; column,String name)&#123;</span><br><span class=\"line\">        &#x2F;&#x2F;重载evaluate，另一种查询方式，返回name值</span><br><span class=\"line\">        if(column.contains(keywords))&#123;</span><br><span class=\"line\">            return name;</span><br><span class=\"line\">        &#125;else&#123;</span><br><span class=\"line\">            return null;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>使用mvn 打包</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean package</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>导入hive</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add jar &#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar;</span><br><span class=\"line\">create temporary function find_in_array as &#39;com.will.FindInArray&#39;;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; select find_in_array(&#39;beijing&#39;,worklocations) from users;</span><br><span class=\"line\">OK</span><br><span class=\"line\">[&quot;beijing&quot;,&quot;shanghai&quot;,&quot;tianjin&quot;,&quot;hangzhou&quot;]</span><br><span class=\"line\">NULL</span><br><span class=\"line\">Time taken: 0.424 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<blockquote>\n<p>参考：<a href=\"https://blog.csdn.net/Nougats/article/details/71158318\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/Nougats/article/details/71158318</a></p>\n</blockquote>\n<h1 id=\"udaf\"><a href=\"#udaf\" class=\"headerlink\" title=\"udaf\"></a>udaf</h1><blockquote>\n<p>参考： </p>\n<p><a href=\"https://blog.51cto.com/xiaolanlan/2397771\" target=\"_blank\" rel=\"noopener\">https://blog.51cto.com/xiaolanlan/2397771</a></p>\n<p><a href=\"https://www.cnblogs.com/Rudd/p/5137612.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/Rudd/p/5137612.html</a></p>\n</blockquote>\n<h3 id=\"基础知识\"><a href=\"#基础知识\" class=\"headerlink\" title=\"基础知识\"></a>基础知识</h3><p>hive的udtf有两种，Simple和Generic。这是由于历史问题，Simple先出现，但是因为反射问题效率较低，所以现在推荐使用Generic的写法。</p>\n<ul>\n<li>Simple。即继承<code>org.apache.hadoop.hive.ql.exec.UDAF</code>类，并在派生类中以静态内部类的方式实现<code>org.apache.hadoop.hive.ql.exec.UDAFEvaluator</code>接口。在Hive源码包<code>org.apache.hadoop.hive.contrib.udaf.example</code>中包含几个示例。可以直接参阅。但是这些接口已经被注解为Deprecated，建议不要使用这种方式开发新的UDAF函数。</li>\n<li>Generic。这是Hive社区推荐的新的写法，以抽象类代替原有的接口。新的抽象类<code>org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver</code>替代老的UDAF接口，新的抽象类<code>org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator</code>替代老的UDAFEvaluator接口。</li>\n</ul>\n<p>hive是立足于hadoop之上，也就是hive基于mapreduce，hive sql最终还是会转化为mapreduce执行。为了实现mapreduce，udaf中用model来表示mapreduce各个阶段。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static enum Mode &#123;</span><br><span class=\"line\">        PARTIAL1,</span><br><span class=\"line\">        PARTIAL2,</span><br><span class=\"line\">        FINAL,</span><br><span class=\"line\">        COMPLETE;</span><br><span class=\"line\"></span><br><span class=\"line\">        private Mode() &#123;&#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合，将会调用<strong>iterate()</strong>和*<em>terminatePartial() *</em></li>\n<li>PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合，将会调用<strong>merge()</strong> 和 <strong>terminatePartial()</strong> </li>\n<li>FINAL: mapreduce的reduce阶段，从部分数据的聚合到完全聚合，将会调用<strong>merge()</strong>和*<em>terminate() *</em></li>\n</ul>\n<hr>\n<ul>\n<li><p>COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了，从原始数据直接到完全聚合，将会调用 <strong>iterate()</strong>和<strong>terminate()</strong></p>\n<p><img src=\"https://i.loli.net/2020/02/23/DlNqFRHnZ6zLuoA.png\" alt=\"image.png\"></p>\n</li>\n</ul>\n<p><img src=\"https://i.loli.net/2020/02/23/IKn3Vv57jPScxs8.png\" alt=\"image.png\"></p>\n<p>udaf骨架示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class GenericUDAFHistogramNumeric extends AbstractGenericUDAFResolver &#123;</span><br><span class=\"line\">  static final Log LOG &#x3D; LogFactory.getLog(GenericUDAFHistogramNumeric.class.getName());</span><br><span class=\"line\"> </span><br><span class=\"line\">  @Override</span><br><span class=\"line\">  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 这里主要做类型检查</span><br><span class=\"line\"> </span><br><span class=\"line\">    return new GenericUDAFHistogramNumericEvaluator();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">  public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; 确定各个阶段输入输出参数的数据格式ObjectInspectors</span><br><span class=\"line\">         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException&#123;</span><br><span class=\"line\">             return  null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; 保存数据聚集结果的类</span><br><span class=\"line\">         public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class=\"line\">             return null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t &#x2F;&#x2F; 重置聚集结果</span><br><span class=\"line\">         public void reset(AggregationBuffer aggregationBuffer) throws HiveException &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; map阶段，迭代处理输入sql传过来的列数据 </span><br><span class=\"line\">         public void iterate(AggregationBuffer aggregationBuffer, Object[] objects) throws HiveException &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; map与combiner结束返回结果，得到部分数据聚集结果</span><br><span class=\"line\">         public Object terminatePartial(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class=\"line\">             return null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; combiner合并map返回的结果，还有reducer合并mapper或combiner返回的结果。</span><br><span class=\"line\">         public void merge(AggregationBuffer aggregationBuffer, Object o) throws HiveException &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t &#x2F;&#x2F; reducer阶段，输出最终结果 </span><br><span class=\"line\">         public Object terminate(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class=\"line\">             return null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n<h3 id=\"功能-1\"><a href=\"#功能-1\" class=\"headerlink\" title=\"功能\"></a>功能</h3><p>统计字符数</p>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.will;</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.parse.SemanticException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;</span><br><span class=\"line\"></span><br><span class=\"line\">public class TotalNumOfLetttersGenericUDAF extends AbstractGenericUDAFResolver &#123;</span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        if (parameters.length !&#x3D; 1) &#123;</span><br><span class=\"line\">            throw new UDFArgumentTypeException(parameters.length - 1,&quot;Exactly one argument is expected.&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        ObjectInspector oi &#x3D; TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(parameters[0]);</span><br><span class=\"line\"></span><br><span class=\"line\">        if (oi.getCategory() !&#x3D; ObjectInspector.Category.PRIMITIVE)&#123;</span><br><span class=\"line\">            throw new UDFArgumentTypeException(0,</span><br><span class=\"line\">                    &quot;Argument must be PRIMITIVE, but &quot;</span><br><span class=\"line\">                    + oi.getCategory().name()</span><br><span class=\"line\">                    + &quot; was passed.&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        PrimitiveObjectInspector inputOI &#x3D; (PrimitiveObjectInspector) oi;</span><br><span class=\"line\">        if (inputOI.getPrimitiveCategory() !&#x3D; PrimitiveObjectInspector.PrimitiveCategory.STRING)&#123;</span><br><span class=\"line\">            throw new UDFArgumentTypeException(0, &quot;Argument must be String, but &quot;</span><br><span class=\"line\">                     + inputOI.getPrimitiveCategory().name()</span><br><span class=\"line\">                     + &quot; was passed.&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        return new TotalNumOfLettersEvaluator();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public static class TotalNumOfLettersEvaluator extends GenericUDAFEvaluator&#123;</span><br><span class=\"line\">        PrimitiveObjectInspector inputOI;</span><br><span class=\"line\">        ObjectInspector outputOI;</span><br><span class=\"line\">        PrimitiveObjectInspector integerOI;</span><br><span class=\"line\"></span><br><span class=\"line\">        int total &#x3D; 0;</span><br><span class=\"line\">        private boolean warned &#x3D; false;</span><br><span class=\"line\"></span><br><span class=\"line\">        @Override</span><br><span class=\"line\">         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException&#123;</span><br><span class=\"line\">            assert (parameters.length &#x3D;&#x3D; 1);</span><br><span class=\"line\">            super.init(m, parameters);</span><br><span class=\"line\"></span><br><span class=\"line\">            &#x2F;&#x2F;map阶段读取sql列，输入为String基础数据格式</span><br><span class=\"line\">            if (m &#x3D;&#x3D; Mode.PARTIAL1 || m &#x3D;&#x3D; Mode.COMPLETE) &#123;</span><br><span class=\"line\">                inputOI &#x3D; (PrimitiveObjectInspector) parameters[0];</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">                &#x2F;&#x2F;其余阶段，输入为Integer基础数据格式</span><br><span class=\"line\">                integerOI &#x3D; (PrimitiveObjectInspector) parameters[0];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            &#x2F;&#x2F; 指定各个阶段输出数据格式都为Integer类型</span><br><span class=\"line\">            outputOI &#x3D; ObjectInspectorFactory.getReflectionObjectInspector(Integer.class,</span><br><span class=\"line\">                                        ObjectInspectorFactory.ObjectInspectorOptions.JAVA);</span><br><span class=\"line\"></span><br><span class=\"line\">            return outputOI;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#x2F;&#x2F;存储当前字符总数的类</span><br><span class=\"line\">        static class LetterSumAgg implements AggregationBuffer &#123;</span><br><span class=\"line\">            int sum &#x3D; 0;</span><br><span class=\"line\">            void add(int num)&#123;</span><br><span class=\"line\">                sum +&#x3D; num;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg result &#x3D; new LetterSumAgg();</span><br><span class=\"line\">             return result;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public void reset(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg myagg &#x3D; new LetterSumAgg();</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException &#123;</span><br><span class=\"line\">             assert (parameters.length &#x3D;&#x3D; 1);</span><br><span class=\"line\">             if (parameters[0] !&#x3D; null) &#123;</span><br><span class=\"line\">                 LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">                 Object p1 &#x3D; ((PrimitiveObjectInspector) inputOI).getPrimitiveJavaObject(parameters[0]);</span><br><span class=\"line\">                 myagg.add(String.valueOf(p1).length());</span><br><span class=\"line\">             &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public Object terminatePartial(AggregationBuffer agg) throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">             total +&#x3D; myagg.sum;</span><br><span class=\"line\">             return total;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public void merge(AggregationBuffer agg, Object partial) throws HiveException &#123;</span><br><span class=\"line\">             if (partial !&#x3D; null) &#123;</span><br><span class=\"line\">                 LetterSumAgg myagg1 &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">                 Integer partialSum &#x3D; (Integer) integerOI.getPrimitiveJavaObject(partial);</span><br><span class=\"line\">                 LetterSumAgg myagg2 &#x3D; new LetterSumAgg();</span><br><span class=\"line\">                 myagg2.add(partialSum);</span><br><span class=\"line\">                 myagg1.add(myagg2.sum);</span><br><span class=\"line\">             &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public Object terminate(AggregationBuffer agg) throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">             total &#x3D; myagg.sum;</span><br><span class=\"line\">             return myagg.sum;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h3><p>首先准备数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; select * from users;</span><br><span class=\"line\">OK</span><br><span class=\"line\">zhangsan\t[&quot;beijing&quot;,&quot;shanghai&quot;,&quot;tianjin&quot;,&quot;hangzhou&quot;]</span><br><span class=\"line\">lisi\t[&quot;changchu&quot;,&quot;chengdu&quot;,&quot;wuhan&quot;]</span><br></pre></td></tr></table></figure>\n\n\n\n<p>然后添加jar包</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; ADD JAR &#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar;</span><br><span class=\"line\">Added [&#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar] to class path</span><br><span class=\"line\">Added resources: [&#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar]</span><br></pre></td></tr></table></figure>\n\n\n\n<p>定义函数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt;  CREATE TEMPORARY FUNCTION letters as &#39;com.will.TotalNumOfLetttersGenericUDAF&#39;;</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.049 seconds</span><br></pre></td></tr></table></figure>\n\n\n\n<p>执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; select letters(name) from users;</span><br><span class=\"line\">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class=\"line\">2020-02-23 13:25:06,087 Stage-1 map &#x3D; 0%,  reduce &#x3D; 0%</span><br><span class=\"line\">2020-02-23 13:25:11,426 Stage-1 map &#x3D; 100%,  reduce &#x3D; 0%, Cumulative CPU 2.03 sec</span><br><span class=\"line\">2020-02-23 13:25:16,607 Stage-1 map &#x3D; 100%,  reduce &#x3D; 100%, Cumulative CPU 4.01 sec</span><br><span class=\"line\">MapReduce Total cumulative CPU time: 4 seconds 10 msec</span><br><span class=\"line\">Total MapReduce CPU Time Spent: 4 seconds 10 msec</span><br><span class=\"line\">OK</span><br><span class=\"line\">12</span><br><span class=\"line\">Time taken: 23.819 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>\n\n","site":{"data":{}},"excerpt":"<p>hive为我们定义了很多函数，大多数情况下是能够满足我们的需求的。但是在有些情况下，很有必要自己定义一些函数，这样使用起来非常方便。这就是自定义函数(udf)和自定义聚合函数(udaf,user defined aggregation function)。udf是每行返回一个结果，而udaf则是聚合的结果，多行产生一个结果。另外还有udtf，是一行产生多个结果。目前我还没有用到过udtf，就先不介绍它了。下面分别用两个示例介绍udf和udaf","more":"</p>\n<h1 id=\"udf\"><a href=\"#udf\" class=\"headerlink\" title=\"udf\"></a>udf</h1><h3 id=\"功能\"><a href=\"#功能\" class=\"headerlink\" title=\"功能\"></a>功能</h3><p>查找array中是否包含被查询值</p>\n<h3 id=\"步骤\"><a href=\"#步骤\" class=\"headerlink\" title=\"步骤\"></a>步骤</h3><ul>\n<li><p>测试数据准备</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zhangsan        beijing,shanghai,tianjin,hangzhou</span><br><span class=\"line\">lisi    changchu,chengdu,wuhan</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>hive建表与导入</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Create table users(name string, worklocations array&lt;string&gt; ) row format delimited fields terminated by &#39;\\t&#39; collection items terminated by &#39;,&#39;; </span><br><span class=\"line\"></span><br><span class=\"line\">load data local inpath &#39;&#x2F;root&#x2F;person.txt &#39; OVERWRITE INTO TABLE users;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>udf包生成与导入</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.will;</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class=\"line\">import java.util.ArrayList;</span><br><span class=\"line\"></span><br><span class=\"line\">public class FindInArray extends UDF &#123;</span><br><span class=\"line\">    public ArrayList&lt;String&gt; evaluate(String keywords, ArrayList&lt;String&gt; column)&#123;</span><br><span class=\"line\">        &#x2F;&#x2F;参数类型使用arraylist&lt;String&gt;对应hive中的array&lt;string&gt;,而不是String[]</span><br><span class=\"line\">        if(column.contains(keywords))&#123;</span><br><span class=\"line\">            return column;</span><br><span class=\"line\">        &#125;else&#123;</span><br><span class=\"line\">            return null;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    public String evaluate(String keywords,ArrayList&lt;String&gt; column,String name)&#123;</span><br><span class=\"line\">        &#x2F;&#x2F;重载evaluate，另一种查询方式，返回name值</span><br><span class=\"line\">        if(column.contains(keywords))&#123;</span><br><span class=\"line\">            return name;</span><br><span class=\"line\">        &#125;else&#123;</span><br><span class=\"line\">            return null;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>使用mvn 打包</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean package</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>导入hive</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add jar &#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar;</span><br><span class=\"line\">create temporary function find_in_array as &#39;com.will.FindInArray&#39;;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; select find_in_array(&#39;beijing&#39;,worklocations) from users;</span><br><span class=\"line\">OK</span><br><span class=\"line\">[&quot;beijing&quot;,&quot;shanghai&quot;,&quot;tianjin&quot;,&quot;hangzhou&quot;]</span><br><span class=\"line\">NULL</span><br><span class=\"line\">Time taken: 0.424 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<blockquote>\n<p>参考：<a href=\"https://blog.csdn.net/Nougats/article/details/71158318\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/Nougats/article/details/71158318</a></p>\n</blockquote>\n<h1 id=\"udaf\"><a href=\"#udaf\" class=\"headerlink\" title=\"udaf\"></a>udaf</h1><blockquote>\n<p>参考： </p>\n<p><a href=\"https://blog.51cto.com/xiaolanlan/2397771\" target=\"_blank\" rel=\"noopener\">https://blog.51cto.com/xiaolanlan/2397771</a></p>\n<p><a href=\"https://www.cnblogs.com/Rudd/p/5137612.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/Rudd/p/5137612.html</a></p>\n</blockquote>\n<h3 id=\"基础知识\"><a href=\"#基础知识\" class=\"headerlink\" title=\"基础知识\"></a>基础知识</h3><p>hive的udtf有两种，Simple和Generic。这是由于历史问题，Simple先出现，但是因为反射问题效率较低，所以现在推荐使用Generic的写法。</p>\n<ul>\n<li>Simple。即继承<code>org.apache.hadoop.hive.ql.exec.UDAF</code>类，并在派生类中以静态内部类的方式实现<code>org.apache.hadoop.hive.ql.exec.UDAFEvaluator</code>接口。在Hive源码包<code>org.apache.hadoop.hive.contrib.udaf.example</code>中包含几个示例。可以直接参阅。但是这些接口已经被注解为Deprecated，建议不要使用这种方式开发新的UDAF函数。</li>\n<li>Generic。这是Hive社区推荐的新的写法，以抽象类代替原有的接口。新的抽象类<code>org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver</code>替代老的UDAF接口，新的抽象类<code>org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator</code>替代老的UDAFEvaluator接口。</li>\n</ul>\n<p>hive是立足于hadoop之上，也就是hive基于mapreduce，hive sql最终还是会转化为mapreduce执行。为了实现mapreduce，udaf中用model来表示mapreduce各个阶段。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static enum Mode &#123;</span><br><span class=\"line\">        PARTIAL1,</span><br><span class=\"line\">        PARTIAL2,</span><br><span class=\"line\">        FINAL,</span><br><span class=\"line\">        COMPLETE;</span><br><span class=\"line\"></span><br><span class=\"line\">        private Mode() &#123;&#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合，将会调用<strong>iterate()</strong>和*<em>terminatePartial() *</em></li>\n<li>PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合，将会调用<strong>merge()</strong> 和 <strong>terminatePartial()</strong> </li>\n<li>FINAL: mapreduce的reduce阶段，从部分数据的聚合到完全聚合，将会调用<strong>merge()</strong>和*<em>terminate() *</em></li>\n</ul>\n<hr>\n<ul>\n<li><p>COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了，从原始数据直接到完全聚合，将会调用 <strong>iterate()</strong>和<strong>terminate()</strong></p>\n<p><img src=\"https://i.loli.net/2020/02/23/DlNqFRHnZ6zLuoA.png\" alt=\"image.png\"></p>\n</li>\n</ul>\n<p><img src=\"https://i.loli.net/2020/02/23/IKn3Vv57jPScxs8.png\" alt=\"image.png\"></p>\n<p>udaf骨架示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class GenericUDAFHistogramNumeric extends AbstractGenericUDAFResolver &#123;</span><br><span class=\"line\">  static final Log LOG &#x3D; LogFactory.getLog(GenericUDAFHistogramNumeric.class.getName());</span><br><span class=\"line\"> </span><br><span class=\"line\">  @Override</span><br><span class=\"line\">  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 这里主要做类型检查</span><br><span class=\"line\"> </span><br><span class=\"line\">    return new GenericUDAFHistogramNumericEvaluator();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">  public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; 确定各个阶段输入输出参数的数据格式ObjectInspectors</span><br><span class=\"line\">         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException&#123;</span><br><span class=\"line\">             return  null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; 保存数据聚集结果的类</span><br><span class=\"line\">         public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class=\"line\">             return null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t &#x2F;&#x2F; 重置聚集结果</span><br><span class=\"line\">         public void reset(AggregationBuffer aggregationBuffer) throws HiveException &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; map阶段，迭代处理输入sql传过来的列数据 </span><br><span class=\"line\">         public void iterate(AggregationBuffer aggregationBuffer, Object[] objects) throws HiveException &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; map与combiner结束返回结果，得到部分数据聚集结果</span><br><span class=\"line\">         public Object terminatePartial(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class=\"line\">             return null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; combiner合并map返回的结果，还有reducer合并mapper或combiner返回的结果。</span><br><span class=\"line\">         public void merge(AggregationBuffer aggregationBuffer, Object o) throws HiveException &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t &#x2F;&#x2F; reducer阶段，输出最终结果 </span><br><span class=\"line\">         public Object terminate(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class=\"line\">             return null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n<h3 id=\"功能-1\"><a href=\"#功能-1\" class=\"headerlink\" title=\"功能\"></a>功能</h3><p>统计字符数</p>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.will;</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.parse.SemanticException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;</span><br><span class=\"line\"></span><br><span class=\"line\">public class TotalNumOfLetttersGenericUDAF extends AbstractGenericUDAFResolver &#123;</span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        if (parameters.length !&#x3D; 1) &#123;</span><br><span class=\"line\">            throw new UDFArgumentTypeException(parameters.length - 1,&quot;Exactly one argument is expected.&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        ObjectInspector oi &#x3D; TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(parameters[0]);</span><br><span class=\"line\"></span><br><span class=\"line\">        if (oi.getCategory() !&#x3D; ObjectInspector.Category.PRIMITIVE)&#123;</span><br><span class=\"line\">            throw new UDFArgumentTypeException(0,</span><br><span class=\"line\">                    &quot;Argument must be PRIMITIVE, but &quot;</span><br><span class=\"line\">                    + oi.getCategory().name()</span><br><span class=\"line\">                    + &quot; was passed.&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        PrimitiveObjectInspector inputOI &#x3D; (PrimitiveObjectInspector) oi;</span><br><span class=\"line\">        if (inputOI.getPrimitiveCategory() !&#x3D; PrimitiveObjectInspector.PrimitiveCategory.STRING)&#123;</span><br><span class=\"line\">            throw new UDFArgumentTypeException(0, &quot;Argument must be String, but &quot;</span><br><span class=\"line\">                     + inputOI.getPrimitiveCategory().name()</span><br><span class=\"line\">                     + &quot; was passed.&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        return new TotalNumOfLettersEvaluator();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public static class TotalNumOfLettersEvaluator extends GenericUDAFEvaluator&#123;</span><br><span class=\"line\">        PrimitiveObjectInspector inputOI;</span><br><span class=\"line\">        ObjectInspector outputOI;</span><br><span class=\"line\">        PrimitiveObjectInspector integerOI;</span><br><span class=\"line\"></span><br><span class=\"line\">        int total &#x3D; 0;</span><br><span class=\"line\">        private boolean warned &#x3D; false;</span><br><span class=\"line\"></span><br><span class=\"line\">        @Override</span><br><span class=\"line\">         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException&#123;</span><br><span class=\"line\">            assert (parameters.length &#x3D;&#x3D; 1);</span><br><span class=\"line\">            super.init(m, parameters);</span><br><span class=\"line\"></span><br><span class=\"line\">            &#x2F;&#x2F;map阶段读取sql列，输入为String基础数据格式</span><br><span class=\"line\">            if (m &#x3D;&#x3D; Mode.PARTIAL1 || m &#x3D;&#x3D; Mode.COMPLETE) &#123;</span><br><span class=\"line\">                inputOI &#x3D; (PrimitiveObjectInspector) parameters[0];</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">                &#x2F;&#x2F;其余阶段，输入为Integer基础数据格式</span><br><span class=\"line\">                integerOI &#x3D; (PrimitiveObjectInspector) parameters[0];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            &#x2F;&#x2F; 指定各个阶段输出数据格式都为Integer类型</span><br><span class=\"line\">            outputOI &#x3D; ObjectInspectorFactory.getReflectionObjectInspector(Integer.class,</span><br><span class=\"line\">                                        ObjectInspectorFactory.ObjectInspectorOptions.JAVA);</span><br><span class=\"line\"></span><br><span class=\"line\">            return outputOI;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#x2F;&#x2F;存储当前字符总数的类</span><br><span class=\"line\">        static class LetterSumAgg implements AggregationBuffer &#123;</span><br><span class=\"line\">            int sum &#x3D; 0;</span><br><span class=\"line\">            void add(int num)&#123;</span><br><span class=\"line\">                sum +&#x3D; num;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg result &#x3D; new LetterSumAgg();</span><br><span class=\"line\">             return result;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public void reset(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg myagg &#x3D; new LetterSumAgg();</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException &#123;</span><br><span class=\"line\">             assert (parameters.length &#x3D;&#x3D; 1);</span><br><span class=\"line\">             if (parameters[0] !&#x3D; null) &#123;</span><br><span class=\"line\">                 LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">                 Object p1 &#x3D; ((PrimitiveObjectInspector) inputOI).getPrimitiveJavaObject(parameters[0]);</span><br><span class=\"line\">                 myagg.add(String.valueOf(p1).length());</span><br><span class=\"line\">             &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public Object terminatePartial(AggregationBuffer agg) throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">             total +&#x3D; myagg.sum;</span><br><span class=\"line\">             return total;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public void merge(AggregationBuffer agg, Object partial) throws HiveException &#123;</span><br><span class=\"line\">             if (partial !&#x3D; null) &#123;</span><br><span class=\"line\">                 LetterSumAgg myagg1 &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">                 Integer partialSum &#x3D; (Integer) integerOI.getPrimitiveJavaObject(partial);</span><br><span class=\"line\">                 LetterSumAgg myagg2 &#x3D; new LetterSumAgg();</span><br><span class=\"line\">                 myagg2.add(partialSum);</span><br><span class=\"line\">                 myagg1.add(myagg2.sum);</span><br><span class=\"line\">             &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public Object terminate(AggregationBuffer agg) throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">             total &#x3D; myagg.sum;</span><br><span class=\"line\">             return myagg.sum;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h3><p>首先准备数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; select * from users;</span><br><span class=\"line\">OK</span><br><span class=\"line\">zhangsan\t[&quot;beijing&quot;,&quot;shanghai&quot;,&quot;tianjin&quot;,&quot;hangzhou&quot;]</span><br><span class=\"line\">lisi\t[&quot;changchu&quot;,&quot;chengdu&quot;,&quot;wuhan&quot;]</span><br></pre></td></tr></table></figure>\n\n\n\n<p>然后添加jar包</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; ADD JAR &#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar;</span><br><span class=\"line\">Added [&#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar] to class path</span><br><span class=\"line\">Added resources: [&#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar]</span><br></pre></td></tr></table></figure>\n\n\n\n<p>定义函数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt;  CREATE TEMPORARY FUNCTION letters as &#39;com.will.TotalNumOfLetttersGenericUDAF&#39;;</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.049 seconds</span><br></pre></td></tr></table></figure>\n\n\n\n<p>执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; select letters(name) from users;</span><br><span class=\"line\">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class=\"line\">2020-02-23 13:25:06,087 Stage-1 map &#x3D; 0%,  reduce &#x3D; 0%</span><br><span class=\"line\">2020-02-23 13:25:11,426 Stage-1 map &#x3D; 100%,  reduce &#x3D; 0%, Cumulative CPU 2.03 sec</span><br><span class=\"line\">2020-02-23 13:25:16,607 Stage-1 map &#x3D; 100%,  reduce &#x3D; 100%, Cumulative CPU 4.01 sec</span><br><span class=\"line\">MapReduce Total cumulative CPU time: 4 seconds 10 msec</span><br><span class=\"line\">Total MapReduce CPU Time Spent: 4 seconds 10 msec</span><br><span class=\"line\">OK</span><br><span class=\"line\">12</span><br><span class=\"line\">Time taken: 23.819 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>"},{"title":"hadoop在windows 10下安装步骤","date":"2020-02-15T12:05:16.000Z","_content":"\n# 序言\n\n首先劝大家在有条件情况下能用mac就用mac，再不行用linux系统，在windows上运行hadoop不是一个好主意！真的好麻烦。。。\n\n<!--more-->\n\n\n\n# 准备文件\n\n- 在官网上下载hadoop的压缩包\n- 然后有个github项目专门做windows下配置的包，具体链接需要自己搜一下\n\n我用的是hadoop 2.10.0，然后配置文件和hadoop包都在里面，需要自己下载\n\n```\n链接：https://pan.baidu.com/s/18ZVB89xOUq43gJ7cqlZUGA \n提取码：wj3v \n```\n\n\n\n# 安装步骤\n\n- 安装好java环境，这是基础，网上一堆教程\n- 解压缩hadoop压缩包，然后解压下载的另一个配置文件，直接拷贝覆盖即可\n\n这里卡了好久，因为文件路径太长经常解压后部分文件无法解压成功，这个可以参考链接：[点这里]( https://knowledge.autodesk.com/zh-hans/search-result/caas/sfdcarticles/sfdcarticles/CHS/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html)，最好放在盘的第一层，我就放在C:\\下面\n\n- 配置hadoop环境变量\n\n  我的电脑->属性->高级系统设置->环境变量->系统变量\n\n  新建HADOOP_HOME, 我的配置：C:\\hadoop-2.10.0\\bin\n\n  ![b146837bly1gbxf3b0kv0j20s9071dfu.jpg](https://i.loli.net/2020/02/20/T9MjvyiPe8rcE5H.jpg)\n\n​     在PATH变量中添加：%HADOOP_HOME%\n\n- 编辑 hadoop安装目录下 etc/hadoop/hadoop-env.cmd, 替换JAVA_HOME路径，\"D:\\program files\\Java\\jdk1.8.0_171\"为JAVA安装路径。\n\n  set JAVA_HOME=\"D:\\program files\\Java\\jdk1.8.0_171\"\n\n  然后编辑在hadoop根目录下创建data目录，目录中再创建两个空文件夹datanode和namenode，之后编辑 etc/hadoop/hdfs-site.xml，替换路径/hadoop-2.10.0/为你的hadoop安装根目录\n\n  ```\n  <configuration>\n   <property>\n          <name>dfs.replication</name>\n          <value>1</value>\n      </property>\n      <property>\n          <name>dfs.namenode.name.dir</name>\n          <value>/hadoop-2.10.0/data/namenode</value>\n      </property>\n      <property>\n          <name>dfs.datanode.data.dir</name>\n          <value>/hadoop-2.10.0/data/datanode</value>\n      </property>\n  </configuration>\n  ```\n\n- 格式化namenode\n\n  ```\n  在任意目录执行 hdfs namenode -format\n  ```\n\n- 到安装根目录下的sbin目录，执行\n\n  ```\n  start-all.cmd\n  ```\n\n  ![b146837bly1gbxfmuf908j20z50li7gm.jpg](https://i.loli.net/2020/02/20/Lupda64fCRmJEHo.jpg)\n\n  验证是否成功：\n\n  ```\n  jps\n  ```\n\n  会有以下进程在运行：\n\n  NodeManager\n  DataNode\n  ResourceManager\n  NameNode\n\n\n\n# 问题及解决方法\n\n```\njava.lang.NoClassDefFoundError: org/apache/hadoop/yarn/server/timelineservice/collector/TimelineCollectorManager\n        at java.lang.ClassLoader.defineClass1(Native Method)\n        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        at java.lang.Class.getDeclaredMethods0(Native Method)\n        at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)\n        at java.lang.Class.getDeclaredMethods(Class.java:1975)\n        at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:688)\n        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:380)\n        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:399)\n        at com.google.inject.internal.BindingBuilder.toInstance(BindingBuilder.java:84)\n        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:56)\n        at org.apache.hadoop.yarn.webapp.WebApp.configureServlets(WebApp.java:160)\n        at com.google.inject.servlet.ServletModule.configure(ServletModule.java:55)\n        at com.google.inject.AbstractModule.configure(AbstractModule.java:62)\n        at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)\n        at com.google.inject.spi.Elements.getElements(Elements.java:110)\n        at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)\n        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)\n        at com.google.inject.Guice.createInjector(Guice.java:96)\n        at com.google.inject.Guice.createInjector(Guice.java:73)\n        at com.google.inject.Guice.createInjector(Guice.java:62)\n        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:356)\n        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:401)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1137)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1245)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1446)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        ... 36 more\n```\n\n**解决方法： **share\\hadoop\\yarn\\timelineservice 下 hadoop-yarn-server-timelineservice-3.0.3.jar copy 到share\\hadoop\\yarn目录下 ","source":"_posts/hadoop/hadoop-hadoop安装.md","raw":"---\ntitle: hadoop在windows 10下安装步骤\ndate: 2020-02-15 20:05:16\ntags: \n    - hadoop\n    - hadoop安装\n    - windows 10\ncategories: \n    - hadoop\n---\n\n# 序言\n\n首先劝大家在有条件情况下能用mac就用mac，再不行用linux系统，在windows上运行hadoop不是一个好主意！真的好麻烦。。。\n\n<!--more-->\n\n\n\n# 准备文件\n\n- 在官网上下载hadoop的压缩包\n- 然后有个github项目专门做windows下配置的包，具体链接需要自己搜一下\n\n我用的是hadoop 2.10.0，然后配置文件和hadoop包都在里面，需要自己下载\n\n```\n链接：https://pan.baidu.com/s/18ZVB89xOUq43gJ7cqlZUGA \n提取码：wj3v \n```\n\n\n\n# 安装步骤\n\n- 安装好java环境，这是基础，网上一堆教程\n- 解压缩hadoop压缩包，然后解压下载的另一个配置文件，直接拷贝覆盖即可\n\n这里卡了好久，因为文件路径太长经常解压后部分文件无法解压成功，这个可以参考链接：[点这里]( https://knowledge.autodesk.com/zh-hans/search-result/caas/sfdcarticles/sfdcarticles/CHS/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html)，最好放在盘的第一层，我就放在C:\\下面\n\n- 配置hadoop环境变量\n\n  我的电脑->属性->高级系统设置->环境变量->系统变量\n\n  新建HADOOP_HOME, 我的配置：C:\\hadoop-2.10.0\\bin\n\n  ![b146837bly1gbxf3b0kv0j20s9071dfu.jpg](https://i.loli.net/2020/02/20/T9MjvyiPe8rcE5H.jpg)\n\n​     在PATH变量中添加：%HADOOP_HOME%\n\n- 编辑 hadoop安装目录下 etc/hadoop/hadoop-env.cmd, 替换JAVA_HOME路径，\"D:\\program files\\Java\\jdk1.8.0_171\"为JAVA安装路径。\n\n  set JAVA_HOME=\"D:\\program files\\Java\\jdk1.8.0_171\"\n\n  然后编辑在hadoop根目录下创建data目录，目录中再创建两个空文件夹datanode和namenode，之后编辑 etc/hadoop/hdfs-site.xml，替换路径/hadoop-2.10.0/为你的hadoop安装根目录\n\n  ```\n  <configuration>\n   <property>\n          <name>dfs.replication</name>\n          <value>1</value>\n      </property>\n      <property>\n          <name>dfs.namenode.name.dir</name>\n          <value>/hadoop-2.10.0/data/namenode</value>\n      </property>\n      <property>\n          <name>dfs.datanode.data.dir</name>\n          <value>/hadoop-2.10.0/data/datanode</value>\n      </property>\n  </configuration>\n  ```\n\n- 格式化namenode\n\n  ```\n  在任意目录执行 hdfs namenode -format\n  ```\n\n- 到安装根目录下的sbin目录，执行\n\n  ```\n  start-all.cmd\n  ```\n\n  ![b146837bly1gbxfmuf908j20z50li7gm.jpg](https://i.loli.net/2020/02/20/Lupda64fCRmJEHo.jpg)\n\n  验证是否成功：\n\n  ```\n  jps\n  ```\n\n  会有以下进程在运行：\n\n  NodeManager\n  DataNode\n  ResourceManager\n  NameNode\n\n\n\n# 问题及解决方法\n\n```\njava.lang.NoClassDefFoundError: org/apache/hadoop/yarn/server/timelineservice/collector/TimelineCollectorManager\n        at java.lang.ClassLoader.defineClass1(Native Method)\n        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        at java.lang.Class.getDeclaredMethods0(Native Method)\n        at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)\n        at java.lang.Class.getDeclaredMethods(Class.java:1975)\n        at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:688)\n        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:380)\n        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:399)\n        at com.google.inject.internal.BindingBuilder.toInstance(BindingBuilder.java:84)\n        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:56)\n        at org.apache.hadoop.yarn.webapp.WebApp.configureServlets(WebApp.java:160)\n        at com.google.inject.servlet.ServletModule.configure(ServletModule.java:55)\n        at com.google.inject.AbstractModule.configure(AbstractModule.java:62)\n        at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)\n        at com.google.inject.spi.Elements.getElements(Elements.java:110)\n        at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)\n        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)\n        at com.google.inject.Guice.createInjector(Guice.java:96)\n        at com.google.inject.Guice.createInjector(Guice.java:73)\n        at com.google.inject.Guice.createInjector(Guice.java:62)\n        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:356)\n        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:401)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1137)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1245)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1446)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        ... 36 more\n```\n\n**解决方法： **share\\hadoop\\yarn\\timelineservice 下 hadoop-yarn-server-timelineservice-3.0.3.jar copy 到share\\hadoop\\yarn目录下 ","slug":"hadoop/hadoop-hadoop安装","published":1,"updated":"2020-02-19T18:21:13.883Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck70g5sgp00009fzka8o7bvri","content":"<h1 id=\"序言\"><a href=\"#序言\" class=\"headerlink\" title=\"序言\"></a>序言</h1><p>首先劝大家在有条件情况下能用mac就用mac，再不行用linux系统，在windows上运行hadoop不是一个好主意！真的好麻烦。。。</p>\n<a id=\"more\"></a>\n\n\n\n<h1 id=\"准备文件\"><a href=\"#准备文件\" class=\"headerlink\" title=\"准备文件\"></a>准备文件</h1><ul>\n<li>在官网上下载hadoop的压缩包</li>\n<li>然后有个github项目专门做windows下配置的包，具体链接需要自己搜一下</li>\n</ul>\n<p>我用的是hadoop 2.10.0，然后配置文件和hadoop包都在里面，需要自己下载</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;18ZVB89xOUq43gJ7cqlZUGA </span><br><span class=\"line\">提取码：wj3v</span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"安装步骤\"><a href=\"#安装步骤\" class=\"headerlink\" title=\"安装步骤\"></a>安装步骤</h1><ul>\n<li>安装好java环境，这是基础，网上一堆教程</li>\n<li>解压缩hadoop压缩包，然后解压下载的另一个配置文件，直接拷贝覆盖即可</li>\n</ul>\n<p>这里卡了好久，因为文件路径太长经常解压后部分文件无法解压成功，这个可以参考链接：<a href=\"https://knowledge.autodesk.com/zh-hans/search-result/caas/sfdcarticles/sfdcarticles/CHS/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html\" target=\"_blank\" rel=\"noopener\">点这里</a>，最好放在盘的第一层，我就放在C:\\下面</p>\n<ul>\n<li><p>配置hadoop环境变量</p>\n<p>我的电脑-&gt;属性-&gt;高级系统设置-&gt;环境变量-&gt;系统变量</p>\n<p>新建HADOOP_HOME, 我的配置：C:\\hadoop-2.10.0\\bin</p>\n<p><img src=\"https://i.loli.net/2020/02/20/T9MjvyiPe8rcE5H.jpg\" alt=\"b146837bly1gbxf3b0kv0j20s9071dfu.jpg\"></p>\n</li>\n</ul>\n<p>​     在PATH变量中添加：%HADOOP_HOME%</p>\n<ul>\n<li><p>编辑 hadoop安装目录下 etc/hadoop/hadoop-env.cmd, 替换JAVA_HOME路径，”D:\\program files\\Java\\jdk1.8.0_171”为JAVA安装路径。</p>\n<p>set JAVA_HOME=”D:\\program files\\Java\\jdk1.8.0_171”</p>\n<p>然后编辑在hadoop根目录下创建data目录，目录中再创建两个空文件夹datanode和namenode，之后编辑 etc/hadoop/hdfs-site.xml，替换路径/hadoop-2.10.0/为你的hadoop安装根目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\"> &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class=\"line\">        &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class=\"line\">    &lt;&#x2F;property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class=\"line\">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;namenode&lt;&#x2F;value&gt;</span><br><span class=\"line\">    &lt;&#x2F;property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class=\"line\">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;datanode&lt;&#x2F;value&gt;</span><br><span class=\"line\">    &lt;&#x2F;property&gt;</span><br><span class=\"line\">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>格式化namenode</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在任意目录执行 hdfs namenode -format</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>到安装根目录下的sbin目录，执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">start-all.cmd</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://i.loli.net/2020/02/20/Lupda64fCRmJEHo.jpg\" alt=\"b146837bly1gbxfmuf908j20z50li7gm.jpg\"></p>\n<p>验证是否成功：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jps</span><br></pre></td></tr></table></figure>\n\n<p>会有以下进程在运行：</p>\n<p>NodeManager<br>DataNode<br>ResourceManager<br>NameNode</p>\n</li>\n</ul>\n<h1 id=\"问题及解决方法\"><a href=\"#问题及解决方法\" class=\"headerlink\" title=\"问题及解决方法\"></a>问题及解决方法</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;yarn&#x2F;server&#x2F;timelineservice&#x2F;collector&#x2F;TimelineCollectorManager</span><br><span class=\"line\">        at java.lang.ClassLoader.defineClass1(Native Method)</span><br><span class=\"line\">        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)</span><br><span class=\"line\">        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)</span><br><span class=\"line\">        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)</span><br><span class=\"line\">        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)</span><br><span class=\"line\">        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)</span><br><span class=\"line\">        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)</span><br><span class=\"line\">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class=\"line\">        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class=\"line\">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class=\"line\">        at java.lang.Class.getDeclaredMethods0(Native Method)</span><br><span class=\"line\">        at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)</span><br><span class=\"line\">        at java.lang.Class.getDeclaredMethods(Class.java:1975)</span><br><span class=\"line\">        at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:688)</span><br><span class=\"line\">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:380)</span><br><span class=\"line\">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:399)</span><br><span class=\"line\">        at com.google.inject.internal.BindingBuilder.toInstance(BindingBuilder.java:84)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:56)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.webapp.WebApp.configureServlets(WebApp.java:160)</span><br><span class=\"line\">        at com.google.inject.servlet.ServletModule.configure(ServletModule.java:55)</span><br><span class=\"line\">        at com.google.inject.AbstractModule.configure(AbstractModule.java:62)</span><br><span class=\"line\">        at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)</span><br><span class=\"line\">        at com.google.inject.spi.Elements.getElements(Elements.java:110)</span><br><span class=\"line\">        at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)</span><br><span class=\"line\">        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)</span><br><span class=\"line\">        at com.google.inject.Guice.createInjector(Guice.java:96)</span><br><span class=\"line\">        at com.google.inject.Guice.createInjector(Guice.java:73)</span><br><span class=\"line\">        at com.google.inject.Guice.createInjector(Guice.java:62)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:356)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:401)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1137)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1245)</span><br><span class=\"line\">        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1446)</span><br><span class=\"line\">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager</span><br><span class=\"line\">        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class=\"line\">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class=\"line\">        ... 36 more</span><br></pre></td></tr></table></figure>\n\n<p>*<em>解决方法： *</em>share\\hadoop\\yarn\\timelineservice 下 hadoop-yarn-server-timelineservice-3.0.3.jar copy 到share\\hadoop\\yarn目录下 </p>\n","site":{"data":{}},"excerpt":"<h1 id=\"序言\"><a href=\"#序言\" class=\"headerlink\" title=\"序言\"></a>序言</h1><p>首先劝大家在有条件情况下能用mac就用mac，再不行用linux系统，在windows上运行hadoop不是一个好主意！真的好麻烦。。。</p>","more":"<h1 id=\"准备文件\"><a href=\"#准备文件\" class=\"headerlink\" title=\"准备文件\"></a>准备文件</h1><ul>\n<li>在官网上下载hadoop的压缩包</li>\n<li>然后有个github项目专门做windows下配置的包，具体链接需要自己搜一下</li>\n</ul>\n<p>我用的是hadoop 2.10.0，然后配置文件和hadoop包都在里面，需要自己下载</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;18ZVB89xOUq43gJ7cqlZUGA </span><br><span class=\"line\">提取码：wj3v</span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"安装步骤\"><a href=\"#安装步骤\" class=\"headerlink\" title=\"安装步骤\"></a>安装步骤</h1><ul>\n<li>安装好java环境，这是基础，网上一堆教程</li>\n<li>解压缩hadoop压缩包，然后解压下载的另一个配置文件，直接拷贝覆盖即可</li>\n</ul>\n<p>这里卡了好久，因为文件路径太长经常解压后部分文件无法解压成功，这个可以参考链接：<a href=\"https://knowledge.autodesk.com/zh-hans/search-result/caas/sfdcarticles/sfdcarticles/CHS/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html\" target=\"_blank\" rel=\"noopener\">点这里</a>，最好放在盘的第一层，我就放在C:\\下面</p>\n<ul>\n<li><p>配置hadoop环境变量</p>\n<p>我的电脑-&gt;属性-&gt;高级系统设置-&gt;环境变量-&gt;系统变量</p>\n<p>新建HADOOP_HOME, 我的配置：C:\\hadoop-2.10.0\\bin</p>\n<p><img src=\"https://i.loli.net/2020/02/20/T9MjvyiPe8rcE5H.jpg\" alt=\"b146837bly1gbxf3b0kv0j20s9071dfu.jpg\"></p>\n</li>\n</ul>\n<p>​     在PATH变量中添加：%HADOOP_HOME%</p>\n<ul>\n<li><p>编辑 hadoop安装目录下 etc/hadoop/hadoop-env.cmd, 替换JAVA_HOME路径，”D:\\program files\\Java\\jdk1.8.0_171”为JAVA安装路径。</p>\n<p>set JAVA_HOME=”D:\\program files\\Java\\jdk1.8.0_171”</p>\n<p>然后编辑在hadoop根目录下创建data目录，目录中再创建两个空文件夹datanode和namenode，之后编辑 etc/hadoop/hdfs-site.xml，替换路径/hadoop-2.10.0/为你的hadoop安装根目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\"> &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class=\"line\">        &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class=\"line\">    &lt;&#x2F;property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class=\"line\">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;namenode&lt;&#x2F;value&gt;</span><br><span class=\"line\">    &lt;&#x2F;property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class=\"line\">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;datanode&lt;&#x2F;value&gt;</span><br><span class=\"line\">    &lt;&#x2F;property&gt;</span><br><span class=\"line\">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>格式化namenode</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在任意目录执行 hdfs namenode -format</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>到安装根目录下的sbin目录，执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">start-all.cmd</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://i.loli.net/2020/02/20/Lupda64fCRmJEHo.jpg\" alt=\"b146837bly1gbxfmuf908j20z50li7gm.jpg\"></p>\n<p>验证是否成功：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jps</span><br></pre></td></tr></table></figure>\n\n<p>会有以下进程在运行：</p>\n<p>NodeManager<br>DataNode<br>ResourceManager<br>NameNode</p>\n</li>\n</ul>\n<h1 id=\"问题及解决方法\"><a href=\"#问题及解决方法\" class=\"headerlink\" title=\"问题及解决方法\"></a>问题及解决方法</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;yarn&#x2F;server&#x2F;timelineservice&#x2F;collector&#x2F;TimelineCollectorManager</span><br><span class=\"line\">        at java.lang.ClassLoader.defineClass1(Native Method)</span><br><span class=\"line\">        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)</span><br><span class=\"line\">        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)</span><br><span class=\"line\">        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)</span><br><span class=\"line\">        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)</span><br><span class=\"line\">        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)</span><br><span class=\"line\">        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)</span><br><span class=\"line\">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class=\"line\">        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class=\"line\">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class=\"line\">        at java.lang.Class.getDeclaredMethods0(Native Method)</span><br><span class=\"line\">        at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)</span><br><span class=\"line\">        at java.lang.Class.getDeclaredMethods(Class.java:1975)</span><br><span class=\"line\">        at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:688)</span><br><span class=\"line\">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:380)</span><br><span class=\"line\">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:399)</span><br><span class=\"line\">        at com.google.inject.internal.BindingBuilder.toInstance(BindingBuilder.java:84)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:56)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.webapp.WebApp.configureServlets(WebApp.java:160)</span><br><span class=\"line\">        at com.google.inject.servlet.ServletModule.configure(ServletModule.java:55)</span><br><span class=\"line\">        at com.google.inject.AbstractModule.configure(AbstractModule.java:62)</span><br><span class=\"line\">        at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)</span><br><span class=\"line\">        at com.google.inject.spi.Elements.getElements(Elements.java:110)</span><br><span class=\"line\">        at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)</span><br><span class=\"line\">        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)</span><br><span class=\"line\">        at com.google.inject.Guice.createInjector(Guice.java:96)</span><br><span class=\"line\">        at com.google.inject.Guice.createInjector(Guice.java:73)</span><br><span class=\"line\">        at com.google.inject.Guice.createInjector(Guice.java:62)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:356)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:401)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1137)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1245)</span><br><span class=\"line\">        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1446)</span><br><span class=\"line\">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager</span><br><span class=\"line\">        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class=\"line\">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class=\"line\">        ... 36 more</span><br></pre></td></tr></table></figure>\n\n<p>*<em>解决方法： *</em>share\\hadoop\\yarn\\timelineservice 下 hadoop-yarn-server-timelineservice-3.0.3.jar copy 到share\\hadoop\\yarn目录下 </p>"},{"title":"linux---目录","date":"2020-02-09T14:35:00.000Z","top":true,"_content":"\nlinux系统是一种自由和开放源码的类UNIX操作系统。该操作系统的内核由林纳斯·托瓦兹在1991年10月5日首次发布，在加上用户空间的应用程序之后，成为Linux操作系统[1]。linux系统作为当前最流行的系统之一，有各种版本，如centos, ubuntu等，当前移动互联网终端之王---手机---的系统之一Android也是基于linux，可以说随处可以见到linux的身影。\n\n在大数据开发中，linux系统必然会涉及，是必备的基础技能。下面将分为以下一系列文章进行介绍：\n\n- {% post_link linux/linux-基础知识 基础知识%}\n- 常用命令\n- 文件权限\n- shell脚本\n\n\n\n> [1] 维基百科","source":"_posts/linux/Linux目录.md","raw":"---\ntitle: linux---目录\ndate: 2020-02-09 22:35:00\ntags: linux目录\ncategories: \n    - 目录\ntop: true\n---\n\nlinux系统是一种自由和开放源码的类UNIX操作系统。该操作系统的内核由林纳斯·托瓦兹在1991年10月5日首次发布，在加上用户空间的应用程序之后，成为Linux操作系统[1]。linux系统作为当前最流行的系统之一，有各种版本，如centos, ubuntu等，当前移动互联网终端之王---手机---的系统之一Android也是基于linux，可以说随处可以见到linux的身影。\n\n在大数据开发中，linux系统必然会涉及，是必备的基础技能。下面将分为以下一系列文章进行介绍：\n\n- {% post_link linux/linux-基础知识 基础知识%}\n- 常用命令\n- 文件权限\n- shell脚本\n\n\n\n> [1] 维基百科","slug":"linux/Linux目录","published":1,"updated":"2020-02-24T12:40:38.851Z","_id":"ck70g5sh000019fzk08261dji","comments":1,"layout":"post","photos":[],"link":"","content":"<p>linux系统是一种自由和开放源码的类UNIX操作系统。该操作系统的内核由林纳斯·托瓦兹在1991年10月5日首次发布，在加上用户空间的应用程序之后，成为Linux操作系统[1]。linux系统作为当前最流行的系统之一，有各种版本，如centos, ubuntu等，当前移动互联网终端之王—手机—的系统之一Android也是基于linux，可以说随处可以见到linux的身影。</p>\n<p>在大数据开发中，linux系统必然会涉及，是必备的基础技能。下面将分为以下一系列文章进行介绍：</p>\n<ul>\n<li><a href=\"/2020/02/12/linux/linux-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/\" title=\"基础知识\">基础知识</a></li>\n<li>常用命令</li>\n<li>文件权限</li>\n<li>shell脚本</li>\n</ul>\n<blockquote>\n<p>[1] 维基百科</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>linux系统是一种自由和开放源码的类UNIX操作系统。该操作系统的内核由林纳斯·托瓦兹在1991年10月5日首次发布，在加上用户空间的应用程序之后，成为Linux操作系统[1]。linux系统作为当前最流行的系统之一，有各种版本，如centos, ubuntu等，当前移动互联网终端之王—手机—的系统之一Android也是基于linux，可以说随处可以见到linux的身影。</p>\n<p>在大数据开发中，linux系统必然会涉及，是必备的基础技能。下面将分为以下一系列文章进行介绍：</p>\n<ul>\n<li><a href=\"/2020/02/12/linux/linux-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/\" title=\"基础知识\">基础知识</a></li>\n<li>常用命令</li>\n<li>文件权限</li>\n<li>shell脚本</li>\n</ul>\n<blockquote>\n<p>[1] 维基百科</p>\n</blockquote>\n"},{"title":"基础知识","date":"2020-02-11T16:21:21.000Z","_content":"\n\n\n# 计算机\n\n### 组成部分\n\n- 输入单元\n- CPU<!--more-->\n- 内存\n- 外部存储设备\n- 输出单元\n\n\n\n### 分类\n\n- 超级计算机\n- 大型计算机\n- 迷你计算机\n- 工作站\n- 微电脑\n\n\n\n### 文件大小\n\nB=>K=>M=>G=>T=>P=>E\n\n关系都是1024的倍数，如1M=1024K\n\n\n\n# 帮助\n\n因为传统linux系统只有命令行模式，所以需要使用各种命令，但是记住所有的命令和参数这是几乎不可能的，所以知道如何获取帮助非常重要。\n\n获取帮助的命令主要有三个，help, man, info。这三个得到的查询结果详细程度依次递增。\n\n**help命令**\n\nhelp 命令经常使用，可以简洁的列出命令使用方法\n\n示例：\n\n```\nhelp echo\n```\n\n\n\n![b146837bly1gbsy3grssnj21fd09qwff.jpg](https://i.loli.net/2020/02/20/jNaxOQpHEnBAWZy.jpg)\n\n","source":"_posts/linux/linux-基础知识.md","raw":"---\ntitle: 基础知识\ndate: 2020-02-12 00:21:21\ntags:\n    - Linux\n    - 基础知识\ncategories: \n    - Linux\n---\n\n\n\n# 计算机\n\n### 组成部分\n\n- 输入单元\n- CPU<!--more-->\n- 内存\n- 外部存储设备\n- 输出单元\n\n\n\n### 分类\n\n- 超级计算机\n- 大型计算机\n- 迷你计算机\n- 工作站\n- 微电脑\n\n\n\n### 文件大小\n\nB=>K=>M=>G=>T=>P=>E\n\n关系都是1024的倍数，如1M=1024K\n\n\n\n# 帮助\n\n因为传统linux系统只有命令行模式，所以需要使用各种命令，但是记住所有的命令和参数这是几乎不可能的，所以知道如何获取帮助非常重要。\n\n获取帮助的命令主要有三个，help, man, info。这三个得到的查询结果详细程度依次递增。\n\n**help命令**\n\nhelp 命令经常使用，可以简洁的列出命令使用方法\n\n示例：\n\n```\nhelp echo\n```\n\n\n\n![b146837bly1gbsy3grssnj21fd09qwff.jpg](https://i.loli.net/2020/02/20/jNaxOQpHEnBAWZy.jpg)\n\n","slug":"linux/linux-基础知识","published":1,"updated":"2020-02-20T04:34:55.133Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck70g5sh500039fzk1dgq6sek","content":"<h1 id=\"计算机\"><a href=\"#计算机\" class=\"headerlink\" title=\"计算机\"></a>计算机</h1><h3 id=\"组成部分\"><a href=\"#组成部分\" class=\"headerlink\" title=\"组成部分\"></a>组成部分</h3><ul>\n<li>输入单元</li>\n<li>CPU<a id=\"more\"></a></li>\n<li>内存</li>\n<li>外部存储设备</li>\n<li>输出单元</li>\n</ul>\n<h3 id=\"分类\"><a href=\"#分类\" class=\"headerlink\" title=\"分类\"></a>分类</h3><ul>\n<li>超级计算机</li>\n<li>大型计算机</li>\n<li>迷你计算机</li>\n<li>工作站</li>\n<li>微电脑</li>\n</ul>\n<h3 id=\"文件大小\"><a href=\"#文件大小\" class=\"headerlink\" title=\"文件大小\"></a>文件大小</h3><p>B=&gt;K=&gt;M=&gt;G=&gt;T=&gt;P=&gt;E</p>\n<p>关系都是1024的倍数，如1M=1024K</p>\n<h1 id=\"帮助\"><a href=\"#帮助\" class=\"headerlink\" title=\"帮助\"></a>帮助</h1><p>因为传统linux系统只有命令行模式，所以需要使用各种命令，但是记住所有的命令和参数这是几乎不可能的，所以知道如何获取帮助非常重要。</p>\n<p>获取帮助的命令主要有三个，help, man, info。这三个得到的查询结果详细程度依次递增。</p>\n<p><strong>help命令</strong></p>\n<p>help 命令经常使用，可以简洁的列出命令使用方法</p>\n<p>示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">help echo</span><br></pre></td></tr></table></figure>\n\n\n\n<p><img src=\"https://i.loli.net/2020/02/20/jNaxOQpHEnBAWZy.jpg\" alt=\"b146837bly1gbsy3grssnj21fd09qwff.jpg\"></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"计算机\"><a href=\"#计算机\" class=\"headerlink\" title=\"计算机\"></a>计算机</h1><h3 id=\"组成部分\"><a href=\"#组成部分\" class=\"headerlink\" title=\"组成部分\"></a>组成部分</h3><ul>\n<li>输入单元</li>\n<li>CPU","more":"</li>\n<li>内存</li>\n<li>外部存储设备</li>\n<li>输出单元</li>\n</ul>\n<h3 id=\"分类\"><a href=\"#分类\" class=\"headerlink\" title=\"分类\"></a>分类</h3><ul>\n<li>超级计算机</li>\n<li>大型计算机</li>\n<li>迷你计算机</li>\n<li>工作站</li>\n<li>微电脑</li>\n</ul>\n<h3 id=\"文件大小\"><a href=\"#文件大小\" class=\"headerlink\" title=\"文件大小\"></a>文件大小</h3><p>B=&gt;K=&gt;M=&gt;G=&gt;T=&gt;P=&gt;E</p>\n<p>关系都是1024的倍数，如1M=1024K</p>\n<h1 id=\"帮助\"><a href=\"#帮助\" class=\"headerlink\" title=\"帮助\"></a>帮助</h1><p>因为传统linux系统只有命令行模式，所以需要使用各种命令，但是记住所有的命令和参数这是几乎不可能的，所以知道如何获取帮助非常重要。</p>\n<p>获取帮助的命令主要有三个，help, man, info。这三个得到的查询结果详细程度依次递增。</p>\n<p><strong>help命令</strong></p>\n<p>help 命令经常使用，可以简洁的列出命令使用方法</p>\n<p>示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">help echo</span><br></pre></td></tr></table></figure>\n\n\n\n<p><img src=\"https://i.loli.net/2020/02/20/jNaxOQpHEnBAWZy.jpg\" alt=\"b146837bly1gbsy3grssnj21fd09qwff.jpg\"></p>"},{"title":"github个人网站替换自定义域名","date":"2020-02-24T12:29:16.000Z","_content":"\n首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。\ngithub个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。\n<!--more-->\n所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。\n\n\n### 域名申请\n经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。\n接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。\n**这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。**第三次终于申请成功了。\n\n接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。\n\n首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了\n\n![选区_017.png](https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png)\n\n替换后如上图\n\n等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了\n\n![选区_018.png](https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png)\n\n现在再进去添加两条纪录\n\n![选区_020.png](https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png)\n\n上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值\n\n```\nping xxx.github.io\n```\n\n最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。\n\n访问will21.cn，搞定！","source":"_posts/other/github个人网站更改域名.md","raw":"---\ntitle: github个人网站替换自定义域名\ndate: 2020-02-24 20:29:16\ntags: \n    - github个人网站\n    - 域名修改\ncategories: \n    - other\n---\n\n首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。\ngithub个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。\n<!--more-->\n所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。\n\n\n### 域名申请\n经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。\n接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。\n**这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。**第三次终于申请成功了。\n\n接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。\n\n首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了\n\n![选区_017.png](https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png)\n\n替换后如上图\n\n等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了\n\n![选区_018.png](https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png)\n\n现在再进去添加两条纪录\n\n![选区_020.png](https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png)\n\n上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值\n\n```\nping xxx.github.io\n```\n\n最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。\n\n访问will21.cn，搞定！","slug":"other/github个人网站更改域名","published":1,"updated":"2020-02-24T12:34:53.103Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck70g5sh900059fzkcspq9ofp","content":"<p>首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。<br>github个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。</p>\n<a id=\"more\"></a>\n<p>所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。</p>\n<h3 id=\"域名申请\"><a href=\"#域名申请\" class=\"headerlink\" title=\"域名申请\"></a>域名申请</h3><p>经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。<br>接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。<br><strong>这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。</strong>第三次终于申请成功了。</p>\n<p>接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。</p>\n<p>首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了</p>\n<p><img src=\"https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png\" alt=\"选区_017.png\"></p>\n<p>替换后如上图</p>\n<p>等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了</p>\n<p><img src=\"https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png\" alt=\"选区_018.png\"></p>\n<p>现在再进去添加两条纪录</p>\n<p><img src=\"https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png\" alt=\"选区_020.png\"></p>\n<p>上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ping xxx.github.io</span><br></pre></td></tr></table></figure>\n\n<p>最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。</p>\n<p>访问will21.cn，搞定！</p>\n","site":{"data":{}},"excerpt":"<p>首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。<br>github个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。</p>","more":"<p>所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。</p>\n<h3 id=\"域名申请\"><a href=\"#域名申请\" class=\"headerlink\" title=\"域名申请\"></a>域名申请</h3><p>经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。<br>接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。<br><strong>这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。</strong>第三次终于申请成功了。</p>\n<p>接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。</p>\n<p>首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了</p>\n<p><img src=\"https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png\" alt=\"选区_017.png\"></p>\n<p>替换后如上图</p>\n<p>等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了</p>\n<p><img src=\"https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png\" alt=\"选区_018.png\"></p>\n<p>现在再进去添加两条纪录</p>\n<p><img src=\"https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png\" alt=\"选区_020.png\"></p>\n<p>上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ping xxx.github.io</span><br></pre></td></tr></table></figure>\n\n<p>最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。</p>\n<p>访问will21.cn，搞定！</p>"},{"title":"RDD转换","date":"2019-11-02T14:43:50.000Z","_content":"\n\n\n# RDD\n\nRDD(Resilient Distributed Datasets，弹性分布式数据集)，是一个惰性计算、静态类型的分布式数据集合，是Spark实现并发计算的基础数据结构。<!--more-->\n\nRDD包含以下特性（前3个必须有，后两个可选）：\n\n1. partitions()\n\n   返回组成分布式数据集的分区对象数组。\n\n2. itearator(p, parentIters)\n\n   为每个父分区计算分区p的iteartors。\n\n3. dependencies\n\n   返回依赖对象序列。\n\n4. partitioner()---可选\n\n   若RDD有相关元素与分区信息，则返回Scala option type的分区对象。\n\n5. prefferedLocations(p)---可选\n\n   返回数据分区的存储位置信息。\n\n\n\n针对RDD的处理有丰富的操作，包括Action算子和Transformation算子。Action操作返回对象非RDD，Transformation操作返回仍然是RDD。Action算子是触发行动的算子，**Action算子数量等于Spark Job的数量**；Transformation算子会对RDD进行变换，根据父RDD和子RDD的依赖关系不同，Transformation又分为宽依赖和窄依赖。宽依赖的示意图如下：\n\n![image.png](https://i.loli.net/2019/11/03/5EpJ6In4DhlVSxj.png)\n\n窄依赖的严格定义：**each partition of the parrent RDD is used by at most one partition of the child RDD（译：每个父RDD的分区最多被一个子RDD分区使用）**。\n\n这是区分宽窄依赖最严格的定义，还有一个并不是非常严格的说法，但是便于理解：\n\n需要进行shuffle的为宽依赖，不需要的为窄依赖。\n\n**Spark Job中的Stage个数就等于宽依赖个数。**\n\n\n\n常用的Action算子有reduce,collect,count,first,take,aggregate, fold, foreach, saveAsTextFile, countByKey等; 常用的Transformation算子有map, filter, flatmap, mapPartitions, sample, union, intersection, distinct, groupByKey, countByKey, sortBy, join, coalesce, repartition, repartitionAndSortWithinPatitions, mapValues等。\n\n\n\n\n\n# Spark Job阶段划分\n\n![](https://i.loli.net/2019/11/02/Iw4YD79qiK2h6kp.jpg)\n\n","source":"_posts/spark/RDD转换.md","raw":"---\ntitle: RDD转换\ndate: 2019-11-02 22:43:50\ntags:\n    - Spark\n    - High Performance Spark\n    - RDD\ncategories: \n    - Spark\n---\n\n\n\n# RDD\n\nRDD(Resilient Distributed Datasets，弹性分布式数据集)，是一个惰性计算、静态类型的分布式数据集合，是Spark实现并发计算的基础数据结构。<!--more-->\n\nRDD包含以下特性（前3个必须有，后两个可选）：\n\n1. partitions()\n\n   返回组成分布式数据集的分区对象数组。\n\n2. itearator(p, parentIters)\n\n   为每个父分区计算分区p的iteartors。\n\n3. dependencies\n\n   返回依赖对象序列。\n\n4. partitioner()---可选\n\n   若RDD有相关元素与分区信息，则返回Scala option type的分区对象。\n\n5. prefferedLocations(p)---可选\n\n   返回数据分区的存储位置信息。\n\n\n\n针对RDD的处理有丰富的操作，包括Action算子和Transformation算子。Action操作返回对象非RDD，Transformation操作返回仍然是RDD。Action算子是触发行动的算子，**Action算子数量等于Spark Job的数量**；Transformation算子会对RDD进行变换，根据父RDD和子RDD的依赖关系不同，Transformation又分为宽依赖和窄依赖。宽依赖的示意图如下：\n\n![image.png](https://i.loli.net/2019/11/03/5EpJ6In4DhlVSxj.png)\n\n窄依赖的严格定义：**each partition of the parrent RDD is used by at most one partition of the child RDD（译：每个父RDD的分区最多被一个子RDD分区使用）**。\n\n这是区分宽窄依赖最严格的定义，还有一个并不是非常严格的说法，但是便于理解：\n\n需要进行shuffle的为宽依赖，不需要的为窄依赖。\n\n**Spark Job中的Stage个数就等于宽依赖个数。**\n\n\n\n常用的Action算子有reduce,collect,count,first,take,aggregate, fold, foreach, saveAsTextFile, countByKey等; 常用的Transformation算子有map, filter, flatmap, mapPartitions, sample, union, intersection, distinct, groupByKey, countByKey, sortBy, join, coalesce, repartition, repartitionAndSortWithinPatitions, mapValues等。\n\n\n\n\n\n# Spark Job阶段划分\n\n![](https://i.loli.net/2019/11/02/Iw4YD79qiK2h6kp.jpg)\n\n","slug":"spark/RDD转换","published":1,"updated":"2020-02-19T18:00:56.045Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck70g5shb00089fzk2msx30h4","content":"<h1 id=\"RDD\"><a href=\"#RDD\" class=\"headerlink\" title=\"RDD\"></a>RDD</h1><p>RDD(Resilient Distributed Datasets，弹性分布式数据集)，是一个惰性计算、静态类型的分布式数据集合，是Spark实现并发计算的基础数据结构。<a id=\"more\"></a></p>\n<p>RDD包含以下特性（前3个必须有，后两个可选）：</p>\n<ol>\n<li><p>partitions()</p>\n<p>返回组成分布式数据集的分区对象数组。</p>\n</li>\n<li><p>itearator(p, parentIters)</p>\n<p>为每个父分区计算分区p的iteartors。</p>\n</li>\n<li><p>dependencies</p>\n<p>返回依赖对象序列。</p>\n</li>\n<li><p>partitioner()—可选</p>\n<p>若RDD有相关元素与分区信息，则返回Scala option type的分区对象。</p>\n</li>\n<li><p>prefferedLocations(p)—可选</p>\n<p>返回数据分区的存储位置信息。</p>\n</li>\n</ol>\n<p>针对RDD的处理有丰富的操作，包括Action算子和Transformation算子。Action操作返回对象非RDD，Transformation操作返回仍然是RDD。Action算子是触发行动的算子，<strong>Action算子数量等于Spark Job的数量</strong>；Transformation算子会对RDD进行变换，根据父RDD和子RDD的依赖关系不同，Transformation又分为宽依赖和窄依赖。宽依赖的示意图如下：</p>\n<p><img src=\"https://i.loli.net/2019/11/03/5EpJ6In4DhlVSxj.png\" alt=\"image.png\"></p>\n<p>窄依赖的严格定义：<strong>each partition of the parrent RDD is used by at most one partition of the child RDD（译：每个父RDD的分区最多被一个子RDD分区使用）</strong>。</p>\n<p>这是区分宽窄依赖最严格的定义，还有一个并不是非常严格的说法，但是便于理解：</p>\n<p>需要进行shuffle的为宽依赖，不需要的为窄依赖。</p>\n<p><strong>Spark Job中的Stage个数就等于宽依赖个数。</strong></p>\n<p>常用的Action算子有reduce,collect,count,first,take,aggregate, fold, foreach, saveAsTextFile, countByKey等; 常用的Transformation算子有map, filter, flatmap, mapPartitions, sample, union, intersection, distinct, groupByKey, countByKey, sortBy, join, coalesce, repartition, repartitionAndSortWithinPatitions, mapValues等。</p>\n<h1 id=\"Spark-Job阶段划分\"><a href=\"#Spark-Job阶段划分\" class=\"headerlink\" title=\"Spark Job阶段划分\"></a>Spark Job阶段划分</h1><p><img src=\"https://i.loli.net/2019/11/02/Iw4YD79qiK2h6kp.jpg\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"RDD\"><a href=\"#RDD\" class=\"headerlink\" title=\"RDD\"></a>RDD</h1><p>RDD(Resilient Distributed Datasets，弹性分布式数据集)，是一个惰性计算、静态类型的分布式数据集合，是Spark实现并发计算的基础数据结构。","more":"</p>\n<p>RDD包含以下特性（前3个必须有，后两个可选）：</p>\n<ol>\n<li><p>partitions()</p>\n<p>返回组成分布式数据集的分区对象数组。</p>\n</li>\n<li><p>itearator(p, parentIters)</p>\n<p>为每个父分区计算分区p的iteartors。</p>\n</li>\n<li><p>dependencies</p>\n<p>返回依赖对象序列。</p>\n</li>\n<li><p>partitioner()—可选</p>\n<p>若RDD有相关元素与分区信息，则返回Scala option type的分区对象。</p>\n</li>\n<li><p>prefferedLocations(p)—可选</p>\n<p>返回数据分区的存储位置信息。</p>\n</li>\n</ol>\n<p>针对RDD的处理有丰富的操作，包括Action算子和Transformation算子。Action操作返回对象非RDD，Transformation操作返回仍然是RDD。Action算子是触发行动的算子，<strong>Action算子数量等于Spark Job的数量</strong>；Transformation算子会对RDD进行变换，根据父RDD和子RDD的依赖关系不同，Transformation又分为宽依赖和窄依赖。宽依赖的示意图如下：</p>\n<p><img src=\"https://i.loli.net/2019/11/03/5EpJ6In4DhlVSxj.png\" alt=\"image.png\"></p>\n<p>窄依赖的严格定义：<strong>each partition of the parrent RDD is used by at most one partition of the child RDD（译：每个父RDD的分区最多被一个子RDD分区使用）</strong>。</p>\n<p>这是区分宽窄依赖最严格的定义，还有一个并不是非常严格的说法，但是便于理解：</p>\n<p>需要进行shuffle的为宽依赖，不需要的为窄依赖。</p>\n<p><strong>Spark Job中的Stage个数就等于宽依赖个数。</strong></p>\n<p>常用的Action算子有reduce,collect,count,first,take,aggregate, fold, foreach, saveAsTextFile, countByKey等; 常用的Transformation算子有map, filter, flatmap, mapPartitions, sample, union, intersection, distinct, groupByKey, countByKey, sortBy, join, coalesce, repartition, repartitionAndSortWithinPatitions, mapValues等。</p>\n<h1 id=\"Spark-Job阶段划分\"><a href=\"#Spark-Job阶段划分\" class=\"headerlink\" title=\"Spark Job阶段划分\"></a>Spark Job阶段划分</h1><p><img src=\"https://i.loli.net/2019/11/02/Iw4YD79qiK2h6kp.jpg\" alt=\"\"></p>"},{"title":"spark架构","date":"2019-11-02T04:43:16.000Z","_content":"\n# Spark架构\n\n![](https://i.loli.net/2019/11/02/TLMwH9aV1xStu5Y.png)\n\n<!--more-->\n\n一个sparkApplication对应 Driver程序中一个SparkContext，而SparkContext由一系列Spark job组成。一个Worker Node可以由多个Executer组成，但是Executer不能跨Worker Node。\n\n\n\n# spark数据处理系统\n\n Spark可以在仅有单个JVM的单台机器上运行，但更常与分布式存储系统和集群管理器串联组成如下数据处理系统。分布式存储系统用来存放数据，集群管理器用来协调管理集群spark任务。Spark目前支持4种集群管理器：Standalone集群管理器，Apache Mesos，Hadoop YARN，EC2。\n\n![Spark](https://i.loli.net/2019/11/02/5ijlposeYMDScIU.png)\n\n# spark生态系统\n\n![spark生态系统](https://i.loli.net/2019/11/02/B2JjgU7hczEba1r.png)\n\n\n\n","source":"_posts/spark/spark架构.md","raw":"---\ntitle: spark架构\ndate: 2019-11-02 12:43:16\ntags: \n    - Spark\n    - High Performance Spark\n    - spark架构\ncategories: \n    - Spark\n---\n\n# Spark架构\n\n![](https://i.loli.net/2019/11/02/TLMwH9aV1xStu5Y.png)\n\n<!--more-->\n\n一个sparkApplication对应 Driver程序中一个SparkContext，而SparkContext由一系列Spark job组成。一个Worker Node可以由多个Executer组成，但是Executer不能跨Worker Node。\n\n\n\n# spark数据处理系统\n\n Spark可以在仅有单个JVM的单台机器上运行，但更常与分布式存储系统和集群管理器串联组成如下数据处理系统。分布式存储系统用来存放数据，集群管理器用来协调管理集群spark任务。Spark目前支持4种集群管理器：Standalone集群管理器，Apache Mesos，Hadoop YARN，EC2。\n\n![Spark](https://i.loli.net/2019/11/02/5ijlposeYMDScIU.png)\n\n# spark生态系统\n\n![spark生态系统](https://i.loli.net/2019/11/02/B2JjgU7hczEba1r.png)\n\n\n\n","slug":"spark/spark架构","published":1,"updated":"2020-02-19T18:00:56.045Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck70g5shd000b9fzkgtzm4gpg","content":"<h1 id=\"Spark架构\"><a href=\"#Spark架构\" class=\"headerlink\" title=\"Spark架构\"></a>Spark架构</h1><p><img src=\"https://i.loli.net/2019/11/02/TLMwH9aV1xStu5Y.png\" alt=\"\"></p>\n<a id=\"more\"></a>\n\n<p>一个sparkApplication对应 Driver程序中一个SparkContext，而SparkContext由一系列Spark job组成。一个Worker Node可以由多个Executer组成，但是Executer不能跨Worker Node。</p>\n<h1 id=\"spark数据处理系统\"><a href=\"#spark数据处理系统\" class=\"headerlink\" title=\"spark数据处理系统\"></a>spark数据处理系统</h1><p> Spark可以在仅有单个JVM的单台机器上运行，但更常与分布式存储系统和集群管理器串联组成如下数据处理系统。分布式存储系统用来存放数据，集群管理器用来协调管理集群spark任务。Spark目前支持4种集群管理器：Standalone集群管理器，Apache Mesos，Hadoop YARN，EC2。</p>\n<p><img src=\"https://i.loli.net/2019/11/02/5ijlposeYMDScIU.png\" alt=\"Spark\"></p>\n<h1 id=\"spark生态系统\"><a href=\"#spark生态系统\" class=\"headerlink\" title=\"spark生态系统\"></a>spark生态系统</h1><p><img src=\"https://i.loli.net/2019/11/02/B2JjgU7hczEba1r.png\" alt=\"spark生态系统\"></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"Spark架构\"><a href=\"#Spark架构\" class=\"headerlink\" title=\"Spark架构\"></a>Spark架构</h1><p><img src=\"https://i.loli.net/2019/11/02/TLMwH9aV1xStu5Y.png\" alt=\"\"></p>","more":"<p>一个sparkApplication对应 Driver程序中一个SparkContext，而SparkContext由一系列Spark job组成。一个Worker Node可以由多个Executer组成，但是Executer不能跨Worker Node。</p>\n<h1 id=\"spark数据处理系统\"><a href=\"#spark数据处理系统\" class=\"headerlink\" title=\"spark数据处理系统\"></a>spark数据处理系统</h1><p> Spark可以在仅有单个JVM的单台机器上运行，但更常与分布式存储系统和集群管理器串联组成如下数据处理系统。分布式存储系统用来存放数据，集群管理器用来协调管理集群spark任务。Spark目前支持4种集群管理器：Standalone集群管理器，Apache Mesos，Hadoop YARN，EC2。</p>\n<p><img src=\"https://i.loli.net/2019/11/02/5ijlposeYMDScIU.png\" alt=\"Spark\"></p>\n<h1 id=\"spark生态系统\"><a href=\"#spark生态系统\" class=\"headerlink\" title=\"spark生态系统\"></a>spark生态系统</h1><p><img src=\"https://i.loli.net/2019/11/02/B2JjgU7hczEba1r.png\" alt=\"spark生态系统\"></p>"},{"title":"Spark绪论","date":"2019-10-24T08:33:13.000Z","_content":"\n\n# 前言\n\n- 为什么会有spark\n\n  现在的计算机性能越来越高，单台机器处理M级别的数据简直小菜一碟。但是如果数据量更大，达到G、T甚至P、E级别，这时单台机器的性能可能就有点不够了。也许你会反驳，目前世界上最快的超级计算<!--more--> 机“Summit”浮点运算速度峰值可达20亿亿次/秒，这相当快了吧！确实很快，但是它的使用和维护费用都很高呀，一般人或者说公司根本负担不起。所以聪明的人类就发明了大数据工具。虽然单台机器的计算性能可能并不是很好，但是如果多台机器联合起来一起做计算，那么不就可以实现相对廉价的机器实现超大数据规模的计算吗？所以诞生了大数据工具Hadoop。Hadoop基于HDFS和Map-Reduce奠定了大数据领域的霸主地位。随后各种工具层出不穷。Spark针对Hadoop计算中慢的缺点，即每次计算结果都要读写到磁盘导致速度变慢的设计，作了如下改进：\n\n  1. 计算中间结果保存在内存中。Hadoop每次的计算结果都会放回HDFS，这种设计严重影响了计算速度。spark基于内存的设计则改进了这一点。\n  2. 惰性计算。Spark使用有向无环图（DAG）调度机制，遇到action操作算子才进行实际计算，所有就有了优化空间。\n\n  > 可参考对比：https://www.zhihu.com/question/26568496\n\n- Spark是什么\n\n  官方定义：**Apache Spark™** is a unified analytics engine for large-scale data processing。翻译过来就是：用于大规模数据处理的分析引擎。再直白一点就是说：spark是用于快速处理大量数据的工具。\n\n  <img src=\"https://ericfu.me/images/2018/06/spark-banner.png\" width=\"700\" hegiht=\"113\" align=center />\n\n\n\n# 和其他工具对比\n\n> 引用自：https://www.boxuegu.com/news/458.html\n\n\n\n- **Hadoop框架**\n\n  提起大数据，第一个想起的肯定是Hadoop，因为Hadoop是目前世界上应用最广泛的大数据工具，它凭借极高的容错率和极低的硬件价格，在大数据市场上风生水起。Hadoop还是第一个在开源社区上引发高度关注的批处理框架，他提出的Map和Reduce的计算模式简洁而优雅。迄今为止，Hadoop已经成为了一个广阔的生态圈，实现了大量算法和组件。由于Hadoop的计算任务需要在集群的多个节点上多次读写，因此在速度上会稍显劣势，但是其吞吐量也同样是其他框架所不能匹敌的。\n\n\n\n- **Storm框架**\n  与Hadoop的批处理模式不同，Storm采用的是流计算框架，由Twitter开源并且托管在GitHub上。与Hadoop类似的是，Storm也提出了两个计算角色，分别为Spout和Bolt。如果说Hadoop是水桶，只能一桶一桶的去井里扛，那么Storm就是水龙头，只要打开就可以源源不断的出水。Storm支持的语言也比较多，Java、Ruby、Python等语言都能很好的支持。由于Storm是流计算框架，因此使用的是内存，延迟上有极大的优势，但是Storm不会持久化数据。\n\n \n\n- **Samza框架**\n  Smaza也是一种流计算框架，但他目前只支持JVM语言，灵活度上略显不足，并且Samza必须和Kafka共同使用。但是响应的，其也继承了Kafka的低延时、分区、避免回压等优势。对于已经有Hadoop+Kafka工作环境的团队来说，Samza是一个不错的选择，并且Samza在多个团队使用的时候能体现良好的性能。\n\n \n\n- **Spark框架**\n  Spark属于前两种框架形式的集合体，是一种混合式的计算框架。它既有自带的实时流处理工具，也可以和Hadoop集成，代替其中的MapReduce，甚至Spark还可以单独拿出来部署集群，但是还得借助HDFS等分布式存储系统。Spark的强大之处在于其运算速度，与Storm类似，Spark也是基于内存的，并且在内存满负载的时候，硬盘也能运算，运算结果表示，Spark的速度大约为Hadoop的一百倍，并且其成本可能比Hadoop更低。但是Spark目前还没有像Hadoop哪有拥有上万级别的集群，因此现阶段的Spark和Hadoop搭配起来使用更加合适。\n\n \n\n- **Flink框架**\n  Flink也是一种混合式的计算框架，但是在设计初始，Fink的侧重点在于处理流式数据，这与Spark的设计初衷恰恰相反，而在市场需求的驱使下，两者都在朝着更多的兼容性发展。Flink目前不是很成熟，更多情况下Flink还是起到一个借鉴的作用。\n\n\n","source":"_posts/spark/spark.md","raw":"---\ntitle: Spark绪论\ndate: 2019-10-24 16:33:13\ntags: \n    - Spark\n    - High Performance Spark\n    - 大数据工具\ncategories: \n    - Spark\n---\n\n\n# 前言\n\n- 为什么会有spark\n\n  现在的计算机性能越来越高，单台机器处理M级别的数据简直小菜一碟。但是如果数据量更大，达到G、T甚至P、E级别，这时单台机器的性能可能就有点不够了。也许你会反驳，目前世界上最快的超级计算<!--more--> 机“Summit”浮点运算速度峰值可达20亿亿次/秒，这相当快了吧！确实很快，但是它的使用和维护费用都很高呀，一般人或者说公司根本负担不起。所以聪明的人类就发明了大数据工具。虽然单台机器的计算性能可能并不是很好，但是如果多台机器联合起来一起做计算，那么不就可以实现相对廉价的机器实现超大数据规模的计算吗？所以诞生了大数据工具Hadoop。Hadoop基于HDFS和Map-Reduce奠定了大数据领域的霸主地位。随后各种工具层出不穷。Spark针对Hadoop计算中慢的缺点，即每次计算结果都要读写到磁盘导致速度变慢的设计，作了如下改进：\n\n  1. 计算中间结果保存在内存中。Hadoop每次的计算结果都会放回HDFS，这种设计严重影响了计算速度。spark基于内存的设计则改进了这一点。\n  2. 惰性计算。Spark使用有向无环图（DAG）调度机制，遇到action操作算子才进行实际计算，所有就有了优化空间。\n\n  > 可参考对比：https://www.zhihu.com/question/26568496\n\n- Spark是什么\n\n  官方定义：**Apache Spark™** is a unified analytics engine for large-scale data processing。翻译过来就是：用于大规模数据处理的分析引擎。再直白一点就是说：spark是用于快速处理大量数据的工具。\n\n  <img src=\"https://ericfu.me/images/2018/06/spark-banner.png\" width=\"700\" hegiht=\"113\" align=center />\n\n\n\n# 和其他工具对比\n\n> 引用自：https://www.boxuegu.com/news/458.html\n\n\n\n- **Hadoop框架**\n\n  提起大数据，第一个想起的肯定是Hadoop，因为Hadoop是目前世界上应用最广泛的大数据工具，它凭借极高的容错率和极低的硬件价格，在大数据市场上风生水起。Hadoop还是第一个在开源社区上引发高度关注的批处理框架，他提出的Map和Reduce的计算模式简洁而优雅。迄今为止，Hadoop已经成为了一个广阔的生态圈，实现了大量算法和组件。由于Hadoop的计算任务需要在集群的多个节点上多次读写，因此在速度上会稍显劣势，但是其吞吐量也同样是其他框架所不能匹敌的。\n\n\n\n- **Storm框架**\n  与Hadoop的批处理模式不同，Storm采用的是流计算框架，由Twitter开源并且托管在GitHub上。与Hadoop类似的是，Storm也提出了两个计算角色，分别为Spout和Bolt。如果说Hadoop是水桶，只能一桶一桶的去井里扛，那么Storm就是水龙头，只要打开就可以源源不断的出水。Storm支持的语言也比较多，Java、Ruby、Python等语言都能很好的支持。由于Storm是流计算框架，因此使用的是内存，延迟上有极大的优势，但是Storm不会持久化数据。\n\n \n\n- **Samza框架**\n  Smaza也是一种流计算框架，但他目前只支持JVM语言，灵活度上略显不足，并且Samza必须和Kafka共同使用。但是响应的，其也继承了Kafka的低延时、分区、避免回压等优势。对于已经有Hadoop+Kafka工作环境的团队来说，Samza是一个不错的选择，并且Samza在多个团队使用的时候能体现良好的性能。\n\n \n\n- **Spark框架**\n  Spark属于前两种框架形式的集合体，是一种混合式的计算框架。它既有自带的实时流处理工具，也可以和Hadoop集成，代替其中的MapReduce，甚至Spark还可以单独拿出来部署集群，但是还得借助HDFS等分布式存储系统。Spark的强大之处在于其运算速度，与Storm类似，Spark也是基于内存的，并且在内存满负载的时候，硬盘也能运算，运算结果表示，Spark的速度大约为Hadoop的一百倍，并且其成本可能比Hadoop更低。但是Spark目前还没有像Hadoop哪有拥有上万级别的集群，因此现阶段的Spark和Hadoop搭配起来使用更加合适。\n\n \n\n- **Flink框架**\n  Flink也是一种混合式的计算框架，但是在设计初始，Fink的侧重点在于处理流式数据，这与Spark的设计初衷恰恰相反，而在市场需求的驱使下，两者都在朝着更多的兼容性发展。Flink目前不是很成熟，更多情况下Flink还是起到一个借鉴的作用。\n\n\n","slug":"spark/spark","published":1,"updated":"2020-02-19T18:00:56.045Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck70g5she000e9fzk5vwna4bw","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><ul>\n<li><p>为什么会有spark</p>\n<p>现在的计算机性能越来越高，单台机器处理M级别的数据简直小菜一碟。但是如果数据量更大，达到G、T甚至P、E级别，这时单台机器的性能可能就有点不够了。也许你会反驳，目前世界上最快的超级计算<a id=\"more\"></a> 机“Summit”浮点运算速度峰值可达20亿亿次/秒，这相当快了吧！确实很快，但是它的使用和维护费用都很高呀，一般人或者说公司根本负担不起。所以聪明的人类就发明了大数据工具。虽然单台机器的计算性能可能并不是很好，但是如果多台机器联合起来一起做计算，那么不就可以实现相对廉价的机器实现超大数据规模的计算吗？所以诞生了大数据工具Hadoop。Hadoop基于HDFS和Map-Reduce奠定了大数据领域的霸主地位。随后各种工具层出不穷。Spark针对Hadoop计算中慢的缺点，即每次计算结果都要读写到磁盘导致速度变慢的设计，作了如下改进：</p>\n<ol>\n<li>计算中间结果保存在内存中。Hadoop每次的计算结果都会放回HDFS，这种设计严重影响了计算速度。spark基于内存的设计则改进了这一点。</li>\n<li>惰性计算。Spark使用有向无环图（DAG）调度机制，遇到action操作算子才进行实际计算，所有就有了优化空间。</li>\n</ol>\n<blockquote>\n<p>可参考对比：<a href=\"https://www.zhihu.com/question/26568496\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/26568496</a></p>\n</blockquote>\n</li>\n<li><p>Spark是什么</p>\n<p>官方定义：<strong>Apache Spark™</strong> is a unified analytics engine for large-scale data processing。翻译过来就是：用于大规模数据处理的分析引擎。再直白一点就是说：spark是用于快速处理大量数据的工具。</p>\n<img src=\"https://ericfu.me/images/2018/06/spark-banner.png\" width=\"700\" hegiht=\"113\" align=center />\n\n\n\n</li>\n</ul>\n<h1 id=\"和其他工具对比\"><a href=\"#和其他工具对比\" class=\"headerlink\" title=\"和其他工具对比\"></a>和其他工具对比</h1><blockquote>\n<p>引用自：<a href=\"https://www.boxuegu.com/news/458.html\" target=\"_blank\" rel=\"noopener\">https://www.boxuegu.com/news/458.html</a></p>\n</blockquote>\n<ul>\n<li><p><strong>Hadoop框架</strong></p>\n<p>提起大数据，第一个想起的肯定是Hadoop，因为Hadoop是目前世界上应用最广泛的大数据工具，它凭借极高的容错率和极低的硬件价格，在大数据市场上风生水起。Hadoop还是第一个在开源社区上引发高度关注的批处理框架，他提出的Map和Reduce的计算模式简洁而优雅。迄今为止，Hadoop已经成为了一个广阔的生态圈，实现了大量算法和组件。由于Hadoop的计算任务需要在集群的多个节点上多次读写，因此在速度上会稍显劣势，但是其吞吐量也同样是其他框架所不能匹敌的。</p>\n</li>\n</ul>\n<ul>\n<li><strong>Storm框架</strong><br>与Hadoop的批处理模式不同，Storm采用的是流计算框架，由Twitter开源并且托管在GitHub上。与Hadoop类似的是，Storm也提出了两个计算角色，分别为Spout和Bolt。如果说Hadoop是水桶，只能一桶一桶的去井里扛，那么Storm就是水龙头，只要打开就可以源源不断的出水。Storm支持的语言也比较多，Java、Ruby、Python等语言都能很好的支持。由于Storm是流计算框架，因此使用的是内存，延迟上有极大的优势，但是Storm不会持久化数据。</li>\n</ul>\n<ul>\n<li><strong>Samza框架</strong><br>Smaza也是一种流计算框架，但他目前只支持JVM语言，灵活度上略显不足，并且Samza必须和Kafka共同使用。但是响应的，其也继承了Kafka的低延时、分区、避免回压等优势。对于已经有Hadoop+Kafka工作环境的团队来说，Samza是一个不错的选择，并且Samza在多个团队使用的时候能体现良好的性能。</li>\n</ul>\n<ul>\n<li><strong>Spark框架</strong><br>Spark属于前两种框架形式的集合体，是一种混合式的计算框架。它既有自带的实时流处理工具，也可以和Hadoop集成，代替其中的MapReduce，甚至Spark还可以单独拿出来部署集群，但是还得借助HDFS等分布式存储系统。Spark的强大之处在于其运算速度，与Storm类似，Spark也是基于内存的，并且在内存满负载的时候，硬盘也能运算，运算结果表示，Spark的速度大约为Hadoop的一百倍，并且其成本可能比Hadoop更低。但是Spark目前还没有像Hadoop哪有拥有上万级别的集群，因此现阶段的Spark和Hadoop搭配起来使用更加合适。</li>\n</ul>\n<ul>\n<li><strong>Flink框架</strong><br>Flink也是一种混合式的计算框架，但是在设计初始，Fink的侧重点在于处理流式数据，这与Spark的设计初衷恰恰相反，而在市场需求的驱使下，两者都在朝着更多的兼容性发展。Flink目前不是很成熟，更多情况下Flink还是起到一个借鉴的作用。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><ul>\n<li><p>为什么会有spark</p>\n<p>现在的计算机性能越来越高，单台机器处理M级别的数据简直小菜一碟。但是如果数据量更大，达到G、T甚至P、E级别，这时单台机器的性能可能就有点不够了。也许你会反驳，目前世界上最快的超级计算","more":"机“Summit”浮点运算速度峰值可达20亿亿次/秒，这相当快了吧！确实很快，但是它的使用和维护费用都很高呀，一般人或者说公司根本负担不起。所以聪明的人类就发明了大数据工具。虽然单台机器的计算性能可能并不是很好，但是如果多台机器联合起来一起做计算，那么不就可以实现相对廉价的机器实现超大数据规模的计算吗？所以诞生了大数据工具Hadoop。Hadoop基于HDFS和Map-Reduce奠定了大数据领域的霸主地位。随后各种工具层出不穷。Spark针对Hadoop计算中慢的缺点，即每次计算结果都要读写到磁盘导致速度变慢的设计，作了如下改进：</p>\n<ol>\n<li>计算中间结果保存在内存中。Hadoop每次的计算结果都会放回HDFS，这种设计严重影响了计算速度。spark基于内存的设计则改进了这一点。</li>\n<li>惰性计算。Spark使用有向无环图（DAG）调度机制，遇到action操作算子才进行实际计算，所有就有了优化空间。</li>\n</ol>\n<blockquote>\n<p>可参考对比：<a href=\"https://www.zhihu.com/question/26568496\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/26568496</a></p>\n</blockquote>\n</li>\n<li><p>Spark是什么</p>\n<p>官方定义：<strong>Apache Spark™</strong> is a unified analytics engine for large-scale data processing。翻译过来就是：用于大规模数据处理的分析引擎。再直白一点就是说：spark是用于快速处理大量数据的工具。</p>\n<img src=\"https://ericfu.me/images/2018/06/spark-banner.png\" width=\"700\" hegiht=\"113\" align=center />\n\n\n\n</li>\n</ul>\n<h1 id=\"和其他工具对比\"><a href=\"#和其他工具对比\" class=\"headerlink\" title=\"和其他工具对比\"></a>和其他工具对比</h1><blockquote>\n<p>引用自：<a href=\"https://www.boxuegu.com/news/458.html\" target=\"_blank\" rel=\"noopener\">https://www.boxuegu.com/news/458.html</a></p>\n</blockquote>\n<ul>\n<li><p><strong>Hadoop框架</strong></p>\n<p>提起大数据，第一个想起的肯定是Hadoop，因为Hadoop是目前世界上应用最广泛的大数据工具，它凭借极高的容错率和极低的硬件价格，在大数据市场上风生水起。Hadoop还是第一个在开源社区上引发高度关注的批处理框架，他提出的Map和Reduce的计算模式简洁而优雅。迄今为止，Hadoop已经成为了一个广阔的生态圈，实现了大量算法和组件。由于Hadoop的计算任务需要在集群的多个节点上多次读写，因此在速度上会稍显劣势，但是其吞吐量也同样是其他框架所不能匹敌的。</p>\n</li>\n</ul>\n<ul>\n<li><strong>Storm框架</strong><br>与Hadoop的批处理模式不同，Storm采用的是流计算框架，由Twitter开源并且托管在GitHub上。与Hadoop类似的是，Storm也提出了两个计算角色，分别为Spout和Bolt。如果说Hadoop是水桶，只能一桶一桶的去井里扛，那么Storm就是水龙头，只要打开就可以源源不断的出水。Storm支持的语言也比较多，Java、Ruby、Python等语言都能很好的支持。由于Storm是流计算框架，因此使用的是内存，延迟上有极大的优势，但是Storm不会持久化数据。</li>\n</ul>\n<ul>\n<li><strong>Samza框架</strong><br>Smaza也是一种流计算框架，但他目前只支持JVM语言，灵活度上略显不足，并且Samza必须和Kafka共同使用。但是响应的，其也继承了Kafka的低延时、分区、避免回压等优势。对于已经有Hadoop+Kafka工作环境的团队来说，Samza是一个不错的选择，并且Samza在多个团队使用的时候能体现良好的性能。</li>\n</ul>\n<ul>\n<li><strong>Spark框架</strong><br>Spark属于前两种框架形式的集合体，是一种混合式的计算框架。它既有自带的实时流处理工具，也可以和Hadoop集成，代替其中的MapReduce，甚至Spark还可以单独拿出来部署集群，但是还得借助HDFS等分布式存储系统。Spark的强大之处在于其运算速度，与Storm类似，Spark也是基于内存的，并且在内存满负载的时候，硬盘也能运算，运算结果表示，Spark的速度大约为Hadoop的一百倍，并且其成本可能比Hadoop更低。但是Spark目前还没有像Hadoop哪有拥有上万级别的集群，因此现阶段的Spark和Hadoop搭配起来使用更加合适。</li>\n</ul>\n<ul>\n<li><strong>Flink框架</strong><br>Flink也是一种混合式的计算框架，但是在设计初始，Fink的侧重点在于处理流式数据，这与Spark的设计初衷恰恰相反，而在市场需求的驱使下，两者都在朝着更多的兼容性发展。Flink目前不是很成熟，更多情况下Flink还是起到一个借鉴的作用。</li>\n</ul>"},{"title":"spark---目录","date":"2019-11-02T04:50:06.000Z","top":true,"_content":"\nSpark作为一个开源大数据分析引擎，分布式、计算速度快是其显著的优势，特别适用于机器学习（ML）等计算中需要迭代的算法。下面将主要针对Spark Core部分，分为以下一系列文章进行介绍：\n\n- {% post_link spark/spark Spark绪论%}\n- {% post_link spark/spark架构 Spark架构%}\n- {% post_link spark/RDD转换 RDD转换%}\n- 键值对处理\n\n\n\n> 参考书目：high performance spark, Holden karau & Rachel Warren","source":"_posts/spark/spark目录.md","raw":"---\ntitle: spark---目录\ndate: 2019-11-02 12:50:06\ntags: spark目录\ncategories: \n    - 目录\ntop: true\n---\n\nSpark作为一个开源大数据分析引擎，分布式、计算速度快是其显著的优势，特别适用于机器学习（ML）等计算中需要迭代的算法。下面将主要针对Spark Core部分，分为以下一系列文章进行介绍：\n\n- {% post_link spark/spark Spark绪论%}\n- {% post_link spark/spark架构 Spark架构%}\n- {% post_link spark/RDD转换 RDD转换%}\n- 键值对处理\n\n\n\n> 参考书目：high performance spark, Holden karau & Rachel Warren","slug":"spark/spark目录","published":1,"updated":"2020-02-24T12:41:40.095Z","_id":"ck70g5shg000h9fzkbjiabzop","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Spark作为一个开源大数据分析引擎，分布式、计算速度快是其显著的优势，特别适用于机器学习（ML）等计算中需要迭代的算法。下面将主要针对Spark Core部分，分为以下一系列文章进行介绍：</p>\n<ul>\n<li><a href=\"/2019/10/24/spark/spark/\" title=\"Spark绪论\">Spark绪论</a></li>\n<li><a href=\"/2019/11/02/spark/spark%E6%9E%B6%E6%9E%84/\" title=\"Spark架构\">Spark架构</a></li>\n<li><a href=\"/2019/11/02/spark/RDD%E8%BD%AC%E6%8D%A2/\" title=\"RDD转换\">RDD转换</a></li>\n<li>键值对处理</li>\n</ul>\n<blockquote>\n<p>参考书目：high performance spark, Holden karau &amp; Rachel Warren</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>Spark作为一个开源大数据分析引擎，分布式、计算速度快是其显著的优势，特别适用于机器学习（ML）等计算中需要迭代的算法。下面将主要针对Spark Core部分，分为以下一系列文章进行介绍：</p>\n<ul>\n<li><a href=\"/2019/10/24/spark/spark/\" title=\"Spark绪论\">Spark绪论</a></li>\n<li><a href=\"/2019/11/02/spark/spark%E6%9E%B6%E6%9E%84/\" title=\"Spark架构\">Spark架构</a></li>\n<li><a href=\"/2019/11/02/spark/RDD%E8%BD%AC%E6%8D%A2/\" title=\"RDD转换\">RDD转换</a></li>\n<li>键值对处理</li>\n</ul>\n<blockquote>\n<p>参考书目：high performance spark, Holden karau &amp; Rachel Warren</p>\n</blockquote>\n"},{"title":"github个人网站替换自定义域名","date":"2020-02-24T12:29:16.000Z","_content":"\n首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。\ngithub个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。\n<!--more-->\n所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。\n\n\n### 域名申请\n经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。\n接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。\n**这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。**第三次终于申请成功了。\n\n接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。\n\n首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了\n\n![选区_017.png](https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png)\n\n替换后如上图\n\n等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了\n\n![选区_018.png](https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png)\n\n现在再进去添加两条纪录\n\n![选区_020.png](https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png)\n\n上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值\n\n```\nping xxx.github.io\n```\n\n最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。\n\n访问will21.cn，搞定！","source":"_posts/github个人网站更改域名.md","raw":"---\ntitle: github个人网站替换自定义域名\ndate: 2020-02-24 20:29:16\ntags: \n    - github个人网站\n    - 域名修改\ncategories: \n    - other\n---\n\n首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。\ngithub个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。\n<!--more-->\n所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。\n\n\n### 域名申请\n经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。\n接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。\n**这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。**第三次终于申请成功了。\n\n接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。\n\n首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了\n\n![选区_017.png](https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png)\n\n替换后如上图\n\n等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了\n\n![选区_018.png](https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png)\n\n现在再进去添加两条纪录\n\n![选区_020.png](https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png)\n\n上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值\n\n```\nping xxx.github.io\n```\n\n最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。\n\n访问will21.cn，搞定！","slug":"github个人网站更改域名","published":1,"updated":"2020-02-24T12:40:12.044Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck70g8u7c0000gozkdeaybm29","content":"<p>首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。<br>github个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。</p>\n<a id=\"more\"></a>\n<p>所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。</p>\n<h3 id=\"域名申请\"><a href=\"#域名申请\" class=\"headerlink\" title=\"域名申请\"></a>域名申请</h3><p>经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。<br>接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。<br><strong>这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。</strong>第三次终于申请成功了。</p>\n<p>接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。</p>\n<p>首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了</p>\n<p><img src=\"https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png\" alt=\"选区_017.png\"></p>\n<p>替换后如上图</p>\n<p>等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了</p>\n<p><img src=\"https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png\" alt=\"选区_018.png\"></p>\n<p>现在再进去添加两条纪录</p>\n<p><img src=\"https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png\" alt=\"选区_020.png\"></p>\n<p>上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ping xxx.github.io</span><br></pre></td></tr></table></figure>\n\n<p>最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。</p>\n<p>访问will21.cn，搞定！</p>\n","site":{"data":{}},"excerpt":"<p>首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。<br>github个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。</p>","more":"<p>所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。</p>\n<h3 id=\"域名申请\"><a href=\"#域名申请\" class=\"headerlink\" title=\"域名申请\"></a>域名申请</h3><p>经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。<br>接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。<br><strong>这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。</strong>第三次终于申请成功了。</p>\n<p>接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。</p>\n<p>首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了</p>\n<p><img src=\"https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png\" alt=\"选区_017.png\"></p>\n<p>替换后如上图</p>\n<p>等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了</p>\n<p><img src=\"https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png\" alt=\"选区_018.png\"></p>\n<p>现在再进去添加两条纪录</p>\n<p><img src=\"https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png\" alt=\"选区_020.png\"></p>\n<p>上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ping xxx.github.io</span><br></pre></td></tr></table></figure>\n\n<p>最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。</p>\n<p>访问will21.cn，搞定！</p>"}],"PostAsset":[],"PostCategory":[{"post_id":"ck70g2rlw00003ozk8v1i3nrh","category_id":"ck6vpalj60001auzk3ywm64b7","_id":"ck70g2rm300043ozkeltk8dmn"},{"post_id":"ck70g2rm000013ozkfd9napoe","category_id":"ck6vpalj60001auzk3ywm64b7","_id":"ck70g2rm300063ozkf2mb9r31"},{"post_id":"ck70g5sgp00009fzka8o7bvri","category_id":"ck6tkirg0000791zkcru027tj","_id":"ck70g5shb00069fzk6baxff0u"},{"post_id":"ck70g5sh000019fzk08261dji","category_id":"ck6tkirg4000c91zk95undmh3","_id":"ck70g5shc00099fzk4ybm2tbq"},{"post_id":"ck70g5sh500039fzk1dgq6sek","category_id":"ck6tkirg6000g91zk9rlscwa5","_id":"ck70g5she000c9fzk8hfkal0l"},{"post_id":"ck70g5sh900059fzkcspq9ofp","category_id":"ck70fofqw0001plzk4u30dzsf","_id":"ck70g5shf000f9fzk95012jub"},{"post_id":"ck70g5shb00089fzk2msx30h4","category_id":"ck6tkirfv000291zkegcu7gf8","_id":"ck70g5shg000i9fzk1224cfaa"},{"post_id":"ck70g5shd000b9fzkgtzm4gpg","category_id":"ck6tkirfv000291zkegcu7gf8","_id":"ck70g5shh000k9fzk2pooeq57"},{"post_id":"ck70g5she000e9fzk5vwna4bw","category_id":"ck6tkirfv000291zkegcu7gf8","_id":"ck70g5shh000m9fzk1yub7rkh"},{"post_id":"ck70g5shg000h9fzkbjiabzop","category_id":"ck6tkirg4000c91zk95undmh3","_id":"ck70g5shi000o9fzk31hub67c"},{"post_id":"ck70g8u7c0000gozkdeaybm29","category_id":"ck70fofqw0001plzk4u30dzsf","_id":"ck70g8u7q0003gozk0jvxgbwo"}],"PostTag":[{"post_id":"ck70g2rlw00003ozk8v1i3nrh","tag_id":"ck6vpalj80002auzk00aw4wt2","_id":"ck70g2rm200023ozkcbmc2gch"},{"post_id":"ck70g2rlw00003ozk8v1i3nrh","tag_id":"ck6vpalja0003auzk85uxescl","_id":"ck70g2rm300033ozk5zg8cgbu"},{"post_id":"ck70g2rlw00003ozk8v1i3nrh","tag_id":"ck6vpaljb0005auzk80gha0fb","_id":"ck70g2rm300053ozk83h851qr"},{"post_id":"ck70g2rm000013ozkfd9napoe","tag_id":"ck6vpalj80002auzk00aw4wt2","_id":"ck70g2rm300073ozk9olhf8jk"},{"post_id":"ck70g2rm000013ozkfd9napoe","tag_id":"ck6x1c28f0001ztzk1y4oh5xf","_id":"ck70g2rm400083ozk6so1bpki"},{"post_id":"ck70g2rm000013ozkfd9napoe","tag_id":"ck6x1c28r0003ztzkajm60x2c","_id":"ck70g2rm400093ozk8gl921vv"},{"post_id":"ck70g5sgp00009fzka8o7bvri","tag_id":"ck6tkirg6000h91zk61o6gjxx","_id":"ck70g5sh500029fzk510mam86"},{"post_id":"ck70g5sgp00009fzka8o7bvri","tag_id":"ck6tkirg7000n91zk5d4i0qmo","_id":"ck70g5sh800049fzkcy34aine"},{"post_id":"ck70g5sgp00009fzka8o7bvri","tag_id":"ck6tkirg9000q91zkcslv2n2f","_id":"ck70g5shb00079fzk1fdw66ig"},{"post_id":"ck70g5sh000019fzk08261dji","tag_id":"ck6tkirg9000s91zkfu0v9pa3","_id":"ck70g5shd000a9fzkccxj13de"},{"post_id":"ck70g5sh500039fzk1dgq6sek","tag_id":"ck6tkirga000v91zkezhs85qk","_id":"ck70g5she000d9fzk2wgtf4v3"},{"post_id":"ck70g5sh500039fzk1dgq6sek","tag_id":"ck6tkirga000y91zk1piedqau","_id":"ck70g5shg000g9fzk614e9bmi"},{"post_id":"ck70g5sh900059fzkcspq9ofp","tag_id":"ck70fofqz0002plzkewr16i2i","_id":"ck70g5shh000j9fzk5x5perxs"},{"post_id":"ck70g5sh900059fzkcspq9ofp","tag_id":"ck70fofr00003plzk5spb4adq","_id":"ck70g5shh000l9fzk0lxd6fa3"},{"post_id":"ck70g5shb00089fzk2msx30h4","tag_id":"ck6tkirfw000391zk29705yp5","_id":"ck70g5shi000n9fzkaz6cds3t"},{"post_id":"ck70g5shb00089fzk2msx30h4","tag_id":"ck6tkirg1000891zk8omt4u5q","_id":"ck70g5shi000p9fzk1smhfrn2"},{"post_id":"ck70g5shb00089fzk2msx30h4","tag_id":"ck6tkirg4000d91zk7alc4mxw","_id":"ck70g5shi000q9fzk9ztk6vrk"},{"post_id":"ck70g5shd000b9fzkgtzm4gpg","tag_id":"ck6tkirfw000391zk29705yp5","_id":"ck70g5shi000r9fzk3bov0fsj"},{"post_id":"ck70g5shd000b9fzkgtzm4gpg","tag_id":"ck6tkirg1000891zk8omt4u5q","_id":"ck70g5shi000s9fzkcrmh0k4a"},{"post_id":"ck70g5shd000b9fzkgtzm4gpg","tag_id":"ck6tkirgd001391zk0k4jbdgz","_id":"ck70g5shi000t9fzk7rph57sn"},{"post_id":"ck70g5she000e9fzk5vwna4bw","tag_id":"ck6tkirfw000391zk29705yp5","_id":"ck70g5shi000u9fzk5kqoe5tk"},{"post_id":"ck70g5she000e9fzk5vwna4bw","tag_id":"ck6tkirg1000891zk8omt4u5q","_id":"ck70g5shi000v9fzk37z18ff0"},{"post_id":"ck70g5she000e9fzk5vwna4bw","tag_id":"ck6tkirgb001291zk4bz667a6","_id":"ck70g5shi000w9fzkedy2cgok"},{"post_id":"ck70g5shg000h9fzkbjiabzop","tag_id":"ck6tkirge001691zk4jqu91tu","_id":"ck70g5shj000x9fzkc2o27g06"},{"post_id":"ck70g8u7c0000gozkdeaybm29","tag_id":"ck70fofqz0002plzkewr16i2i","_id":"ck70g8u7p0001gozk5x0retoa"},{"post_id":"ck70g8u7c0000gozkdeaybm29","tag_id":"ck70fofr00003plzk5spb4adq","_id":"ck70g8u7q0002gozk212p4oz6"}],"Tag":[{"name":"Spark","_id":"ck6tkirfw000391zk29705yp5"},{"name":"High Performance Spark","_id":"ck6tkirg1000891zk8omt4u5q"},{"name":"RDD","_id":"ck6tkirg4000d91zk7alc4mxw"},{"name":"hadoop","_id":"ck6tkirg6000h91zk61o6gjxx"},{"name":"hadoop安装","_id":"ck6tkirg7000n91zk5d4i0qmo"},{"name":"windows 10","_id":"ck6tkirg9000q91zkcslv2n2f"},{"name":"linux目录","_id":"ck6tkirg9000s91zkfu0v9pa3"},{"name":"Linux","_id":"ck6tkirga000v91zkezhs85qk"},{"name":"基础知识","_id":"ck6tkirga000y91zk1piedqau"},{"name":"大数据工具","_id":"ck6tkirgb001291zk4bz667a6"},{"name":"spark架构","_id":"ck6tkirgd001391zk0k4jbdgz"},{"name":"spark目录","_id":"ck6tkirge001691zk4jqu91tu"},{"name":"hive","_id":"ck6vpalj80002auzk00aw4wt2"},{"name":"源码","_id":"ck6vpalja0003auzk85uxescl"},{"name":"开发","_id":"ck6vpaljb0005auzk80gha0fb"},{"name":"udf","_id":"ck6x1c28f0001ztzk1y4oh5xf"},{"name":"udaf","_id":"ck6x1c28r0003ztzkajm60x2c"},{"name":"github个人网站","_id":"ck70fofqz0002plzkewr16i2i"},{"name":"域名修改","_id":"ck70fofr00003plzk5spb4adq"}]}}
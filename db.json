{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/anime.min.js","path":"lib/anime.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"9e50bb7377d3775cf2cac9f84f26e5baa4aedca4","modified":1582543112592},{"_id":"themes/next/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1583257056325},{"_id":"themes/next/.editorconfig","hash":"8570735a8d8d034a3a175afd1dd40b39140b3e6a","modified":1583257056321},{"_id":"themes/next/.gitattributes","hash":"a54f902957d49356376b59287b894b1a3d7a003f","modified":1583257056325},{"_id":"themes/next/.gitignore","hash":"7b68ca7a46104cf9aa84ec0541a4856ab1836eca","modified":1583257056325},{"_id":"themes/next/.stylintrc","hash":"2cf4d637b56d8eb423f59656a11f6403aa90f550","modified":1583257056325},{"_id":"themes/next/LICENSE.md","hash":"18144d8ed58c75af66cb419d54f3f63374cd5c5b","modified":1583257056325},{"_id":"themes/next/.travis.yml","hash":"ecca3b919a5b15886e3eca58aa84aafc395590da","modified":1583257056325},{"_id":"themes/next/README.md","hash":"d3035c6961280c1b4afb3a07661f5a635ce1eaff","modified":1583257056325},{"_id":"themes/next/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1583257056325},{"_id":"themes/next/_config.yml","hash":"c05d8d5cf7eed641090a5aa10de366d63e613028","modified":1583258213852},{"_id":"themes/next/gulpfile.js","hash":"72e6d5a6e32d5f95d82e4c4d0c963d39555bb760","modified":1583257056329},{"_id":"themes/next/package.json","hash":"2941b27b2f62b5fe4821556de8cb90b570fbfb1d","modified":1583257056337},{"_id":"source/categories/index.md","hash":"eb6a63627133b1a6b762e68a5fea329075e678bd","modified":1583284337273},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"aa4cb7aff595ca628cb58160ee1eee117989ec4e","modified":1583257056325},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"1b87a7d22d466d78856900bd94875944181c991a","modified":1583257056325},{"_id":"source/tags/index.md","hash":"65054f99f5c26c893482076520adac451c7032bb","modified":1583284419953},{"_id":"source/tags/index-1.md","hash":"629981102552b6b10f55b22eae8fe6921a8c4561","modified":1583258272060},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"398a107b5e28fd61fb759b0c0f1c8f39d1c6d2ad","modified":1583257056325},{"_id":"themes/next/.github/config.yml","hash":"e4f4b9afe59bc508c4f7634895b33d7d460a7cb1","modified":1583257056325},{"_id":"themes/next/.github/issue-close-app.yml","hash":"7cba457eec47dbfcfd4086acd1c69eaafca2f0cd","modified":1583257056325},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"fca600ddef6f80c5e61aeed21722d191e5606e5b","modified":1583257056325},{"_id":"themes/next/.github/lock.yml","hash":"61173b9522ebac13db2c544e138808295624f7fd","modified":1583257056325},{"_id":"themes/next/.github/mergeable.yml","hash":"0ee56e23bbc71e1e76427d2bd255a9879bd36e22","modified":1583257056325},{"_id":"themes/next/.github/release-drafter.yml","hash":"3cc10ce75ecc03a5ce86b00363e2a17eb65d15ea","modified":1583257056325},{"_id":"themes/next/.github/stale.yml","hash":"941209526c2f7d916c76163c9e1ac1af9d956679","modified":1583257056325},{"_id":"themes/next/.github/support.yml","hash":"d75db6ffa7b4ca3b865a925f9de9aef3fc51925c","modified":1583257056325},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"c7a994b9542040317d8f99affa1405c143a94a38","modified":1583257056329},{"_id":"themes/next/docs/AUTHORS.md","hash":"10135a2f78ac40e9f46b3add3e360c025400752f","modified":1583257056329},{"_id":"themes/next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1583257056329},{"_id":"themes/next/docs/DATA-FILES.md","hash":"40a8089076005e0d26ef7c0db58a2b5b464cda6c","modified":1583257056329},{"_id":"themes/next/docs/INSTALLATION.md","hash":"af88bcce035780aaa061261ed9d0d6c697678618","modified":1583257056329},{"_id":"themes/next/docs/LICENSE.txt","hash":"368bf2c29d70f27d8726dd914f1b3211cae4bbab","modified":1583257056329},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"a6e625cb0805a5c246820c561ba8f7f59c9b5659","modified":1583257056329},{"_id":"themes/next/docs/MATH.md","hash":"d645b025ec7fb9fbf799b9bb76af33b9f5b9ed93","modified":1583257056329},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"682937d48bf5d243842a76190921322e26c75247","modified":1583257056329},{"_id":"themes/next/languages/de.yml","hash":"109943f7adcd5cdbe4c6c95c9d279603f07edacb","modified":1583257056329},{"_id":"themes/next/languages/default.yml","hash":"20f951dc4df8602ffdd05a1d5899c5a9bc1759cc","modified":1583257056329},{"_id":"themes/next/languages/en.yml","hash":"20f951dc4df8602ffdd05a1d5899c5a9bc1759cc","modified":1583257056329},{"_id":"themes/next/languages/es.yml","hash":"53ef4a621fd628748b8ed711fe86080e9c9c91c8","modified":1583257056333},{"_id":"themes/next/languages/fa.yml","hash":"a24e0cf28e9f137d0d2219498778693c3c3960b2","modified":1583257056333},{"_id":"themes/next/languages/fr.yml","hash":"578a30a51b9ecbbcb4c200362ad9a37ffd3272db","modified":1583257056333},{"_id":"themes/next/languages/hu.yml","hash":"074d069af9aed5ad34fa809bd058a3b9e2d01051","modified":1583257056333},{"_id":"themes/next/languages/id.yml","hash":"6037450ecd02796e08ca2e98037845b7c30c2807","modified":1583257056333},{"_id":"themes/next/languages/ja.yml","hash":"5e13b521201944815665bd077b65d7ce69622b81","modified":1583257056333},{"_id":"themes/next/languages/it.yml","hash":"ba2c8f51f2f719dabe71b6053c6fe6866161ec66","modified":1583257056333},{"_id":"themes/next/languages/pt-BR.yml","hash":"0660471e067d01ec80962d5721ae282aafff274d","modified":1583257056333},{"_id":"themes/next/languages/pt.yml","hash":"f7516b9d86b52c80bf63d3efc7ee6fd985205001","modified":1583257056333},{"_id":"themes/next/languages/ru.yml","hash":"7dcb2aab65a4b202476856f3e004862334229bcb","modified":1583257056333},{"_id":"themes/next/languages/tr.yml","hash":"145d28f6f051129dc6393affe8f68cd7ba925078","modified":1583257056333},{"_id":"themes/next/languages/ko.yml","hash":"4aa8f3bf06e02879863b19901476cb23ecd2d709","modified":1583257056333},{"_id":"themes/next/languages/uk.yml","hash":"21a573cdf8e26d87d5e32c5555bc645983268abe","modified":1583257056333},{"_id":"themes/next/languages/vi.yml","hash":"ffc144f606e171fdd8cdb41808ac36e406015a54","modified":1583257056333},{"_id":"themes/next/languages/nl.yml","hash":"e27b29c60d88ef4c30de291b595cf8cad639c5d1","modified":1583257056333},{"_id":"themes/next/languages/zh-CN.yml","hash":"038c3a650d2e3a288be9ba6580564172c50b4289","modified":1583257056333},{"_id":"themes/next/languages/zh-HK.yml","hash":"2620632caa3c94022d9513ab1971d15512e737e7","modified":1583257056333},{"_id":"themes/next/languages/zh-TW.yml","hash":"0964e90406bbd495e901d6b9d5f10124c8cad950","modified":1583257056333},{"_id":"themes/next/layout/_layout.swig","hash":"29ee038b0d5ffdb45327598733ea968588367769","modified":1583257056333},{"_id":"themes/next/layout/archive.swig","hash":"26526c09a4334099e2141456697696fcd1f9783f","modified":1583257056337},{"_id":"themes/next/layout/category.swig","hash":"c55debb2588e4746b02d31ec249bf0a84fdea260","modified":1583257056337},{"_id":"themes/next/layout/index.swig","hash":"3bc6fb1e9707d74b96e1346d3f03fe6584f764f4","modified":1583257056337},{"_id":"themes/next/layout/page.swig","hash":"ae6c8549242c1fb2483fd68ce9ae1c083785e2ff","modified":1583257056337},{"_id":"themes/next/layout/post.swig","hash":"382d9f9a9b35e1f369585f7f9f9b5dd6fa58d2f0","modified":1583257056337},{"_id":"themes/next/layout/tag.swig","hash":"7ff6e34d557a3da1c6a29ecd97842bf73ff213dc","modified":1583257056337},{"_id":"themes/next/scripts/renderer.js","hash":"49a65df2028a1bc24814dc72fa50d52231ca4f05","modified":1583257056341},{"_id":"source/_posts/hive/hive-source-modification.md","hash":"cbcf0b334a29c87ff0369d3ed85e555488046ad2","modified":1582280341555},{"_id":"source/_posts/linux/Linux目录.md","hash":"704a0b36c4de9a674881dbb106fc695c3048962c","modified":1583772832050},{"_id":"source/_posts/hadoop/hadoop-hadoop安装.md","hash":"816afd002ce28abd0d336adaa2fb03fd6b93b1ee","modified":1582136473883},{"_id":"source/_posts/linux/linux-基础知识.md","hash":"4265e94240d4aea5aeb534324f7d8771f3dcf4c9","modified":1583771253449},{"_id":"source/_posts/hive/hive-udf.md","hash":"fd87fb6c80743cee668223477bb5fc78944f16cd","modified":1583511334463},{"_id":"source/_posts/other/rebase合并commit.md","hash":"17d8a707db0c5feee521971f0f3e624059a3f53d","modified":1582814721906},{"_id":"source/_posts/spark/spark架构.md","hash":"2f61badc97b0d81f9a0202a4d510641feaefc112","modified":1582135256045},{"_id":"source/_posts/spark/RDD转换.md","hash":"55b8c29e43542b7112c2188d21b3020a67d031d8","modified":1582135256045},{"_id":"source/_posts/other/github个人网站更改域名.md","hash":"b4e06d16cc2e7aea67a34cf3c4e5f3237faeb3b3","modified":1582547693103},{"_id":"themes/next/.github/ISSUE_TEMPLATE/question.md","hash":"789a3cceb8f37a4b63b1fb2452a03332a3c365ed","modified":1583257056325},{"_id":"source/_posts/spark/spark目录.md","hash":"4c412c768392e9cfc39702af6d6492ffa0806192","modified":1582548100095},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"8ae029084b9ac482adf0fae2a0979dd388476513","modified":1583257056325},{"_id":"source/_posts/spark/spark.md","hash":"ec3708b8b70b81ab7fb489b64fa5a8a1f2d9b4f2","modified":1582135256045},{"_id":"themes/next/.github/ISSUE_TEMPLATE/other.md","hash":"89667adbb85c25716dba607cd7a38191acf60736","modified":1583257056325},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"7a9526f749205c882d672a4f51e6a3033c80ca6e","modified":1583257056325},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"9c4fe2873123bf9ceacab5c50d17d8a0f1baef27","modified":1583257056329},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"0bd2d696f62a997a11a7d84fec0130122234174e","modified":1583257056329},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"fe3f5cda1975114884d84bef384a562920d70335","modified":1583257056329},{"_id":"themes/next/docs/ru/README.md","hash":"2b3988e79d96b66640d6a98f0c0e6de9099805e6","modified":1583257056329},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"34b88784ec120dfdc20fa82aadeb5f64ef614d14","modified":1583257056329},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"fb23b85db6f7d8279d73ae1f41631f92f64fc864","modified":1583257056329},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"d3f03be036b75dc71cf3c366cd75aee7c127c874","modified":1583257056329},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"579c7bd8341873fb8be4732476d412814f1a3df7","modified":1583257056329},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"ca1030efdfca5e20f9db2e7a428998e66a24c0d0","modified":1583257056329},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"a80a2ece86306b5520d7790777660d122ea6b5cc","modified":1583257056329},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"b92585d251f1f9ebe401abb5d932cb920f9b8b10","modified":1583257056329},{"_id":"themes/next/docs/zh-CN/README.md","hash":"e5f6668c3a79e4a364931b9b4e5fa92f8c771ec8","modified":1583257056329},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"7f37327bbcae7ed7d04d187fd5e9bc6bbf14926a","modified":1583257056329},{"_id":"themes/next/layout/_macro/post.swig","hash":"ee01368d65fbb8d387f0956398daf62ae9ba1645","modified":1583257056333},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"815676d904f92748ddf4f529bed2baf066997bc6","modified":1583257056333},{"_id":"themes/next/layout/_scripts/index.swig","hash":"cea942b450bcb0f352da78d76dc6d6f1d23d5029","modified":1583257056333},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"71655ca21907e9061b6e8ac52d0d8fbf54d0062b","modified":1583257056333},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"d1f2bfde6f1da51a2b35a7ab9e7e8eb6eefd1c6b","modified":1583257056333},{"_id":"themes/next/layout/_scripts/pjax.swig","hash":"a3462c37ab6d7642b1e95860ea5c4cfbac78efab","modified":1583257056337},{"_id":"themes/next/layout/_scripts/three.swig","hash":"a4f42f2301866bd25a784a2281069d8b66836d0b","modified":1583257056337},{"_id":"themes/next/layout/_partials/comments.swig","hash":"0c4914a5fd08f15beec71940218c814ad9a89f3f","modified":1583257056333},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"ef38c213679e7b6d2a4116f56c9e55d678446069","modified":1583257056337},{"_id":"themes/next/layout/_partials/footer.swig","hash":"1ee6335c12773dc43f8b92136770cb10d460c25c","modified":1583257056333},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9876dbfc15713c7a47d4bcaa301f4757bd978269","modified":1583257056333},{"_id":"themes/next/layout/_partials/widgets.swig","hash":"83a40ce83dfd5cada417444fb2d6f5470aae6bb0","modified":1583257056333},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"8627c8c8b031ecee16c522433b66fa4d6979b8ea","modified":1583257056337},{"_id":"themes/next/scripts/events/index.js","hash":"9047d8ae2670e43429b16a7919a08a0a0a81afe0","modified":1583257056337},{"_id":"themes/next/scripts/helpers/engine.js","hash":"cb211b6b50913454b1737782e9e2af96cfa40448","modified":1583257056337},{"_id":"themes/next/scripts/helpers/font.js","hash":"32268fb4c59c5b37c1eb1c9582ab630e09e5cc7d","modified":1583257056337},{"_id":"themes/next/scripts/helpers/next-config.js","hash":"5e11f30ddb5093a88a687446617a46b048fa02e5","modified":1583257056337},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"58347687b02f7ab5e64bef07525c8efa97c9e8fb","modified":1583257056337},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"aec50ed57b9d5d3faf2db3c88374f107203617e0","modified":1583257056337},{"_id":"themes/next/scripts/filters/front-matter.js","hash":"703bdd142a671b4b67d3d9dfb4a19d1dd7e7e8f7","modified":1583257056337},{"_id":"themes/next/scripts/filters/locals.js","hash":"5bbfdc1c373542159660b7a68ed0b57ca18ad10b","modified":1583257056337},{"_id":"themes/next/layout/_third-party/index.swig","hash":"70c3c01dd181de81270c57f3d99b6d8f4c723404","modified":1583257056337},{"_id":"themes/next/scripts/filters/post.js","hash":"f2f566f2577c554377fd704442399acdd14a8118","modified":1583257056337},{"_id":"themes/next/scripts/filters/minify.js","hash":"19985723b9f677ff775f3b17dcebf314819a76ac","modified":1583257056337},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"1574848233f1bb2f45313ca08fef2dd33856a80b","modified":1583257056337},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"2731e262a6b88eaee2a3ca61e6a3583a7f594702","modified":1583257056337},{"_id":"themes/next/source/css/_colors.styl","hash":"7d07d83cb5c9a5f23751bb46019e853eb4d0cd0a","modified":1583257056341},{"_id":"themes/next/scripts/tags/button.js","hash":"946dd7beede408d1f090d5e9774d74763828b97c","modified":1583257056341},{"_id":"themes/next/source/css/_mixins.styl","hash":"b79ff3debd5709397b122292fc7e551ae9d40782","modified":1583257056349},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"94e0bbc7999b359baa42fa3731bdcf89c79ae2b3","modified":1583257056341},{"_id":"themes/next/source/css/main.styl","hash":"a3a3bbb5a973052f0186b3523911cb2539ff7b88","modified":1583257056353},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"0f133f27b61e8351cfd0959ba8a1b8551a9a8cc6","modified":1583257056341},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"d902fd313e8d35c3cc36f237607c2a0536c9edf1","modified":1583257056341},{"_id":"themes/next/scripts/tags/label.js","hash":"fc5b267d903facb7a35001792db28b801cccb1f8","modified":1583257056341},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"983c6c4adea86160ecc0ba2204bc312aa338121d","modified":1583257056341},{"_id":"themes/next/scripts/tags/note.js","hash":"0a02bb4c15aec41f6d5f1271cdb5c65889e265d9","modified":1583257056341},{"_id":"themes/next/scripts/tags/pdf.js","hash":"f780cc72bff91d2720626e7af69eed25e9c12a29","modified":1583257056341},{"_id":"themes/next/scripts/tags/tabs.js","hash":"00ca6340d4fe0ccdae7525373e4729117775bbfa","modified":1583257056341},{"_id":"themes/next/scripts/tags/video.js","hash":"e5ff4c44faee604dd3ea9db6b222828c4750c227","modified":1583257056341},{"_id":"themes/next/source/js/algolia-search.js","hash":"77cd98b1c790df12dd6cd8119bb3d99f72866635","modified":1583257056353},{"_id":"themes/next/source/js/bookmark.js","hash":"a00945ff886e9f6f835731cdaf29a3a3727c8877","modified":1583257056353},{"_id":"themes/next/source/js/local-search.js","hash":"bd42a1e05d37352270d2653ebb5adcb5585afc73","modified":1583257056353},{"_id":"themes/next/source/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1583257056353},{"_id":"themes/next/source/js/next-boot.js","hash":"f7045763e277e685c271bd4b4c37e531d699ac63","modified":1583257056353},{"_id":"themes/next/source/js/utils.js","hash":"a155950bc52396a701d0fe9988b3751c271f1741","modified":1583257056353},{"_id":"themes/next/source/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1583257056353},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1583257056353},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1583257056353},{"_id":"themes/next/source/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1583257056353},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1583257056353},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1583257056353},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1583257056353},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1583257056353},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1583257056353},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1583257056353},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1583257056353},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1583257056353},{"_id":"themes/next/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1583257056353},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1583257056353},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1583257056353},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1583257056353},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1583257056337},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"7f14ef43d9e82bc1efc204c5adf0b1dbfc919a9f","modified":1583257056337},{"_id":"themes/next/layout/_scripts/pages/schedule.swig","hash":"a2bb0bec243685e670b60a3d54142950adc03af0","modified":1583257056333},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1583257056337},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"1c910fc066c06d5fbbe9f2b0c47447539e029af7","modified":1583257056337},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"d3a16f0f343ea70b59e33e4b9cdecae3c8df91cd","modified":1583257056333},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"30528a8da30994b1ef9355a72b09b2cd85a7c0e9","modified":1583257056333},{"_id":"themes/next/layout/_partials/header/menu-item.swig","hash":"12aeb9ee0d1d49d347f82a91e6bab568e1b59037","modified":1583257056333},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"bbf0c8e42491fac70f4f8165224f1d7d92a040d7","modified":1583257056333},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"ae2261bea836581918a1c2b0d1028a78718434e0","modified":1583257056333},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"5ff544013e2905138ffeb07bf9a57062faed75b2","modified":1583257056333},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"1ea12d4b9490d9065ebf1b8739b90ce5defd6398","modified":1583257056333},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"c851717497ca64789f2176c9ecd1dedab237b752","modified":1583257056333},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"a56e4f6ad95c106f361d354f828d1ef4810b1d76","modified":1583257056333},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"94d54b0c65d504f772af1e62424952e092b6c21d","modified":1583257056333},{"_id":"themes/next/layout/_partials/post/post-followme.swig","hash":"14c33bd544903e74388739599fffe3ecb66ed4b0","modified":1583257056333},{"_id":"themes/next/layout/_partials/post/post-footer.swig","hash":"8f14f3f8a1b2998d5114cc56b680fb5c419a6b07","modified":1583257056333},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"f79c44692451db26efce704813f7a8872b7e63a0","modified":1583257056333},{"_id":"themes/next/layout/_partials/post/post-reward.swig","hash":"2b1a73556595c37951e39574df5a3f20b2edeaef","modified":1583257056333},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"48430bd03b8f19c9b8cdb2642005ed67d56c6e0b","modified":1583257056333},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"d6fa9e4432b87004c5678dfe2d4b2c1f4a702b93","modified":1583257056333},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"2791a8dc20a276704fc8b03f9822f76578a9152d","modified":1583257056333},{"_id":"themes/next/layout/_partials/sidebar/site-overview.swig","hash":"716b78cd90addc4216413719554721cb362b0c18","modified":1583257056333},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"4790058691b7d36cf6d2d6b4e93795a7b8d608ad","modified":1583257056337},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"2fa2b51d56bfac6a1ea76d651c93b9c20b01c09b","modified":1583257056337},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"5adea065641e8c55994dd2328ddae53215604928","modified":1583257056337},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"1472cabb0181f60a6a0b7fec8899a4d03dfb2040","modified":1583257056337},{"_id":"themes/next/layout/_third-party/chat/chatra.swig","hash":"f910618292c63871ca2e6c6e66c491f344fa7b1f","modified":1583257056337},{"_id":"themes/next/layout/_third-party/chat/tidio.swig","hash":"cba0e6e0fad08568a9e74ba9a5bee5341cfc04c1","modified":1583257056337},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"f39a5bf3ce9ee9adad282501235e0c588e4356ec","modified":1583257056337},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b14908644225d78c864cd0a9b60c52407de56183","modified":1583257056337},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"82f5b6822aa5ec958aa987b101ef860494c6cf1f","modified":1583257056337},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"d6ceb70648555338a80ae5724b778c8c58d7060d","modified":1583257056337},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"f7a9eca599a682479e8ca863db59be7c9c7508c8","modified":1583257056337},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"be0a8eccf1f6dc21154af297fc79555343031277","modified":1583257056337},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"6c5976621efd5db5f7c4c6b4f11bc79d6554885f","modified":1583257056337},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"4791c977a730f29c846efcf6c9c15131b9400ead","modified":1583257056337},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"ecf751321e799f0fb3bf94d049e535130e2547aa","modified":1583257056337},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"767b6c714c22588bcd26ba70b0fc19b6810cbacd","modified":1583257056337},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"d35a999d67f4c302f76fdf13744ceef3c6506481","modified":1583257056337},{"_id":"themes/next/layout/_third-party/search/swiftype.swig","hash":"ba0dbc06b9d244073a1c681ff7a722dcbf920b51","modified":1583257056337},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.swig","hash":"d7258d02bcf0dac6c0fd8377c0909ddecb09d1d4","modified":1583257056337},{"_id":"themes/next/layout/_third-party/statistics/cnzz-analytics.swig","hash":"a17ace37876822327a2f9306a472974442c9005d","modified":1583257056337},{"_id":"themes/next/layout/_third-party/statistics/firestore.swig","hash":"b26ac2bfbe91dd88267f8b96aee6bb222b265b7a","modified":1583257056337},{"_id":"themes/next/layout/_third-party/statistics/index.swig","hash":"5f6a966c509680dbfa70433f9d658cee59c304d7","modified":1583257056337},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.swig","hash":"afeeed84b395797429d5a852b70e12fd79f7410b","modified":1583257056337},{"_id":"themes/next/layout/_third-party/tags/mermaid.swig","hash":"f3c43664a071ff3c0b28bd7e59b5523446829576","modified":1583257056337},{"_id":"themes/next/layout/_third-party/tags/pdf.swig","hash":"5adab7380491e9df5c9ada4f4feb204b866ec14b","modified":1583257056337},{"_id":"themes/next/scripts/events/lib/config.js","hash":"b205d72a56b1827681f0a260c266e0c02065fd08","modified":1583257056337},{"_id":"themes/next/scripts/events/lib/injects-point.js","hash":"6661c1c91c7cbdefc6a5e6a034b443b8811235a1","modified":1583257056337},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"f233d8d0103ae7f9b861344aa65c1a3c1de8a845","modified":1583257056337},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"1f20213af8da3127701e6bb9da995e5c91be2051","modified":1583257056337},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"0803d4f4d3d02c24417c163ad0b27b60fda79250","modified":1583257056337},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"7f2d93af012c1e14b8596fecbfc7febb43d9b7f5","modified":1583257056337},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"19cbd24880d0fbbd4d5698cd54da598f03b942da","modified":1583257056337},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"7f8b92913d21070b489457fa5ed996d2a55f2c32","modified":1583257056337},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"e51dc3072c1ba0ea3008f09ecae8b46242ec6021","modified":1583257056337},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"d5fefc31fba4ab0188305b1af1feb61da49fdeb0","modified":1583257056337},{"_id":"themes/next/scripts/filters/comment/valine.js","hash":"6a72b5928cdab9526a288177991e4b2aedd028cf","modified":1583257056337},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"f4e694e5db81e57442c7e34505a416d818b3044a","modified":1583257056349},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"38597817ede20418e73ae4afc50047ea5088c73e","modified":1583257056349},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"62df49459d552bbf73841753da8011a1f5e875c8","modified":1583257056349},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"9840998e1a1fbcd419c52a1e38fa54a003eac963","modified":1583257056349},{"_id":"themes/next/source/css/_variables/base.styl","hash":"85c46ac85689e7c5f5398fc8b6b40b7a8e94bafb","modified":1583257056353},{"_id":"themes/next/source/js/schemes/muse.js","hash":"47c4f60eb7f7dc3303e84914b611dc34827069e1","modified":1583257056353},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"f068b46f8c305c7436c2767492a6bed42dcd764c","modified":1583257056353},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1583257056353},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1583257056353},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1583257056353},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1583257056353},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1583257056353},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1583257056357},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1583257056357},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"a47725574e1bee3bc3b63b0ff2039cc982b17eff","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"ca5e70662dcfb261c25191cc5db5084dcf661c76","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"e2355c213fc25d635f1179fe317b826e0b9dad17","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"170c4598cbbe49cd1527f94158d97d2320a6b906","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/rainbow.styl","hash":"bc87cea0b534f2d75db60f300b069456f6516d1b","modified":1583257056341},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"990bd301ce2de0a6b936781c58318f3945d81bc2","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"44fe82eadbdbb2f66adda37ac83ebd0f85876bfc","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"38f632dce42481da83a5ffab382c281269885e9c","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"62209da61b4ac49e3a7ff8174e28e075060835ec","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"3fee8cbe5704a04107ff0816db1221edb40dbb9b","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"8f58570a1bbc34c4989a47a1b7d42a8030f38b06","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"b3bea92eef0e1fe2e7e294dac2184d16b5b8d666","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"179e33b8ac7f4d8a8e76736a7e4f965fe9ab8b42","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"f6516d0f7d89dc7b6c6e143a5af54b926f585d82","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"4794bd45d5e32ea005c805bcbc65b871e9927d02","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"41e5a7c567735e780ef9bfdacd4af1ff4b5e1d2a","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"7104b9cef90ca3b140d7a7afcf15540a250218fc","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"a30db9fffd34d522d378aeaeaa400d1a84505b38","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"a717969829fa6ef88225095737df3f8ee86c286b","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"21003fd0b43dc3b3592e916d585f7f78478cbeb3","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"4d1c17345d2d39ef7698f7acf82dfc0f59308c34","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"bb392700f04d956bb5f606efb052453efeade53d","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"2b2e7b5cea7783c9c8bb92655e26a67c266886f0","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"e282df938bd029f391c466168d0e68389978f120","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"70a4324b70501132855b5e59029acfc5d3da1ebd","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"ccb71d732b12acd02ac26ed6bbda4861d027857d","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e4f958677a75de87ee1caf7e22ba46a0602f22dd","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"82025c3ad7af12e532e2e81be98deb0a74ff23ac","modified":1583257056349},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1583257056349},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1583257056353},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1583257056353},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1583257056353},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1583257056357},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"fafc96c86926b22afba8bb9418c05e6afbc05a57","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"2bd0eb1512415325653b26d62a4463e6de83c5ac","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"c1daeb60c23945f745703ac2c3f4bf99d0ea3d95","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"7ddb7453bf9b85b01bff136e9d10a7f06baac9e8","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"c0944ea35dc2bd3b2da9b64f5d05e7e78b6660f1","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f49ca072b5a800f735e8f01fc3518f885951dd8e","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"e3ade812b5541eca5b863ad3ff234ea95925bf31","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"5d5c022aa3b2f89c2f2a178212338bb64804dd75","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/post/post-followme.styl","hash":"1e4190c10c9e0c9ce92653b0dbcec21754b0b69d","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"72d495a88f7d6515af425c12cbc67308a57d88ea","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"887aa8de61ae060150a6312d5cb00d4da065db77","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"796eb941ba0ca03fd5ca6d15a1f6a56afd9aa174","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"eca4d80dd0df1c3b1bc06bd39e6a4bd6c56198df","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"f5c2788a78790aca1a2f37f7149d6058afb539e0","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"99e12c9ce3d14d4837e3d3f12fc867ba9c565317","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"5b5649b9749e3fd8b63aef22ceeece0a6e1df605","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"c7939407797acbd1ae0d8bae8e13b2bf045f870e","modified":1583257056341},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"8a7fc03a568b95be8d3337195e38bc7ec5ba2b23","modified":1583257056345},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"b49e9fbd3c182b8fc066b8c2caf248e3eb748619","modified":1583257056345},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"e2992846b39bf3857b5104675af02ba73e72eed5","modified":1583257056345},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"705759bdd1649e9aa1caedb82f6432a991ae3e71","modified":1583257056345},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9a878d0119785a2316f42aebcceaa05a120b9a7a","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/footer/footer.styl","hash":"f5821481440a0624c8aec5fc85f093de1527095f","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"e02b1097a72a7d2ddc45ea8d53aa6d77c25ac407","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"e7a9fdb6478b8674b1cdf94de4f8052843fb71d9","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/header/header.styl","hash":"a793cfff86ad4af818faef04c18013077873f8f0","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/header/headerband.styl","hash":"0caf32492692ba8e854da43697a2ec8a41612194","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"555762730f1f31451113e8fdc84ec438ea738d90","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"0ec7bafed7eec36504df7781207ccd1ce4448536","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"b2fc519828fe89a1f8f03ff7b809ad68cd46f3d7","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"2cb1876e9e0c9ac32160888af27b1178dbcb0616","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"fa0222197b5eee47e18ac864cdc6eac75678b8fe","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"44487d9ab290dc97871fa8dd4487016deb56e123","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"a237c290e8934d1a8cbbf22b3f30503d9663021d","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"9b479c2f9a9bfed77885e5093b8245cc5d768ec7","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"a960a2dd587b15d3b3fe1b59525d6fa971c6a6ec","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"a05a4031e799bc864a4536f9ef61fe643cd421af","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"b3220db827e1adbca7880c2bb23e78fa7cbe95cb","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar.styl","hash":"a9cd93c36bae5af9223e7804963096274e8a4f03","modified":1583257056345},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"2a47f8a6bb589c2fb635e6c1e4a2563c7f63c407","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"796415ae7490e17857e64ffef7e470b65c655a6b","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/highlight/diff.styl","hash":"d3f73688bb7423e3ab0de1efdf6db46db5e34f80","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/highlight/highlight.styl","hash":"8136f9efe1f018ebe4b4a2d8bd3683bb393ff456","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/highlight/theme.styl","hash":"3b3acc5caa0b95a2598bef4eeacb21bab21bea56","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"7213e3d0ad7c95717ecd4e701d6ee9248ef2bf9f","modified":1583257056345},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"709d10f763e357e1472d6471f8be384ec9e2d983","modified":1583257056349},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"d7fce4b51b5f4b7c31d93a9edb6c6ce740aa0d6b","modified":1583257056349},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"9b3cce30c58e57b59e45d3f668a71a4129d3a8e4","modified":1583257056349},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"2dc2a5b7becb11de1d4bdab6b5195588ae878cfc","modified":1583257056349},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"5ac97054b302fe3ce47822a03a3b56aa3d582005","modified":1583257056349},{"_id":"themes/next/source/css/_common/scaffolding/tags/tags.styl","hash":"9e4c0653cfd3cc6908fa0d97581bcf80861fb1e7","modified":1583257056349},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1583257056357},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1583257056353},{"_id":"public/./public/search.xml","hash":"5fce9083c1915337702bf5e54d03938d32ad536f","modified":1584462731286},{"_id":"public/categories/index.html","hash":"8e1a98d76d8c0fa4349aa569f8e248c957a1c130","modified":1583772508130},{"_id":"public/tags/index.html","hash":"9b323813310f75ac9e33f9cd8cc93578c9a80d4f","modified":1583772508130},{"_id":"public/tags/index-1.html","hash":"aaa0d993951b555dae3b0cbc57a7aeca9920703a","modified":1583284361310},{"_id":"public/2020/02/21/hive/hive-source-modification/index.html","hash":"f8ffb28d164e0dd24ddcf76954a40d25eb3a7edf","modified":1583772508130},{"_id":"public/2020/02/09/linux/Linux目录/index.html","hash":"468e38dd838928704f26772836e5ac4c2dcc3530","modified":1583772847004},{"_id":"public/2019/11/02/spark/spark目录/index.html","hash":"2929e07eabc1fe3ea8eb7b29d5a6d2745de69116","modified":1583772508130},{"_id":"public/categories/Hive/index.html","hash":"642ae6c9ecd1671dde369119d5759cbe9a345799","modified":1583772508130},{"_id":"public/categories/目录/index.html","hash":"f91ebdcbdc0af3c3922e3c699f09585c8ccd45ac","modified":1583772508130},{"_id":"public/categories/hadoop/index.html","hash":"0cdb01822bcf2ba13c9ede94b04f7887599b4e0a","modified":1583772508130},{"_id":"public/categories/Linux/index.html","hash":"af1d586a5e1961f1dd1269c7d2801d983266e985","modified":1583772508130},{"_id":"public/categories/other/index.html","hash":"c02234abe982fc408cab20cda49e93e735fdf3fe","modified":1583772508130},{"_id":"public/categories/Spark/index.html","hash":"b65e92296d608b108e843894e9febae269385000","modified":1583772508130},{"_id":"public/archives/index.html","hash":"fad0276fef8b744f8e0539255c7d0614280c6b80","modified":1583772508130},{"_id":"public/archives/page/2/index.html","hash":"55898df1929901bb776e9e52d052eadef6379f97","modified":1583772508130},{"_id":"public/archives/page/3/index.html","hash":"f5d547257190bf3997936d757e50a13850bede37","modified":1583772508130},{"_id":"public/archives/2019/index.html","hash":"3903db5e755c9a247905deb9ccae8a5ad111ad75","modified":1583772508130},{"_id":"public/archives/2019/10/index.html","hash":"be428f408e74d2ff2efd32f30368ca8afdbf8948","modified":1583772508130},{"_id":"public/archives/2019/11/index.html","hash":"249c78f3e02aed953fac493c995f90b7dab56d55","modified":1583772508130},{"_id":"public/archives/2020/index.html","hash":"b71d68c7c4e2d46113e30f30739f898a8ac0a411","modified":1583772508130},{"_id":"public/archives/2020/page/2/index.html","hash":"889f93bf8ea4dd02615fd2d30e13db2a2e1c4e02","modified":1583772508130},{"_id":"public/archives/2020/02/index.html","hash":"1d8dcb399f8c93cb04c8ccb004697a03d72d90b8","modified":1583772508130},{"_id":"public/archives/2020/02/page/2/index.html","hash":"007be1d3b1af4c41a8bc2bb193239a517df7c142","modified":1583772508130},{"_id":"public/page/3/index.html","hash":"7134c37e6fb730e59e69aeb309945c5b61a6b90e","modified":1583772508130},{"_id":"public/tags/hive/index.html","hash":"d474ac27b74f8befa2862080598681f0943bfa5c","modified":1583772508130},{"_id":"public/tags/源码/index.html","hash":"d4cd50a08c4c95feebff3eb58a0bb627fe23a161","modified":1583772508130},{"_id":"public/tags/开发/index.html","hash":"ea91233e3e321b07d64bcdde3c962233d02610ea","modified":1583772508130},{"_id":"public/tags/linux目录/index.html","hash":"171504c2902e6e3cc2058586eb9e4daa13500233","modified":1583772508130},{"_id":"public/tags/hadoop/index.html","hash":"8607549ea8ca569a16f8a2812462613da07bc248","modified":1583772508130},{"_id":"public/tags/hadoop安装/index.html","hash":"3106f3d73ac077126dbec1b9c83f2a5203184e54","modified":1583772508130},{"_id":"public/tags/windows-10/index.html","hash":"8a6b187972c7f2a5084b5c4fe64b280606ef1e4f","modified":1583772508130},{"_id":"public/tags/Linux/index.html","hash":"e51a5f56f0687ec8f652ef6bacfd98b126db0cd4","modified":1583772508130},{"_id":"public/tags/基础知识/index.html","hash":"ff7d36eabb09e39cb23eccc2ca136014b23f623b","modified":1583772508130},{"_id":"public/tags/udf/index.html","hash":"fc0aa56b891c0c9c933d09c673973dda0e1e2685","modified":1583772508130},{"_id":"public/tags/udaf/index.html","hash":"fcce4c47341531c1cea0558afc1aeea7ec0b0102","modified":1583772508130},{"_id":"public/tags/Git/index.html","hash":"a226270294b77c87a1b116888d7fea734551aab1","modified":1583772508130},{"_id":"public/tags/rebase/index.html","hash":"ed6d05e03757f07a09964cae989cf038993d5653","modified":1583772508130},{"_id":"public/tags/commits合并/index.html","hash":"25586558840501cfe60c63af8fcca3e513fbed34","modified":1583772508130},{"_id":"public/tags/Spark/index.html","hash":"7a48295150639b45a3f8130c7e9a408072699468","modified":1583772508130},{"_id":"public/tags/High-Performance-Spark/index.html","hash":"42d322b7ba39bd8a14f735fe8e0c7a24b0e50a14","modified":1583772508130},{"_id":"public/tags/spark架构/index.html","hash":"82c081901749c589e59c2efe658eeecff2631a51","modified":1583772508130},{"_id":"public/tags/RDD/index.html","hash":"c46cb2551c6674b5fc2170e5843c01d57fa3948f","modified":1583772508130},{"_id":"public/tags/github个人网站/index.html","hash":"9c15e6216385622e29a197c046b6e76e3c8b5c25","modified":1583772508130},{"_id":"public/tags/域名修改/index.html","hash":"c01df8562bf8db100807a10a396a7b25fef8a999","modified":1583772508130},{"_id":"public/tags/spark目录/index.html","hash":"d2162778111bc1792bf755404ad16838388c25a8","modified":1583772508130},{"_id":"public/tags/大数据工具/index.html","hash":"224e09cd2de1e161658d7d913827d7709302b551","modified":1583772508130},{"_id":"public/2020/02/27/other/rebase合并commit/index.html","hash":"8f4b6285f453985f44f631c8ac629915ada7acb5","modified":1583772508130},{"_id":"public/2020/02/24/other/github个人网站更改域名/index.html","hash":"9794ec899255cc7c02daf99362772855a32043ca","modified":1583772508130},{"_id":"public/2020/02/21/hive/hive-udf/index.html","hash":"05d0c54fb15afc5d9cdc9bec8b45637eb5fdc14f","modified":1583772508130},{"_id":"public/2020/02/15/hadoop/hadoop-hadoop安装/index.html","hash":"d0af7a865381f3a86320b749f6f3c4bc45c5e20c","modified":1583772508130},{"_id":"public/2020/02/12/linux/linux-基础知识/index.html","hash":"af1d0cf6ec05127866fab69758a55b40210c16ef","modified":1583772508130},{"_id":"public/2019/11/02/spark/RDD转换/index.html","hash":"5aa618ba0bfb0604dcde134bc472c9f1a14d60df","modified":1583772508130},{"_id":"public/2019/11/02/spark/spark架构/index.html","hash":"e11080b80093f7b29017342ac10293f4cf1f7223","modified":1583772508130},{"_id":"public/2019/10/24/spark/spark/index.html","hash":"7cbf38521475ab9db9af4ed103b232e763825eff","modified":1583772508130},{"_id":"public/index.html","hash":"a0585035afea3ac122de628b07f29776b84624f6","modified":1584462731286},{"_id":"public/page/2/index.html","hash":"b9eeaeb7e87e88c311964e720fa1859d77efd8c5","modified":1583772508130},{"_id":"public/CNAME","hash":"9e50bb7377d3775cf2cac9f84f26e5baa4aedca4","modified":1583284361310},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1583284361310},{"_id":"public/images/avatar.gif","hash":"18c53e15eb0c84b139995f9334ed8522b40aeaf6","modified":1583284361310},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1583284361310},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1583284361310},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1583284361310},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1583284361310},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1583284361310},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1583284361310},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1583284361310},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1583284361310},{"_id":"public/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1583284361310},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1583284361310},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1583284361310},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1583284361310},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1583284361310},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1583284361310},{"_id":"public/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1583284361310},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1583284361310},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1583284361310},{"_id":"public/js/algolia-search.js","hash":"77cd98b1c790df12dd6cd8119bb3d99f72866635","modified":1583284361310},{"_id":"public/js/bookmark.js","hash":"a00945ff886e9f6f835731cdaf29a3a3727c8877","modified":1583284361310},{"_id":"public/js/local-search.js","hash":"bd42a1e05d37352270d2653ebb5adcb5585afc73","modified":1583284361310},{"_id":"public/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1583284361310},{"_id":"public/js/next-boot.js","hash":"f7045763e277e685c271bd4b4c37e531d699ac63","modified":1583284361310},{"_id":"public/js/utils.js","hash":"a155950bc52396a701d0fe9988b3751c271f1741","modified":1583284361310},{"_id":"public/js/schemes/muse.js","hash":"47c4f60eb7f7dc3303e84914b611dc34827069e1","modified":1583284361310},{"_id":"public/js/schemes/pisces.js","hash":"f068b46f8c305c7436c2767492a6bed42dcd764c","modified":1583284361310},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1583284361310},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1583284361310},{"_id":"public/css/main.css","hash":"c8f25474fb5c75a8519d8d746951420a244c0404","modified":1583284361310},{"_id":"public/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1583284361310},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1583284361310},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1583284361310},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1583284361310},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1583284361310},{"_id":"source/_posts/other/计算机网络之联网.md","hash":"0f7e04eb06775d2d6c44191c9ef3fa8d05447b68","modified":1583654263266},{"_id":"public/2020/03/05/other/计算机网络之联网/index.html","hash":"480c0eac73ceada97795b48f25eedd1237d44bdd","modified":1583772508130},{"_id":"public/archives/2020/03/index.html","hash":"e950ff73004f5bc92922f78b0c7a658df9a8714c","modified":1583772508130},{"_id":"public/tags/ip寻址/index.html","hash":"970e1202420a9291c43c3ba73404b617fc30ea87","modified":1583772508130},{"_id":"public/tags/上网/index.html","hash":"ba86e082c8ff17094f714dc58a4e08b2c6554000","modified":1583772508130},{"_id":"public/tags/计算机网络/index.html","hash":"c17f1bbca725eb9cf0df200aa1de57d3154b759b","modified":1583772508130},{"_id":"source/_posts/linux/linux-文件权限.md","hash":"7bb535d13627a56f8a274444d151666313371d81","modified":1584462722971},{"_id":"public/2020/03/10/linux/linux-文件权限/index.html","hash":"2244b4871c427b85ac25ea4c6fefbe483a73140f","modified":1584462731286},{"_id":"public/tags/文件/index.html","hash":"9d5c93487d2881958df4a9aa8857e3c43b34b419","modified":1583772508130},{"_id":"public/tags/权限/index.html","hash":"af20643f937832a84bfdb5a26381fce56bf84cc5","modified":1583772508130}],"Category":[{"name":"Hive","_id":"ck7cmmo8u0005clzkbilzgd03"},{"name":"目录","_id":"ck7cmmo8y000aclzk56gfcbk3"},{"name":"hadoop","_id":"ck7cmmo96000fclzk3tqbdowf"},{"name":"Linux","_id":"ck7cmmo9a000lclzk0t313ahl"},{"name":"other","_id":"ck7cmmo9e000sclzk1bvw6oiv"},{"name":"Spark","_id":"ck7cmmo9g000yclzkh6miada4"}],"Data":[],"Page":[{"title":"categories","date":"2020-03-03T17:58:47.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2020-03-04 01:58:47\ntype: categories\n---\n","updated":"2020-03-04T01:12:17.273Z","path":"categories/index.html","comments":1,"layout":"page","_id":"ck7cmmo7q0000clzk8jcy0h4t","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2020-03-03T17:57:42.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2020-03-04 01:57:42\ntype: tags\n---\n","updated":"2020-03-04T01:13:39.953Z","path":"tags/index.html","_id":"ck7cmmo7v0001clzkgd3t5e95","comments":1,"layout":"page","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"hive源码调试入门","date":"2020-02-21T04:45:21.000Z","_content":"\n\n\n在windows上折腾安装好了hadoop，hive因为文件路径太长不支持等各种奇奇怪怪的问题始终运行不起来，下了大决心放弃windows系统，unbuntu走起！重装系统还算顺利，几个小时搞定。然后就是ubuntu上运行hadoop,成功; ubuntu上运行hive，成功！下一步hive源码走起。<!--more-->\n\nhive依赖hadoop，看hive启动脚本，最终是调用hadoop启动，而hadoop最终还是执行的java -jar xxx.jar形式，所以决定先从hive java源码入口看。先试一试改动源码打印个信息。\n\n首先在hive的main函数入口增加一行\n\n```\nSystem.out.println(\"Will's first hive source code modification: test err print info\");\n```\n\n<img src=\"https://i.loli.net/2020/02/21/bQOyidqXgsMR3Ca.png\" alt=\"启动\" style=\"zoom:80%;\" />\n\n打包，然后替换lib目录下hive-cli-xxx.jar。\n\n运行hive\n\n![选区_001.png](https://i.loli.net/2020/02/21/LZ16gVt4rfdXqyu.png)\n\n\n\n好啦，正式开启与hive源码的斗争！\n\n","source":"_posts/hive/hive-source-modification.md","raw":"---\ntitle: hive源码调试入门\ndate: 2020-02-21 12:45:21\ntags:\n    - hive\n    - 源码\n    - 开发\ncategories: \n    - Hive\n---\n\n\n\n在windows上折腾安装好了hadoop，hive因为文件路径太长不支持等各种奇奇怪怪的问题始终运行不起来，下了大决心放弃windows系统，unbuntu走起！重装系统还算顺利，几个小时搞定。然后就是ubuntu上运行hadoop,成功; ubuntu上运行hive，成功！下一步hive源码走起。<!--more-->\n\nhive依赖hadoop，看hive启动脚本，最终是调用hadoop启动，而hadoop最终还是执行的java -jar xxx.jar形式，所以决定先从hive java源码入口看。先试一试改动源码打印个信息。\n\n首先在hive的main函数入口增加一行\n\n```\nSystem.out.println(\"Will's first hive source code modification: test err print info\");\n```\n\n<img src=\"https://i.loli.net/2020/02/21/bQOyidqXgsMR3Ca.png\" alt=\"启动\" style=\"zoom:80%;\" />\n\n打包，然后替换lib目录下hive-cli-xxx.jar。\n\n运行hive\n\n![选区_001.png](https://i.loli.net/2020/02/21/LZ16gVt4rfdXqyu.png)\n\n\n\n好啦，正式开启与hive源码的斗争！\n\n","slug":"hive/hive-source-modification","published":1,"updated":"2020-02-21T10:19:01.555Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck7cmmo8q0003clzkf3p196cg","content":"<p>在windows上折腾安装好了hadoop，hive因为文件路径太长不支持等各种奇奇怪怪的问题始终运行不起来，下了大决心放弃windows系统，unbuntu走起！重装系统还算顺利，几个小时搞定。然后就是ubuntu上运行hadoop,成功; ubuntu上运行hive，成功！下一步hive源码走起。<a id=\"more\"></a></p>\n<p>hive依赖hadoop，看hive启动脚本，最终是调用hadoop启动，而hadoop最终还是执行的java -jar xxx.jar形式，所以决定先从hive java源码入口看。先试一试改动源码打印个信息。</p>\n<p>首先在hive的main函数入口增加一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">System.out.println(&quot;Will&#39;s first hive source code modification: test err print info&quot;);</span><br></pre></td></tr></table></figure>\n\n<img src=\"https://i.loli.net/2020/02/21/bQOyidqXgsMR3Ca.png\" alt=\"启动\" style=\"zoom:80%;\" />\n\n<p>打包，然后替换lib目录下hive-cli-xxx.jar。</p>\n<p>运行hive</p>\n<p><img src=\"https://i.loli.net/2020/02/21/LZ16gVt4rfdXqyu.png\" alt=\"选区_001.png\"></p>\n<p>好啦，正式开启与hive源码的斗争！</p>\n","site":{"data":{}},"excerpt":"<p>在windows上折腾安装好了hadoop，hive因为文件路径太长不支持等各种奇奇怪怪的问题始终运行不起来，下了大决心放弃windows系统，unbuntu走起！重装系统还算顺利，几个小时搞定。然后就是ubuntu上运行hadoop,成功; ubuntu上运行hive，成功！下一步hive源码走起。","more":"</p>\n<p>hive依赖hadoop，看hive启动脚本，最终是调用hadoop启动，而hadoop最终还是执行的java -jar xxx.jar形式，所以决定先从hive java源码入口看。先试一试改动源码打印个信息。</p>\n<p>首先在hive的main函数入口增加一行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">System.out.println(&quot;Will&#39;s first hive source code modification: test err print info&quot;);</span><br></pre></td></tr></table></figure>\n\n<img src=\"https://i.loli.net/2020/02/21/bQOyidqXgsMR3Ca.png\" alt=\"启动\" style=\"zoom:80%;\" />\n\n<p>打包，然后替换lib目录下hive-cli-xxx.jar。</p>\n<p>运行hive</p>\n<p><img src=\"https://i.loli.net/2020/02/21/LZ16gVt4rfdXqyu.png\" alt=\"选区_001.png\"></p>\n<p>好啦，正式开启与hive源码的斗争！</p>"},{"title":"linux---目录","date":"2020-02-09T14:35:00.000Z","top":true,"_content":"\nlinux系统是一种自由和开放源码的类UNIX操作系统。该操作系统的内核由林纳斯·托瓦兹在1991年10月5日首次发布，在加上用户空间的应用程序之后，成为Linux操作系统[1]。linux系统作为当前最流行的系统之一，有各种版本，如centos, ubuntu等，当前移动互联网终端之王---手机---的系统之一Android也是基于linux，可以说随处可以见到linux的身影。\n\n在大数据开发中，linux系统必然会涉及，是必备的基础技能。下面将分为以下一系列文章进行介绍：\n\n- {% post_link linux/linux-基础知识 基础知识%}\n- 常用命令\n- {% post_link linux/linux-文件权限 文件及用户权限%}\n- shell脚本\n\n\n\n> [1] 维基百科","source":"_posts/linux/Linux目录.md","raw":"---\ntitle: linux---目录\ndate: 2020-02-09 22:35:00\ntags:  linux目录\ncategories: \n    - 目录\ntop: true\n---\n\nlinux系统是一种自由和开放源码的类UNIX操作系统。该操作系统的内核由林纳斯·托瓦兹在1991年10月5日首次发布，在加上用户空间的应用程序之后，成为Linux操作系统[1]。linux系统作为当前最流行的系统之一，有各种版本，如centos, ubuntu等，当前移动互联网终端之王---手机---的系统之一Android也是基于linux，可以说随处可以见到linux的身影。\n\n在大数据开发中，linux系统必然会涉及，是必备的基础技能。下面将分为以下一系列文章进行介绍：\n\n- {% post_link linux/linux-基础知识 基础知识%}\n- 常用命令\n- {% post_link linux/linux-文件权限 文件及用户权限%}\n- shell脚本\n\n\n\n> [1] 维基百科","slug":"linux/Linux目录","published":1,"updated":"2020-03-09T16:53:52.050Z","_id":"ck7cmmo8s0004clzkgmbj2cvt","comments":1,"layout":"post","photos":[],"link":"","content":"<p>linux系统是一种自由和开放源码的类UNIX操作系统。该操作系统的内核由林纳斯·托瓦兹在1991年10月5日首次发布，在加上用户空间的应用程序之后，成为Linux操作系统[1]。linux系统作为当前最流行的系统之一，有各种版本，如centos, ubuntu等，当前移动互联网终端之王—手机—的系统之一Android也是基于linux，可以说随处可以见到linux的身影。</p>\n<p>在大数据开发中，linux系统必然会涉及，是必备的基础技能。下面将分为以下一系列文章进行介绍：</p>\n<ul>\n<li><a href=\"/2020/02/12/linux/linux-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/\" title=\"基础知识\">基础知识</a></li>\n<li>常用命令</li>\n<li><a href=\"/2020/03/10/linux/linux-%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/\" title=\"文件及用户权限\">文件及用户权限</a></li>\n<li>shell脚本</li>\n</ul>\n<blockquote>\n<p>[1] 维基百科</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>linux系统是一种自由和开放源码的类UNIX操作系统。该操作系统的内核由林纳斯·托瓦兹在1991年10月5日首次发布，在加上用户空间的应用程序之后，成为Linux操作系统[1]。linux系统作为当前最流行的系统之一，有各种版本，如centos, ubuntu等，当前移动互联网终端之王—手机—的系统之一Android也是基于linux，可以说随处可以见到linux的身影。</p>\n<p>在大数据开发中，linux系统必然会涉及，是必备的基础技能。下面将分为以下一系列文章进行介绍：</p>\n<ul>\n<li><a href=\"/2020/02/12/linux/linux-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/\" title=\"基础知识\">基础知识</a></li>\n<li>常用命令</li>\n<li><a href=\"/2020/03/10/linux/linux-%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/\" title=\"文件及用户权限\">文件及用户权限</a></li>\n<li>shell脚本</li>\n</ul>\n<blockquote>\n<p>[1] 维基百科</p>\n</blockquote>\n"},{"title":"hadoop在windows 10下安装步骤","date":"2020-02-15T12:05:16.000Z","_content":"\n# 序言\n\n首先劝大家在有条件情况下能用mac就用mac，再不行用linux系统，在windows上运行hadoop不是一个好主意！真的好麻烦。。。\n\n<!--more-->\n\n\n\n# 准备文件\n\n- 在官网上下载hadoop的压缩包\n- 然后有个github项目专门做windows下配置的包，具体链接需要自己搜一下\n\n我用的是hadoop 2.10.0，然后配置文件和hadoop包都在里面，需要自己下载\n\n```\n链接：https://pan.baidu.com/s/18ZVB89xOUq43gJ7cqlZUGA \n提取码：wj3v \n```\n\n\n\n# 安装步骤\n\n- 安装好java环境，这是基础，网上一堆教程\n- 解压缩hadoop压缩包，然后解压下载的另一个配置文件，直接拷贝覆盖即可\n\n这里卡了好久，因为文件路径太长经常解压后部分文件无法解压成功，这个可以参考链接：[点这里]( https://knowledge.autodesk.com/zh-hans/search-result/caas/sfdcarticles/sfdcarticles/CHS/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html)，最好放在盘的第一层，我就放在C:\\下面\n\n- 配置hadoop环境变量\n\n  我的电脑->属性->高级系统设置->环境变量->系统变量\n\n  新建HADOOP_HOME, 我的配置：C:\\hadoop-2.10.0\\bin\n\n  ![b146837bly1gbxf3b0kv0j20s9071dfu.jpg](https://i.loli.net/2020/02/20/T9MjvyiPe8rcE5H.jpg)\n\n​     在PATH变量中添加：%HADOOP_HOME%\n\n- 编辑 hadoop安装目录下 etc/hadoop/hadoop-env.cmd, 替换JAVA_HOME路径，\"D:\\program files\\Java\\jdk1.8.0_171\"为JAVA安装路径。\n\n  set JAVA_HOME=\"D:\\program files\\Java\\jdk1.8.0_171\"\n\n  然后编辑在hadoop根目录下创建data目录，目录中再创建两个空文件夹datanode和namenode，之后编辑 etc/hadoop/hdfs-site.xml，替换路径/hadoop-2.10.0/为你的hadoop安装根目录\n\n  ```\n  <configuration>\n   <property>\n          <name>dfs.replication</name>\n          <value>1</value>\n      </property>\n      <property>\n          <name>dfs.namenode.name.dir</name>\n          <value>/hadoop-2.10.0/data/namenode</value>\n      </property>\n      <property>\n          <name>dfs.datanode.data.dir</name>\n          <value>/hadoop-2.10.0/data/datanode</value>\n      </property>\n  </configuration>\n  ```\n\n- 格式化namenode\n\n  ```\n  在任意目录执行 hdfs namenode -format\n  ```\n\n- 到安装根目录下的sbin目录，执行\n\n  ```\n  start-all.cmd\n  ```\n\n  ![b146837bly1gbxfmuf908j20z50li7gm.jpg](https://i.loli.net/2020/02/20/Lupda64fCRmJEHo.jpg)\n\n  验证是否成功：\n\n  ```\n  jps\n  ```\n\n  会有以下进程在运行：\n\n  NodeManager\n  DataNode\n  ResourceManager\n  NameNode\n\n\n\n# 问题及解决方法\n\n```\njava.lang.NoClassDefFoundError: org/apache/hadoop/yarn/server/timelineservice/collector/TimelineCollectorManager\n        at java.lang.ClassLoader.defineClass1(Native Method)\n        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        at java.lang.Class.getDeclaredMethods0(Native Method)\n        at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)\n        at java.lang.Class.getDeclaredMethods(Class.java:1975)\n        at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:688)\n        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:380)\n        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:399)\n        at com.google.inject.internal.BindingBuilder.toInstance(BindingBuilder.java:84)\n        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:56)\n        at org.apache.hadoop.yarn.webapp.WebApp.configureServlets(WebApp.java:160)\n        at com.google.inject.servlet.ServletModule.configure(ServletModule.java:55)\n        at com.google.inject.AbstractModule.configure(AbstractModule.java:62)\n        at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)\n        at com.google.inject.spi.Elements.getElements(Elements.java:110)\n        at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)\n        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)\n        at com.google.inject.Guice.createInjector(Guice.java:96)\n        at com.google.inject.Guice.createInjector(Guice.java:73)\n        at com.google.inject.Guice.createInjector(Guice.java:62)\n        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:356)\n        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:401)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1137)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1245)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1446)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        ... 36 more\n```\n\n**解决方法： **share\\hadoop\\yarn\\timelineservice 下 hadoop-yarn-server-timelineservice-3.0.3.jar copy 到share\\hadoop\\yarn目录下 ","source":"_posts/hadoop/hadoop-hadoop安装.md","raw":"---\ntitle: hadoop在windows 10下安装步骤\ndate: 2020-02-15 20:05:16\ntags: \n    - hadoop\n    - hadoop安装\n    - windows 10\ncategories: \n    - hadoop\n---\n\n# 序言\n\n首先劝大家在有条件情况下能用mac就用mac，再不行用linux系统，在windows上运行hadoop不是一个好主意！真的好麻烦。。。\n\n<!--more-->\n\n\n\n# 准备文件\n\n- 在官网上下载hadoop的压缩包\n- 然后有个github项目专门做windows下配置的包，具体链接需要自己搜一下\n\n我用的是hadoop 2.10.0，然后配置文件和hadoop包都在里面，需要自己下载\n\n```\n链接：https://pan.baidu.com/s/18ZVB89xOUq43gJ7cqlZUGA \n提取码：wj3v \n```\n\n\n\n# 安装步骤\n\n- 安装好java环境，这是基础，网上一堆教程\n- 解压缩hadoop压缩包，然后解压下载的另一个配置文件，直接拷贝覆盖即可\n\n这里卡了好久，因为文件路径太长经常解压后部分文件无法解压成功，这个可以参考链接：[点这里]( https://knowledge.autodesk.com/zh-hans/search-result/caas/sfdcarticles/sfdcarticles/CHS/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html)，最好放在盘的第一层，我就放在C:\\下面\n\n- 配置hadoop环境变量\n\n  我的电脑->属性->高级系统设置->环境变量->系统变量\n\n  新建HADOOP_HOME, 我的配置：C:\\hadoop-2.10.0\\bin\n\n  ![b146837bly1gbxf3b0kv0j20s9071dfu.jpg](https://i.loli.net/2020/02/20/T9MjvyiPe8rcE5H.jpg)\n\n​     在PATH变量中添加：%HADOOP_HOME%\n\n- 编辑 hadoop安装目录下 etc/hadoop/hadoop-env.cmd, 替换JAVA_HOME路径，\"D:\\program files\\Java\\jdk1.8.0_171\"为JAVA安装路径。\n\n  set JAVA_HOME=\"D:\\program files\\Java\\jdk1.8.0_171\"\n\n  然后编辑在hadoop根目录下创建data目录，目录中再创建两个空文件夹datanode和namenode，之后编辑 etc/hadoop/hdfs-site.xml，替换路径/hadoop-2.10.0/为你的hadoop安装根目录\n\n  ```\n  <configuration>\n   <property>\n          <name>dfs.replication</name>\n          <value>1</value>\n      </property>\n      <property>\n          <name>dfs.namenode.name.dir</name>\n          <value>/hadoop-2.10.0/data/namenode</value>\n      </property>\n      <property>\n          <name>dfs.datanode.data.dir</name>\n          <value>/hadoop-2.10.0/data/datanode</value>\n      </property>\n  </configuration>\n  ```\n\n- 格式化namenode\n\n  ```\n  在任意目录执行 hdfs namenode -format\n  ```\n\n- 到安装根目录下的sbin目录，执行\n\n  ```\n  start-all.cmd\n  ```\n\n  ![b146837bly1gbxfmuf908j20z50li7gm.jpg](https://i.loli.net/2020/02/20/Lupda64fCRmJEHo.jpg)\n\n  验证是否成功：\n\n  ```\n  jps\n  ```\n\n  会有以下进程在运行：\n\n  NodeManager\n  DataNode\n  ResourceManager\n  NameNode\n\n\n\n# 问题及解决方法\n\n```\njava.lang.NoClassDefFoundError: org/apache/hadoop/yarn/server/timelineservice/collector/TimelineCollectorManager\n        at java.lang.ClassLoader.defineClass1(Native Method)\n        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        at java.lang.Class.getDeclaredMethods0(Native Method)\n        at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)\n        at java.lang.Class.getDeclaredMethods(Class.java:1975)\n        at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:688)\n        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:380)\n        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:399)\n        at com.google.inject.internal.BindingBuilder.toInstance(BindingBuilder.java:84)\n        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:56)\n        at org.apache.hadoop.yarn.webapp.WebApp.configureServlets(WebApp.java:160)\n        at com.google.inject.servlet.ServletModule.configure(ServletModule.java:55)\n        at com.google.inject.AbstractModule.configure(AbstractModule.java:62)\n        at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)\n        at com.google.inject.spi.Elements.getElements(Elements.java:110)\n        at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)\n        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)\n        at com.google.inject.Guice.createInjector(Guice.java:96)\n        at com.google.inject.Guice.createInjector(Guice.java:73)\n        at com.google.inject.Guice.createInjector(Guice.java:62)\n        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:356)\n        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:401)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1137)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1245)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1446)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        ... 36 more\n```\n\n**解决方法： **share\\hadoop\\yarn\\timelineservice 下 hadoop-yarn-server-timelineservice-3.0.3.jar copy 到share\\hadoop\\yarn目录下 ","slug":"hadoop/hadoop-hadoop安装","published":1,"updated":"2020-02-19T18:21:13.883Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck7cmmo8w0007clzkdi6jc0xq","content":"<h1 id=\"序言\"><a href=\"#序言\" class=\"headerlink\" title=\"序言\"></a>序言</h1><p>首先劝大家在有条件情况下能用mac就用mac，再不行用linux系统，在windows上运行hadoop不是一个好主意！真的好麻烦。。。</p>\n<a id=\"more\"></a>\n\n\n\n<h1 id=\"准备文件\"><a href=\"#准备文件\" class=\"headerlink\" title=\"准备文件\"></a>准备文件</h1><ul>\n<li>在官网上下载hadoop的压缩包</li>\n<li>然后有个github项目专门做windows下配置的包，具体链接需要自己搜一下</li>\n</ul>\n<p>我用的是hadoop 2.10.0，然后配置文件和hadoop包都在里面，需要自己下载</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;18ZVB89xOUq43gJ7cqlZUGA </span><br><span class=\"line\">提取码：wj3v</span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"安装步骤\"><a href=\"#安装步骤\" class=\"headerlink\" title=\"安装步骤\"></a>安装步骤</h1><ul>\n<li>安装好java环境，这是基础，网上一堆教程</li>\n<li>解压缩hadoop压缩包，然后解压下载的另一个配置文件，直接拷贝覆盖即可</li>\n</ul>\n<p>这里卡了好久，因为文件路径太长经常解压后部分文件无法解压成功，这个可以参考链接：<a href=\"https://knowledge.autodesk.com/zh-hans/search-result/caas/sfdcarticles/sfdcarticles/CHS/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html\" target=\"_blank\" rel=\"noopener\">点这里</a>，最好放在盘的第一层，我就放在C:\\下面</p>\n<ul>\n<li><p>配置hadoop环境变量</p>\n<p>我的电脑-&gt;属性-&gt;高级系统设置-&gt;环境变量-&gt;系统变量</p>\n<p>新建HADOOP_HOME, 我的配置：C:\\hadoop-2.10.0\\bin</p>\n<p><img src=\"https://i.loli.net/2020/02/20/T9MjvyiPe8rcE5H.jpg\" alt=\"b146837bly1gbxf3b0kv0j20s9071dfu.jpg\"></p>\n</li>\n</ul>\n<p>​     在PATH变量中添加：%HADOOP_HOME%</p>\n<ul>\n<li><p>编辑 hadoop安装目录下 etc/hadoop/hadoop-env.cmd, 替换JAVA_HOME路径，”D:\\program files\\Java\\jdk1.8.0_171”为JAVA安装路径。</p>\n<p>set JAVA_HOME=”D:\\program files\\Java\\jdk1.8.0_171”</p>\n<p>然后编辑在hadoop根目录下创建data目录，目录中再创建两个空文件夹datanode和namenode，之后编辑 etc/hadoop/hdfs-site.xml，替换路径/hadoop-2.10.0/为你的hadoop安装根目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\"> &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class=\"line\">        &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class=\"line\">    &lt;&#x2F;property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class=\"line\">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;namenode&lt;&#x2F;value&gt;</span><br><span class=\"line\">    &lt;&#x2F;property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class=\"line\">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;datanode&lt;&#x2F;value&gt;</span><br><span class=\"line\">    &lt;&#x2F;property&gt;</span><br><span class=\"line\">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>格式化namenode</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在任意目录执行 hdfs namenode -format</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>到安装根目录下的sbin目录，执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">start-all.cmd</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://i.loli.net/2020/02/20/Lupda64fCRmJEHo.jpg\" alt=\"b146837bly1gbxfmuf908j20z50li7gm.jpg\"></p>\n<p>验证是否成功：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jps</span><br></pre></td></tr></table></figure>\n\n<p>会有以下进程在运行：</p>\n<p>NodeManager<br>DataNode<br>ResourceManager<br>NameNode</p>\n</li>\n</ul>\n<h1 id=\"问题及解决方法\"><a href=\"#问题及解决方法\" class=\"headerlink\" title=\"问题及解决方法\"></a>问题及解决方法</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;yarn&#x2F;server&#x2F;timelineservice&#x2F;collector&#x2F;TimelineCollectorManager</span><br><span class=\"line\">        at java.lang.ClassLoader.defineClass1(Native Method)</span><br><span class=\"line\">        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)</span><br><span class=\"line\">        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)</span><br><span class=\"line\">        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)</span><br><span class=\"line\">        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)</span><br><span class=\"line\">        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)</span><br><span class=\"line\">        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)</span><br><span class=\"line\">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class=\"line\">        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class=\"line\">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class=\"line\">        at java.lang.Class.getDeclaredMethods0(Native Method)</span><br><span class=\"line\">        at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)</span><br><span class=\"line\">        at java.lang.Class.getDeclaredMethods(Class.java:1975)</span><br><span class=\"line\">        at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:688)</span><br><span class=\"line\">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:380)</span><br><span class=\"line\">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:399)</span><br><span class=\"line\">        at com.google.inject.internal.BindingBuilder.toInstance(BindingBuilder.java:84)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:56)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.webapp.WebApp.configureServlets(WebApp.java:160)</span><br><span class=\"line\">        at com.google.inject.servlet.ServletModule.configure(ServletModule.java:55)</span><br><span class=\"line\">        at com.google.inject.AbstractModule.configure(AbstractModule.java:62)</span><br><span class=\"line\">        at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)</span><br><span class=\"line\">        at com.google.inject.spi.Elements.getElements(Elements.java:110)</span><br><span class=\"line\">        at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)</span><br><span class=\"line\">        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)</span><br><span class=\"line\">        at com.google.inject.Guice.createInjector(Guice.java:96)</span><br><span class=\"line\">        at com.google.inject.Guice.createInjector(Guice.java:73)</span><br><span class=\"line\">        at com.google.inject.Guice.createInjector(Guice.java:62)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:356)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:401)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1137)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1245)</span><br><span class=\"line\">        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1446)</span><br><span class=\"line\">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager</span><br><span class=\"line\">        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class=\"line\">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class=\"line\">        ... 36 more</span><br></pre></td></tr></table></figure>\n\n<p>*<em>解决方法： *</em>share\\hadoop\\yarn\\timelineservice 下 hadoop-yarn-server-timelineservice-3.0.3.jar copy 到share\\hadoop\\yarn目录下 </p>\n","site":{"data":{}},"excerpt":"<h1 id=\"序言\"><a href=\"#序言\" class=\"headerlink\" title=\"序言\"></a>序言</h1><p>首先劝大家在有条件情况下能用mac就用mac，再不行用linux系统，在windows上运行hadoop不是一个好主意！真的好麻烦。。。</p>","more":"<h1 id=\"准备文件\"><a href=\"#准备文件\" class=\"headerlink\" title=\"准备文件\"></a>准备文件</h1><ul>\n<li>在官网上下载hadoop的压缩包</li>\n<li>然后有个github项目专门做windows下配置的包，具体链接需要自己搜一下</li>\n</ul>\n<p>我用的是hadoop 2.10.0，然后配置文件和hadoop包都在里面，需要自己下载</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;18ZVB89xOUq43gJ7cqlZUGA </span><br><span class=\"line\">提取码：wj3v</span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"安装步骤\"><a href=\"#安装步骤\" class=\"headerlink\" title=\"安装步骤\"></a>安装步骤</h1><ul>\n<li>安装好java环境，这是基础，网上一堆教程</li>\n<li>解压缩hadoop压缩包，然后解压下载的另一个配置文件，直接拷贝覆盖即可</li>\n</ul>\n<p>这里卡了好久，因为文件路径太长经常解压后部分文件无法解压成功，这个可以参考链接：<a href=\"https://knowledge.autodesk.com/zh-hans/search-result/caas/sfdcarticles/sfdcarticles/CHS/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html\" target=\"_blank\" rel=\"noopener\">点这里</a>，最好放在盘的第一层，我就放在C:\\下面</p>\n<ul>\n<li><p>配置hadoop环境变量</p>\n<p>我的电脑-&gt;属性-&gt;高级系统设置-&gt;环境变量-&gt;系统变量</p>\n<p>新建HADOOP_HOME, 我的配置：C:\\hadoop-2.10.0\\bin</p>\n<p><img src=\"https://i.loli.net/2020/02/20/T9MjvyiPe8rcE5H.jpg\" alt=\"b146837bly1gbxf3b0kv0j20s9071dfu.jpg\"></p>\n</li>\n</ul>\n<p>​     在PATH变量中添加：%HADOOP_HOME%</p>\n<ul>\n<li><p>编辑 hadoop安装目录下 etc/hadoop/hadoop-env.cmd, 替换JAVA_HOME路径，”D:\\program files\\Java\\jdk1.8.0_171”为JAVA安装路径。</p>\n<p>set JAVA_HOME=”D:\\program files\\Java\\jdk1.8.0_171”</p>\n<p>然后编辑在hadoop根目录下创建data目录，目录中再创建两个空文件夹datanode和namenode，之后编辑 etc/hadoop/hdfs-site.xml，替换路径/hadoop-2.10.0/为你的hadoop安装根目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;configuration&gt;</span><br><span class=\"line\"> &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class=\"line\">        &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class=\"line\">    &lt;&#x2F;property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class=\"line\">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;namenode&lt;&#x2F;value&gt;</span><br><span class=\"line\">    &lt;&#x2F;property&gt;</span><br><span class=\"line\">    &lt;property&gt;</span><br><span class=\"line\">        &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class=\"line\">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;datanode&lt;&#x2F;value&gt;</span><br><span class=\"line\">    &lt;&#x2F;property&gt;</span><br><span class=\"line\">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>格式化namenode</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">在任意目录执行 hdfs namenode -format</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>到安装根目录下的sbin目录，执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">start-all.cmd</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://i.loli.net/2020/02/20/Lupda64fCRmJEHo.jpg\" alt=\"b146837bly1gbxfmuf908j20z50li7gm.jpg\"></p>\n<p>验证是否成功：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jps</span><br></pre></td></tr></table></figure>\n\n<p>会有以下进程在运行：</p>\n<p>NodeManager<br>DataNode<br>ResourceManager<br>NameNode</p>\n</li>\n</ul>\n<h1 id=\"问题及解决方法\"><a href=\"#问题及解决方法\" class=\"headerlink\" title=\"问题及解决方法\"></a>问题及解决方法</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;yarn&#x2F;server&#x2F;timelineservice&#x2F;collector&#x2F;TimelineCollectorManager</span><br><span class=\"line\">        at java.lang.ClassLoader.defineClass1(Native Method)</span><br><span class=\"line\">        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)</span><br><span class=\"line\">        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)</span><br><span class=\"line\">        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)</span><br><span class=\"line\">        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)</span><br><span class=\"line\">        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)</span><br><span class=\"line\">        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)</span><br><span class=\"line\">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class=\"line\">        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class=\"line\">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class=\"line\">        at java.lang.Class.getDeclaredMethods0(Native Method)</span><br><span class=\"line\">        at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)</span><br><span class=\"line\">        at java.lang.Class.getDeclaredMethods(Class.java:1975)</span><br><span class=\"line\">        at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:688)</span><br><span class=\"line\">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:380)</span><br><span class=\"line\">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:399)</span><br><span class=\"line\">        at com.google.inject.internal.BindingBuilder.toInstance(BindingBuilder.java:84)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:56)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.webapp.WebApp.configureServlets(WebApp.java:160)</span><br><span class=\"line\">        at com.google.inject.servlet.ServletModule.configure(ServletModule.java:55)</span><br><span class=\"line\">        at com.google.inject.AbstractModule.configure(AbstractModule.java:62)</span><br><span class=\"line\">        at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)</span><br><span class=\"line\">        at com.google.inject.spi.Elements.getElements(Elements.java:110)</span><br><span class=\"line\">        at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)</span><br><span class=\"line\">        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)</span><br><span class=\"line\">        at com.google.inject.Guice.createInjector(Guice.java:96)</span><br><span class=\"line\">        at com.google.inject.Guice.createInjector(Guice.java:73)</span><br><span class=\"line\">        at com.google.inject.Guice.createInjector(Guice.java:62)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:356)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:401)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1137)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1245)</span><br><span class=\"line\">        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)</span><br><span class=\"line\">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1446)</span><br><span class=\"line\">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager</span><br><span class=\"line\">        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class=\"line\">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class=\"line\">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class=\"line\">        ... 36 more</span><br></pre></td></tr></table></figure>\n\n<p>*<em>解决方法： *</em>share\\hadoop\\yarn\\timelineservice 下 hadoop-yarn-server-timelineservice-3.0.3.jar copy 到share\\hadoop\\yarn目录下 </p>"},{"title":"基础知识","date":"2020-02-11T16:21:21.000Z","_content":"\n\n\n# 计算机\n\n### 组成部分\n\n- 输入单元\n- CPU<!--more-->\n- 内存\n- 外部存储设备\n- 输出单元\n\n\n\n### 分类\n\n- 超级计算机\n- 大型计算机\n- 迷你计算机\n- 工作站\n- 微电脑\n\n\n\n### 文件大小\n\nB=>K=>M=>G=>T=>P=>E\n\n关系都是1024的倍数，如1M=1024K\n\n\n\n### 结构层次\n\n网络图片，侵删\n\n![image.png](https://i.loli.net/2020/03/10/kmq52KJte6GyuzW.png)\n\n\n\n普通用户熟悉的是操作系统和应用程序。linux操作系统的核心为linux内核。在linux中，常常用到命令行工具，称为shell。shell是一个用户调用接口，包含各种命令可以和系统交互。内核和系统硬件进行交互。\n\n\n\n\n\n# 帮助\n\n因为传统linux系统只有命令行模式，所以需要使用各种命令，但是记住所有的命令和参数这是几乎不可能的，所以知道如何获取帮助非常重要。\n\n获取帮助的命令主要有三个，help, man, info。这三个得到的查询结果详细程度依次递增。\n\n**help命令**\n\nhelp 命令经常使用，可以简洁的列出命令使用方法\n\n示例：\n\n```\nhelp echo\n```\n\n\n\n![b146837bly1gbsy3grssnj21fd09qwff.jpg](https://i.loli.net/2020/02/20/jNaxOQpHEnBAWZy.jpg)\n\n","source":"_posts/linux/linux-基础知识.md","raw":"---\ntitle: 基础知识\ndate: 2020-02-12 00:21:21\ntags:\n    - Linux\n    - 基础知识\ncategories: \n    - Linux\n---\n\n\n\n# 计算机\n\n### 组成部分\n\n- 输入单元\n- CPU<!--more-->\n- 内存\n- 外部存储设备\n- 输出单元\n\n\n\n### 分类\n\n- 超级计算机\n- 大型计算机\n- 迷你计算机\n- 工作站\n- 微电脑\n\n\n\n### 文件大小\n\nB=>K=>M=>G=>T=>P=>E\n\n关系都是1024的倍数，如1M=1024K\n\n\n\n### 结构层次\n\n网络图片，侵删\n\n![image.png](https://i.loli.net/2020/03/10/kmq52KJte6GyuzW.png)\n\n\n\n普通用户熟悉的是操作系统和应用程序。linux操作系统的核心为linux内核。在linux中，常常用到命令行工具，称为shell。shell是一个用户调用接口，包含各种命令可以和系统交互。内核和系统硬件进行交互。\n\n\n\n\n\n# 帮助\n\n因为传统linux系统只有命令行模式，所以需要使用各种命令，但是记住所有的命令和参数这是几乎不可能的，所以知道如何获取帮助非常重要。\n\n获取帮助的命令主要有三个，help, man, info。这三个得到的查询结果详细程度依次递增。\n\n**help命令**\n\nhelp 命令经常使用，可以简洁的列出命令使用方法\n\n示例：\n\n```\nhelp echo\n```\n\n\n\n![b146837bly1gbsy3grssnj21fd09qwff.jpg](https://i.loli.net/2020/02/20/jNaxOQpHEnBAWZy.jpg)\n\n","slug":"linux/linux-基础知识","published":1,"updated":"2020-03-09T16:27:33.449Z","_id":"ck7cmmo8x0008clzk57cahwwp","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"计算机\"><a href=\"#计算机\" class=\"headerlink\" title=\"计算机\"></a>计算机</h1><h3 id=\"组成部分\"><a href=\"#组成部分\" class=\"headerlink\" title=\"组成部分\"></a>组成部分</h3><ul>\n<li>输入单元</li>\n<li>CPU<a id=\"more\"></a></li>\n<li>内存</li>\n<li>外部存储设备</li>\n<li>输出单元</li>\n</ul>\n<h3 id=\"分类\"><a href=\"#分类\" class=\"headerlink\" title=\"分类\"></a>分类</h3><ul>\n<li>超级计算机</li>\n<li>大型计算机</li>\n<li>迷你计算机</li>\n<li>工作站</li>\n<li>微电脑</li>\n</ul>\n<h3 id=\"文件大小\"><a href=\"#文件大小\" class=\"headerlink\" title=\"文件大小\"></a>文件大小</h3><p>B=&gt;K=&gt;M=&gt;G=&gt;T=&gt;P=&gt;E</p>\n<p>关系都是1024的倍数，如1M=1024K</p>\n<h3 id=\"结构层次\"><a href=\"#结构层次\" class=\"headerlink\" title=\"结构层次\"></a>结构层次</h3><p>网络图片，侵删</p>\n<p><img src=\"https://i.loli.net/2020/03/10/kmq52KJte6GyuzW.png\" alt=\"image.png\"></p>\n<p>普通用户熟悉的是操作系统和应用程序。linux操作系统的核心为linux内核。在linux中，常常用到命令行工具，称为shell。shell是一个用户调用接口，包含各种命令可以和系统交互。内核和系统硬件进行交互。</p>\n<h1 id=\"帮助\"><a href=\"#帮助\" class=\"headerlink\" title=\"帮助\"></a>帮助</h1><p>因为传统linux系统只有命令行模式，所以需要使用各种命令，但是记住所有的命令和参数这是几乎不可能的，所以知道如何获取帮助非常重要。</p>\n<p>获取帮助的命令主要有三个，help, man, info。这三个得到的查询结果详细程度依次递增。</p>\n<p><strong>help命令</strong></p>\n<p>help 命令经常使用，可以简洁的列出命令使用方法</p>\n<p>示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">help echo</span><br></pre></td></tr></table></figure>\n\n\n\n<p><img src=\"https://i.loli.net/2020/02/20/jNaxOQpHEnBAWZy.jpg\" alt=\"b146837bly1gbsy3grssnj21fd09qwff.jpg\"></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"计算机\"><a href=\"#计算机\" class=\"headerlink\" title=\"计算机\"></a>计算机</h1><h3 id=\"组成部分\"><a href=\"#组成部分\" class=\"headerlink\" title=\"组成部分\"></a>组成部分</h3><ul>\n<li>输入单元</li>\n<li>CPU","more":"</li>\n<li>内存</li>\n<li>外部存储设备</li>\n<li>输出单元</li>\n</ul>\n<h3 id=\"分类\"><a href=\"#分类\" class=\"headerlink\" title=\"分类\"></a>分类</h3><ul>\n<li>超级计算机</li>\n<li>大型计算机</li>\n<li>迷你计算机</li>\n<li>工作站</li>\n<li>微电脑</li>\n</ul>\n<h3 id=\"文件大小\"><a href=\"#文件大小\" class=\"headerlink\" title=\"文件大小\"></a>文件大小</h3><p>B=&gt;K=&gt;M=&gt;G=&gt;T=&gt;P=&gt;E</p>\n<p>关系都是1024的倍数，如1M=1024K</p>\n<h3 id=\"结构层次\"><a href=\"#结构层次\" class=\"headerlink\" title=\"结构层次\"></a>结构层次</h3><p>网络图片，侵删</p>\n<p><img src=\"https://i.loli.net/2020/03/10/kmq52KJte6GyuzW.png\" alt=\"image.png\"></p>\n<p>普通用户熟悉的是操作系统和应用程序。linux操作系统的核心为linux内核。在linux中，常常用到命令行工具，称为shell。shell是一个用户调用接口，包含各种命令可以和系统交互。内核和系统硬件进行交互。</p>\n<h1 id=\"帮助\"><a href=\"#帮助\" class=\"headerlink\" title=\"帮助\"></a>帮助</h1><p>因为传统linux系统只有命令行模式，所以需要使用各种命令，但是记住所有的命令和参数这是几乎不可能的，所以知道如何获取帮助非常重要。</p>\n<p>获取帮助的命令主要有三个，help, man, info。这三个得到的查询结果详细程度依次递增。</p>\n<p><strong>help命令</strong></p>\n<p>help 命令经常使用，可以简洁的列出命令使用方法</p>\n<p>示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">help echo</span><br></pre></td></tr></table></figure>\n\n\n\n<p><img src=\"https://i.loli.net/2020/02/20/jNaxOQpHEnBAWZy.jpg\" alt=\"b146837bly1gbsy3grssnj21fd09qwff.jpg\"></p>"},{"title":"hive udf&udaf","date":"2020-02-21T10:20:21.000Z","_content":"\n\n\nhive为我们定义了很多函数，大多数情况下是能够满足我们的需求的。但是在有些情况下，很有必要自己定义一些函数，这样使用起来非常方便。这就是自定义函数(udf)和自定义聚合函数(udaf,user defined aggregation function)。udf是每行返回一个结果，而udaf则是聚合的结果，多行产生一个结果。另外还有udtf，是一行产生多个结果。目前我还没有用到过udtf，就先不介绍它了。下面分别用两个示例介绍udf和udaf<!--more-->\n\n\n\n# udf\n\n### 功能\n\n查找array中是否包含被查询值\n\n\n\n### 步骤\n\n- 测试数据准备\n\n  ```\n  zhangsan        beijing,shanghai,tianjin,hangzhou\n  lisi    changchu,chengdu,wuhan\n  ```\n\n- hive建表与导入\n\n  ```\n  Create table users(name string, worklocations array<string> ) row format delimited fields terminated by '\\t' collection items terminated by ','; \n  \n  load data local inpath '/root/person.txt ' OVERWRITE INTO TABLE users; \n  ```\n\n- udf包生成与导入\n\n  ```\n  package com.will;\n  \n  import org.apache.hadoop.hive.ql.exec.UDF;\n  import java.util.ArrayList;\n  \n  public class FindInArray extends UDF {\n      public ArrayList<String> evaluate(String keywords, ArrayList<String> column){\n          //参数类型使用arraylist<String>对应hive中的array<string>,而不是String[]\n          if(column.contains(keywords)){\n              return column;\n          }else{\n              return null;\n          }\n      }\n      \n      public String evaluate(String keywords,ArrayList<String> column,String name){\n          //重载evaluate，另一种查询方式，返回name值\n          if(column.contains(keywords)){\n              return name;\n          }else{\n              return null;\n          }\n      }\n  }\n  ```\n\n  使用mvn 打包\n\n  ```\n  mvn clean package\n  ```\n\n- 导入hive\n\n  ```\n  add jar /home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar;\n  create temporary function find_in_array as 'com.will.FindInArray';\n  ```\n\n- 使用\n\n  ```\n  hive> select find_in_array('beijing',worklocations) from users;\n  OK\n  [\"beijing\",\"shanghai\",\"tianjin\",\"hangzhou\"]\n  NULL\n  Time taken: 0.424 seconds, Fetched: 2 row(s)\n  ```\n\n> 参考：https://blog.csdn.net/Nougats/article/details/71158318\n\n\n\n# udaf\n\n> 参考： \n>\n> https://blog.51cto.com/xiaolanlan/2397771\n>\n> https://www.cnblogs.com/Rudd/p/5137612.html\n\n\n\n### 基础知识\n\nhive的udtf有两种，Simple和Generic。这是由于历史问题，Simple先出现，但是因为反射问题效率较低，所以现在推荐使用Generic的写法。\n\n- Simple。即继承`org.apache.hadoop.hive.ql.exec.UDAF`类，并在派生类中以静态内部类的方式实现`org.apache.hadoop.hive.ql.exec.UDAFEvaluator`接口。在Hive源码包`org.apache.hadoop.hive.contrib.udaf.example`中包含几个示例。可以直接参阅。但是这些接口已经被注解为Deprecated，建议不要使用这种方式开发新的UDAF函数。\n- Generic。这是Hive社区推荐的新的写法，以抽象类代替原有的接口。新的抽象类`org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver`替代老的UDAF接口，新的抽象类`org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator`替代老的UDAFEvaluator接口。\n\n\n\nhive是立足于hadoop之上，也就是hive基于mapreduce，hive sql最终还是会转化为mapreduce执行。为了实现mapreduce，udaf中用model来表示mapreduce各个阶段。\n\n```\npublic static enum Mode {\n        PARTIAL1,\n        PARTIAL2,\n        FINAL,\n        COMPLETE;\n\n        private Mode() {}\n    }\n```\n\n- PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合，将会调用**iterate()**和**terminatePartial() **\n-  PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合，将会调用**merge()** 和 **terminatePartial()** \n- FINAL: mapreduce的reduce阶段，从部分数据的聚合到完全聚合，将会调用**merge()**和**terminate() **\n\n---\n\n- COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了，从原始数据直接到完全聚合，将会调用 **iterate()**和**terminate()**\n\n ![image.png](https://i.loli.net/2020/02/23/DlNqFRHnZ6zLuoA.png)\n\n![image.png](https://i.loli.net/2020/02/23/IKn3Vv57jPScxs8.png)\n\n\n\nudaf骨架示例：\n\n```\npublic class GenericUDAFHistogramNumeric extends AbstractGenericUDAFResolver {\n  static final Log LOG = LogFactory.getLog(GenericUDAFHistogramNumeric.class.getName());\n \n  @Override\n  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException {\n    // 这里主要做类型检查\n \n    return new GenericUDAFHistogramNumericEvaluator();\n  }\n \n  public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator {\n         // 确定各个阶段输入输出参数的数据格式ObjectInspectors\n         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException{\n             return  null;\n         }\n\n         // 保存数据聚集结果的类\n         public AggregationBuffer getNewAggregationBuffer() throws HiveException {\n             return null;\n         }\n\n\t\t // 重置聚集结果\n         public void reset(AggregationBuffer aggregationBuffer) throws HiveException {}\n\n         // map阶段，迭代处理输入sql传过来的列数据 \n         public void iterate(AggregationBuffer aggregationBuffer, Object[] objects) throws HiveException {}\n\n         // map与combiner结束返回结果，得到部分数据聚集结果\n         public Object terminatePartial(AggregationBuffer aggregationBuffer) throws HiveException {\n             return null;\n         }\n\n         // combiner合并map返回的结果，还有reducer合并mapper或combiner返回的结果。\n         public void merge(AggregationBuffer aggregationBuffer, Object o) throws HiveException {}\n\n\t\t // reducer阶段，输出最终结果 \n         public Object terminate(AggregationBuffer aggregationBuffer) throws HiveException {\n             return null;\n         }\n  }\n}\n```\n\n\n\n\n\n### 功能\n\n统计字符数\n\n\n\n### 代码\n\n```\npackage com.will;\n\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.parse.SemanticException;\nimport org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;\nimport org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\nimport org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\nimport org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\nimport org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n\npublic class TotalNumOfLetttersGenericUDAF extends AbstractGenericUDAFResolver {\n    @Override\n    public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException {\n\n        if (parameters.length != 1) {\n            throw new UDFArgumentTypeException(parameters.length - 1,\"Exactly one argument is expected.\");\n        }\n\n        ObjectInspector oi = TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(parameters[0]);\n\n        if (oi.getCategory() != ObjectInspector.Category.PRIMITIVE){\n            throw new UDFArgumentTypeException(0,\n                    \"Argument must be PRIMITIVE, but \"\n                    + oi.getCategory().name()\n                    + \" was passed.\");\n        }\n\n        PrimitiveObjectInspector inputOI = (PrimitiveObjectInspector) oi;\n        if (inputOI.getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.STRING){\n            throw new UDFArgumentTypeException(0, \"Argument must be String, but \"\n                     + inputOI.getPrimitiveCategory().name()\n                     + \" was passed.\");\n        }\n\n        return new TotalNumOfLettersEvaluator();\n    }\n\n    public static class TotalNumOfLettersEvaluator extends GenericUDAFEvaluator{\n        PrimitiveObjectInspector inputOI;\n        ObjectInspector outputOI;\n        PrimitiveObjectInspector integerOI;\n\n        int total = 0;\n        private boolean warned = false;\n\n        @Override\n         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException{\n            assert (parameters.length == 1);\n            super.init(m, parameters);\n\n            //map阶段读取sql列，输入为String基础数据格式\n            if (m == Mode.PARTIAL1 || m == Mode.COMPLETE) {\n                inputOI = (PrimitiveObjectInspector) parameters[0];\n            } else {\n                //其余阶段，输入为Integer基础数据格式\n                integerOI = (PrimitiveObjectInspector) parameters[0];\n            }\n\n            // 指定各个阶段输出数据格式都为Integer类型\n            outputOI = ObjectInspectorFactory.getReflectionObjectInspector(Integer.class,\n                                        ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n\n            return outputOI;\n        }\n\n        //存储当前字符总数的类\n        static class LetterSumAgg implements AggregationBuffer {\n            int sum = 0;\n            void add(int num){\n                sum += num;\n            }\n        }\n\n         public AggregationBuffer getNewAggregationBuffer() throws HiveException {\n             LetterSumAgg result = new LetterSumAgg();\n             return result;\n         }\n\n         public void reset(AggregationBuffer aggregationBuffer) throws HiveException {\n             LetterSumAgg myagg = new LetterSumAgg();\n         }\n\n         public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {\n             assert (parameters.length == 1);\n             if (parameters[0] != null) {\n                 LetterSumAgg myagg = (LetterSumAgg) agg;\n                 Object p1 = ((PrimitiveObjectInspector) inputOI).getPrimitiveJavaObject(parameters[0]);\n                 myagg.add(String.valueOf(p1).length());\n             }\n         }\n\n         public Object terminatePartial(AggregationBuffer agg) throws HiveException {\n             LetterSumAgg myagg = (LetterSumAgg) agg;\n             total += myagg.sum;\n             return total;\n         }\n\n         public void merge(AggregationBuffer agg, Object partial) throws HiveException {\n             if (partial != null) {\n                 LetterSumAgg myagg1 = (LetterSumAgg) agg;\n                 Integer partialSum = (Integer) integerOI.getPrimitiveJavaObject(partial);\n                 LetterSumAgg myagg2 = new LetterSumAgg();\n                 myagg2.add(partialSum);\n                 myagg1.add(myagg2.sum);\n             }\n         }\n\n         public Object terminate(AggregationBuffer agg) throws HiveException {\n             LetterSumAgg myagg = (LetterSumAgg) agg;\n             total = myagg.sum;\n             return myagg.sum;\n         }\n    }\n}\n\n```\n\n\n\n### 验证\n\n首先准备数据\n\n```\nhive> select * from users;\nOK\nzhangsan\t[\"beijing\",\"shanghai\",\"tianjin\",\"hangzhou\"]\nlisi\t[\"changchu\",\"chengdu\",\"wuhan\"]\n```\n\n\n\n然后添加jar包\n\n```\n> ADD JAR /home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar;\nAdded [/home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar] to class path\nAdded resources: [/home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar]\n```\n\n\n\n定义函数\n\n```\nhive>  CREATE TEMPORARY FUNCTION letters as 'com.will.TotalNumOfLetttersGenericUDAF';\nOK\nTime taken: 0.049 seconds\n```\n\n\n\n执行\n\n```\nhive> select letters(name) from users;\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n2020-02-23 13:25:06,087 Stage-1 map = 0%,  reduce = 0%\n2020-02-23 13:25:11,426 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.03 sec\n2020-02-23 13:25:16,607 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.01 sec\nMapReduce Total cumulative CPU time: 4 seconds 10 msec\nTotal MapReduce CPU Time Spent: 4 seconds 10 msec\nOK\n12\nTime taken: 23.819 seconds, Fetched: 1 row(s)\n```\n\n\n\n# my own udaf\n\n### 要解决的问题\n\n有一个hive表，其中两列分别代表时间戳和事件。目标是得到指定时间范围的所有事件。\n\n\n\n### 分析\n\n根据上一部分的介绍，要实现聚合首先要设计如何存储，传输的问题。在这个过程中我仔细研究了hive udaf示例的histogram设计，然后设计了自己的udaf。聚合存储使用hashmap，初步解析结果使用string的list保存。\n\n\n\n代码\n\n[github 演示项目](https://github.com/zcenao21/hive-udaf)","source":"_posts/hive/hive-udf.md","raw":"---\ntitle: hive udf&udaf\ndate: 2020-02-21 18:20:21\ntags:\n    - hive\n    - udf\n    - udaf\ncategories: \n    - Hive\n---\n\n\n\nhive为我们定义了很多函数，大多数情况下是能够满足我们的需求的。但是在有些情况下，很有必要自己定义一些函数，这样使用起来非常方便。这就是自定义函数(udf)和自定义聚合函数(udaf,user defined aggregation function)。udf是每行返回一个结果，而udaf则是聚合的结果，多行产生一个结果。另外还有udtf，是一行产生多个结果。目前我还没有用到过udtf，就先不介绍它了。下面分别用两个示例介绍udf和udaf<!--more-->\n\n\n\n# udf\n\n### 功能\n\n查找array中是否包含被查询值\n\n\n\n### 步骤\n\n- 测试数据准备\n\n  ```\n  zhangsan        beijing,shanghai,tianjin,hangzhou\n  lisi    changchu,chengdu,wuhan\n  ```\n\n- hive建表与导入\n\n  ```\n  Create table users(name string, worklocations array<string> ) row format delimited fields terminated by '\\t' collection items terminated by ','; \n  \n  load data local inpath '/root/person.txt ' OVERWRITE INTO TABLE users; \n  ```\n\n- udf包生成与导入\n\n  ```\n  package com.will;\n  \n  import org.apache.hadoop.hive.ql.exec.UDF;\n  import java.util.ArrayList;\n  \n  public class FindInArray extends UDF {\n      public ArrayList<String> evaluate(String keywords, ArrayList<String> column){\n          //参数类型使用arraylist<String>对应hive中的array<string>,而不是String[]\n          if(column.contains(keywords)){\n              return column;\n          }else{\n              return null;\n          }\n      }\n      \n      public String evaluate(String keywords,ArrayList<String> column,String name){\n          //重载evaluate，另一种查询方式，返回name值\n          if(column.contains(keywords)){\n              return name;\n          }else{\n              return null;\n          }\n      }\n  }\n  ```\n\n  使用mvn 打包\n\n  ```\n  mvn clean package\n  ```\n\n- 导入hive\n\n  ```\n  add jar /home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar;\n  create temporary function find_in_array as 'com.will.FindInArray';\n  ```\n\n- 使用\n\n  ```\n  hive> select find_in_array('beijing',worklocations) from users;\n  OK\n  [\"beijing\",\"shanghai\",\"tianjin\",\"hangzhou\"]\n  NULL\n  Time taken: 0.424 seconds, Fetched: 2 row(s)\n  ```\n\n> 参考：https://blog.csdn.net/Nougats/article/details/71158318\n\n\n\n# udaf\n\n> 参考： \n>\n> https://blog.51cto.com/xiaolanlan/2397771\n>\n> https://www.cnblogs.com/Rudd/p/5137612.html\n\n\n\n### 基础知识\n\nhive的udtf有两种，Simple和Generic。这是由于历史问题，Simple先出现，但是因为反射问题效率较低，所以现在推荐使用Generic的写法。\n\n- Simple。即继承`org.apache.hadoop.hive.ql.exec.UDAF`类，并在派生类中以静态内部类的方式实现`org.apache.hadoop.hive.ql.exec.UDAFEvaluator`接口。在Hive源码包`org.apache.hadoop.hive.contrib.udaf.example`中包含几个示例。可以直接参阅。但是这些接口已经被注解为Deprecated，建议不要使用这种方式开发新的UDAF函数。\n- Generic。这是Hive社区推荐的新的写法，以抽象类代替原有的接口。新的抽象类`org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver`替代老的UDAF接口，新的抽象类`org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator`替代老的UDAFEvaluator接口。\n\n\n\nhive是立足于hadoop之上，也就是hive基于mapreduce，hive sql最终还是会转化为mapreduce执行。为了实现mapreduce，udaf中用model来表示mapreduce各个阶段。\n\n```\npublic static enum Mode {\n        PARTIAL1,\n        PARTIAL2,\n        FINAL,\n        COMPLETE;\n\n        private Mode() {}\n    }\n```\n\n- PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合，将会调用**iterate()**和**terminatePartial() **\n-  PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合，将会调用**merge()** 和 **terminatePartial()** \n- FINAL: mapreduce的reduce阶段，从部分数据的聚合到完全聚合，将会调用**merge()**和**terminate() **\n\n---\n\n- COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了，从原始数据直接到完全聚合，将会调用 **iterate()**和**terminate()**\n\n ![image.png](https://i.loli.net/2020/02/23/DlNqFRHnZ6zLuoA.png)\n\n![image.png](https://i.loli.net/2020/02/23/IKn3Vv57jPScxs8.png)\n\n\n\nudaf骨架示例：\n\n```\npublic class GenericUDAFHistogramNumeric extends AbstractGenericUDAFResolver {\n  static final Log LOG = LogFactory.getLog(GenericUDAFHistogramNumeric.class.getName());\n \n  @Override\n  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException {\n    // 这里主要做类型检查\n \n    return new GenericUDAFHistogramNumericEvaluator();\n  }\n \n  public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator {\n         // 确定各个阶段输入输出参数的数据格式ObjectInspectors\n         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException{\n             return  null;\n         }\n\n         // 保存数据聚集结果的类\n         public AggregationBuffer getNewAggregationBuffer() throws HiveException {\n             return null;\n         }\n\n\t\t // 重置聚集结果\n         public void reset(AggregationBuffer aggregationBuffer) throws HiveException {}\n\n         // map阶段，迭代处理输入sql传过来的列数据 \n         public void iterate(AggregationBuffer aggregationBuffer, Object[] objects) throws HiveException {}\n\n         // map与combiner结束返回结果，得到部分数据聚集结果\n         public Object terminatePartial(AggregationBuffer aggregationBuffer) throws HiveException {\n             return null;\n         }\n\n         // combiner合并map返回的结果，还有reducer合并mapper或combiner返回的结果。\n         public void merge(AggregationBuffer aggregationBuffer, Object o) throws HiveException {}\n\n\t\t // reducer阶段，输出最终结果 \n         public Object terminate(AggregationBuffer aggregationBuffer) throws HiveException {\n             return null;\n         }\n  }\n}\n```\n\n\n\n\n\n### 功能\n\n统计字符数\n\n\n\n### 代码\n\n```\npackage com.will;\n\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.parse.SemanticException;\nimport org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;\nimport org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\nimport org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\nimport org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\nimport org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;\n\npublic class TotalNumOfLetttersGenericUDAF extends AbstractGenericUDAFResolver {\n    @Override\n    public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException {\n\n        if (parameters.length != 1) {\n            throw new UDFArgumentTypeException(parameters.length - 1,\"Exactly one argument is expected.\");\n        }\n\n        ObjectInspector oi = TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(parameters[0]);\n\n        if (oi.getCategory() != ObjectInspector.Category.PRIMITIVE){\n            throw new UDFArgumentTypeException(0,\n                    \"Argument must be PRIMITIVE, but \"\n                    + oi.getCategory().name()\n                    + \" was passed.\");\n        }\n\n        PrimitiveObjectInspector inputOI = (PrimitiveObjectInspector) oi;\n        if (inputOI.getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.STRING){\n            throw new UDFArgumentTypeException(0, \"Argument must be String, but \"\n                     + inputOI.getPrimitiveCategory().name()\n                     + \" was passed.\");\n        }\n\n        return new TotalNumOfLettersEvaluator();\n    }\n\n    public static class TotalNumOfLettersEvaluator extends GenericUDAFEvaluator{\n        PrimitiveObjectInspector inputOI;\n        ObjectInspector outputOI;\n        PrimitiveObjectInspector integerOI;\n\n        int total = 0;\n        private boolean warned = false;\n\n        @Override\n         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException{\n            assert (parameters.length == 1);\n            super.init(m, parameters);\n\n            //map阶段读取sql列，输入为String基础数据格式\n            if (m == Mode.PARTIAL1 || m == Mode.COMPLETE) {\n                inputOI = (PrimitiveObjectInspector) parameters[0];\n            } else {\n                //其余阶段，输入为Integer基础数据格式\n                integerOI = (PrimitiveObjectInspector) parameters[0];\n            }\n\n            // 指定各个阶段输出数据格式都为Integer类型\n            outputOI = ObjectInspectorFactory.getReflectionObjectInspector(Integer.class,\n                                        ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n\n            return outputOI;\n        }\n\n        //存储当前字符总数的类\n        static class LetterSumAgg implements AggregationBuffer {\n            int sum = 0;\n            void add(int num){\n                sum += num;\n            }\n        }\n\n         public AggregationBuffer getNewAggregationBuffer() throws HiveException {\n             LetterSumAgg result = new LetterSumAgg();\n             return result;\n         }\n\n         public void reset(AggregationBuffer aggregationBuffer) throws HiveException {\n             LetterSumAgg myagg = new LetterSumAgg();\n         }\n\n         public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException {\n             assert (parameters.length == 1);\n             if (parameters[0] != null) {\n                 LetterSumAgg myagg = (LetterSumAgg) agg;\n                 Object p1 = ((PrimitiveObjectInspector) inputOI).getPrimitiveJavaObject(parameters[0]);\n                 myagg.add(String.valueOf(p1).length());\n             }\n         }\n\n         public Object terminatePartial(AggregationBuffer agg) throws HiveException {\n             LetterSumAgg myagg = (LetterSumAgg) agg;\n             total += myagg.sum;\n             return total;\n         }\n\n         public void merge(AggregationBuffer agg, Object partial) throws HiveException {\n             if (partial != null) {\n                 LetterSumAgg myagg1 = (LetterSumAgg) agg;\n                 Integer partialSum = (Integer) integerOI.getPrimitiveJavaObject(partial);\n                 LetterSumAgg myagg2 = new LetterSumAgg();\n                 myagg2.add(partialSum);\n                 myagg1.add(myagg2.sum);\n             }\n         }\n\n         public Object terminate(AggregationBuffer agg) throws HiveException {\n             LetterSumAgg myagg = (LetterSumAgg) agg;\n             total = myagg.sum;\n             return myagg.sum;\n         }\n    }\n}\n\n```\n\n\n\n### 验证\n\n首先准备数据\n\n```\nhive> select * from users;\nOK\nzhangsan\t[\"beijing\",\"shanghai\",\"tianjin\",\"hangzhou\"]\nlisi\t[\"changchu\",\"chengdu\",\"wuhan\"]\n```\n\n\n\n然后添加jar包\n\n```\n> ADD JAR /home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar;\nAdded [/home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar] to class path\nAdded resources: [/home/will/work/projects/hive_udf_test/target/hive_udf_test-1.0-SNAPSHOT.jar]\n```\n\n\n\n定义函数\n\n```\nhive>  CREATE TEMPORARY FUNCTION letters as 'com.will.TotalNumOfLetttersGenericUDAF';\nOK\nTime taken: 0.049 seconds\n```\n\n\n\n执行\n\n```\nhive> select letters(name) from users;\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n2020-02-23 13:25:06,087 Stage-1 map = 0%,  reduce = 0%\n2020-02-23 13:25:11,426 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.03 sec\n2020-02-23 13:25:16,607 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.01 sec\nMapReduce Total cumulative CPU time: 4 seconds 10 msec\nTotal MapReduce CPU Time Spent: 4 seconds 10 msec\nOK\n12\nTime taken: 23.819 seconds, Fetched: 1 row(s)\n```\n\n\n\n# my own udaf\n\n### 要解决的问题\n\n有一个hive表，其中两列分别代表时间戳和事件。目标是得到指定时间范围的所有事件。\n\n\n\n### 分析\n\n根据上一部分的介绍，要实现聚合首先要设计如何存储，传输的问题。在这个过程中我仔细研究了hive udaf示例的histogram设计，然后设计了自己的udaf。聚合存储使用hashmap，初步解析结果使用string的list保存。\n\n\n\n代码\n\n[github 演示项目](https://github.com/zcenao21/hive-udaf)","slug":"hive/hive-udf","published":1,"updated":"2020-03-06T16:15:34.463Z","_id":"ck7cmmo8y0009clzkefw42vl3","comments":1,"layout":"post","photos":[],"link":"","content":"<p>hive为我们定义了很多函数，大多数情况下是能够满足我们的需求的。但是在有些情况下，很有必要自己定义一些函数，这样使用起来非常方便。这就是自定义函数(udf)和自定义聚合函数(udaf,user defined aggregation function)。udf是每行返回一个结果，而udaf则是聚合的结果，多行产生一个结果。另外还有udtf，是一行产生多个结果。目前我还没有用到过udtf，就先不介绍它了。下面分别用两个示例介绍udf和udaf<a id=\"more\"></a></p>\n<h1 id=\"udf\"><a href=\"#udf\" class=\"headerlink\" title=\"udf\"></a>udf</h1><h3 id=\"功能\"><a href=\"#功能\" class=\"headerlink\" title=\"功能\"></a>功能</h3><p>查找array中是否包含被查询值</p>\n<h3 id=\"步骤\"><a href=\"#步骤\" class=\"headerlink\" title=\"步骤\"></a>步骤</h3><ul>\n<li><p>测试数据准备</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zhangsan        beijing,shanghai,tianjin,hangzhou</span><br><span class=\"line\">lisi    changchu,chengdu,wuhan</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>hive建表与导入</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Create table users(name string, worklocations array&lt;string&gt; ) row format delimited fields terminated by &#39;\\t&#39; collection items terminated by &#39;,&#39;; </span><br><span class=\"line\"></span><br><span class=\"line\">load data local inpath &#39;&#x2F;root&#x2F;person.txt &#39; OVERWRITE INTO TABLE users;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>udf包生成与导入</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.will;</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class=\"line\">import java.util.ArrayList;</span><br><span class=\"line\"></span><br><span class=\"line\">public class FindInArray extends UDF &#123;</span><br><span class=\"line\">    public ArrayList&lt;String&gt; evaluate(String keywords, ArrayList&lt;String&gt; column)&#123;</span><br><span class=\"line\">        &#x2F;&#x2F;参数类型使用arraylist&lt;String&gt;对应hive中的array&lt;string&gt;,而不是String[]</span><br><span class=\"line\">        if(column.contains(keywords))&#123;</span><br><span class=\"line\">            return column;</span><br><span class=\"line\">        &#125;else&#123;</span><br><span class=\"line\">            return null;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    public String evaluate(String keywords,ArrayList&lt;String&gt; column,String name)&#123;</span><br><span class=\"line\">        &#x2F;&#x2F;重载evaluate，另一种查询方式，返回name值</span><br><span class=\"line\">        if(column.contains(keywords))&#123;</span><br><span class=\"line\">            return name;</span><br><span class=\"line\">        &#125;else&#123;</span><br><span class=\"line\">            return null;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>使用mvn 打包</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean package</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>导入hive</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add jar &#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar;</span><br><span class=\"line\">create temporary function find_in_array as &#39;com.will.FindInArray&#39;;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; select find_in_array(&#39;beijing&#39;,worklocations) from users;</span><br><span class=\"line\">OK</span><br><span class=\"line\">[&quot;beijing&quot;,&quot;shanghai&quot;,&quot;tianjin&quot;,&quot;hangzhou&quot;]</span><br><span class=\"line\">NULL</span><br><span class=\"line\">Time taken: 0.424 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<blockquote>\n<p>参考：<a href=\"https://blog.csdn.net/Nougats/article/details/71158318\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/Nougats/article/details/71158318</a></p>\n</blockquote>\n<h1 id=\"udaf\"><a href=\"#udaf\" class=\"headerlink\" title=\"udaf\"></a>udaf</h1><blockquote>\n<p>参考： </p>\n<p><a href=\"https://blog.51cto.com/xiaolanlan/2397771\" target=\"_blank\" rel=\"noopener\">https://blog.51cto.com/xiaolanlan/2397771</a></p>\n<p><a href=\"https://www.cnblogs.com/Rudd/p/5137612.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/Rudd/p/5137612.html</a></p>\n</blockquote>\n<h3 id=\"基础知识\"><a href=\"#基础知识\" class=\"headerlink\" title=\"基础知识\"></a>基础知识</h3><p>hive的udtf有两种，Simple和Generic。这是由于历史问题，Simple先出现，但是因为反射问题效率较低，所以现在推荐使用Generic的写法。</p>\n<ul>\n<li>Simple。即继承<code>org.apache.hadoop.hive.ql.exec.UDAF</code>类，并在派生类中以静态内部类的方式实现<code>org.apache.hadoop.hive.ql.exec.UDAFEvaluator</code>接口。在Hive源码包<code>org.apache.hadoop.hive.contrib.udaf.example</code>中包含几个示例。可以直接参阅。但是这些接口已经被注解为Deprecated，建议不要使用这种方式开发新的UDAF函数。</li>\n<li>Generic。这是Hive社区推荐的新的写法，以抽象类代替原有的接口。新的抽象类<code>org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver</code>替代老的UDAF接口，新的抽象类<code>org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator</code>替代老的UDAFEvaluator接口。</li>\n</ul>\n<p>hive是立足于hadoop之上，也就是hive基于mapreduce，hive sql最终还是会转化为mapreduce执行。为了实现mapreduce，udaf中用model来表示mapreduce各个阶段。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static enum Mode &#123;</span><br><span class=\"line\">        PARTIAL1,</span><br><span class=\"line\">        PARTIAL2,</span><br><span class=\"line\">        FINAL,</span><br><span class=\"line\">        COMPLETE;</span><br><span class=\"line\"></span><br><span class=\"line\">        private Mode() &#123;&#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合，将会调用<strong>iterate()</strong>和*<em>terminatePartial() *</em></li>\n<li>PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合，将会调用<strong>merge()</strong> 和 <strong>terminatePartial()</strong> </li>\n<li>FINAL: mapreduce的reduce阶段，从部分数据的聚合到完全聚合，将会调用<strong>merge()</strong>和*<em>terminate() *</em></li>\n</ul>\n<hr>\n<ul>\n<li><p>COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了，从原始数据直接到完全聚合，将会调用 <strong>iterate()</strong>和<strong>terminate()</strong></p>\n<p><img src=\"https://i.loli.net/2020/02/23/DlNqFRHnZ6zLuoA.png\" alt=\"image.png\"></p>\n</li>\n</ul>\n<p><img src=\"https://i.loli.net/2020/02/23/IKn3Vv57jPScxs8.png\" alt=\"image.png\"></p>\n<p>udaf骨架示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class GenericUDAFHistogramNumeric extends AbstractGenericUDAFResolver &#123;</span><br><span class=\"line\">  static final Log LOG &#x3D; LogFactory.getLog(GenericUDAFHistogramNumeric.class.getName());</span><br><span class=\"line\"> </span><br><span class=\"line\">  @Override</span><br><span class=\"line\">  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 这里主要做类型检查</span><br><span class=\"line\"> </span><br><span class=\"line\">    return new GenericUDAFHistogramNumericEvaluator();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">  public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; 确定各个阶段输入输出参数的数据格式ObjectInspectors</span><br><span class=\"line\">         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException&#123;</span><br><span class=\"line\">             return  null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; 保存数据聚集结果的类</span><br><span class=\"line\">         public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class=\"line\">             return null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t &#x2F;&#x2F; 重置聚集结果</span><br><span class=\"line\">         public void reset(AggregationBuffer aggregationBuffer) throws HiveException &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; map阶段，迭代处理输入sql传过来的列数据 </span><br><span class=\"line\">         public void iterate(AggregationBuffer aggregationBuffer, Object[] objects) throws HiveException &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; map与combiner结束返回结果，得到部分数据聚集结果</span><br><span class=\"line\">         public Object terminatePartial(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class=\"line\">             return null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; combiner合并map返回的结果，还有reducer合并mapper或combiner返回的结果。</span><br><span class=\"line\">         public void merge(AggregationBuffer aggregationBuffer, Object o) throws HiveException &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t &#x2F;&#x2F; reducer阶段，输出最终结果 </span><br><span class=\"line\">         public Object terminate(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class=\"line\">             return null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n<h3 id=\"功能-1\"><a href=\"#功能-1\" class=\"headerlink\" title=\"功能\"></a>功能</h3><p>统计字符数</p>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.will;</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.parse.SemanticException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;</span><br><span class=\"line\"></span><br><span class=\"line\">public class TotalNumOfLetttersGenericUDAF extends AbstractGenericUDAFResolver &#123;</span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        if (parameters.length !&#x3D; 1) &#123;</span><br><span class=\"line\">            throw new UDFArgumentTypeException(parameters.length - 1,&quot;Exactly one argument is expected.&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        ObjectInspector oi &#x3D; TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(parameters[0]);</span><br><span class=\"line\"></span><br><span class=\"line\">        if (oi.getCategory() !&#x3D; ObjectInspector.Category.PRIMITIVE)&#123;</span><br><span class=\"line\">            throw new UDFArgumentTypeException(0,</span><br><span class=\"line\">                    &quot;Argument must be PRIMITIVE, but &quot;</span><br><span class=\"line\">                    + oi.getCategory().name()</span><br><span class=\"line\">                    + &quot; was passed.&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        PrimitiveObjectInspector inputOI &#x3D; (PrimitiveObjectInspector) oi;</span><br><span class=\"line\">        if (inputOI.getPrimitiveCategory() !&#x3D; PrimitiveObjectInspector.PrimitiveCategory.STRING)&#123;</span><br><span class=\"line\">            throw new UDFArgumentTypeException(0, &quot;Argument must be String, but &quot;</span><br><span class=\"line\">                     + inputOI.getPrimitiveCategory().name()</span><br><span class=\"line\">                     + &quot; was passed.&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        return new TotalNumOfLettersEvaluator();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public static class TotalNumOfLettersEvaluator extends GenericUDAFEvaluator&#123;</span><br><span class=\"line\">        PrimitiveObjectInspector inputOI;</span><br><span class=\"line\">        ObjectInspector outputOI;</span><br><span class=\"line\">        PrimitiveObjectInspector integerOI;</span><br><span class=\"line\"></span><br><span class=\"line\">        int total &#x3D; 0;</span><br><span class=\"line\">        private boolean warned &#x3D; false;</span><br><span class=\"line\"></span><br><span class=\"line\">        @Override</span><br><span class=\"line\">         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException&#123;</span><br><span class=\"line\">            assert (parameters.length &#x3D;&#x3D; 1);</span><br><span class=\"line\">            super.init(m, parameters);</span><br><span class=\"line\"></span><br><span class=\"line\">            &#x2F;&#x2F;map阶段读取sql列，输入为String基础数据格式</span><br><span class=\"line\">            if (m &#x3D;&#x3D; Mode.PARTIAL1 || m &#x3D;&#x3D; Mode.COMPLETE) &#123;</span><br><span class=\"line\">                inputOI &#x3D; (PrimitiveObjectInspector) parameters[0];</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">                &#x2F;&#x2F;其余阶段，输入为Integer基础数据格式</span><br><span class=\"line\">                integerOI &#x3D; (PrimitiveObjectInspector) parameters[0];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            &#x2F;&#x2F; 指定各个阶段输出数据格式都为Integer类型</span><br><span class=\"line\">            outputOI &#x3D; ObjectInspectorFactory.getReflectionObjectInspector(Integer.class,</span><br><span class=\"line\">                                        ObjectInspectorFactory.ObjectInspectorOptions.JAVA);</span><br><span class=\"line\"></span><br><span class=\"line\">            return outputOI;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#x2F;&#x2F;存储当前字符总数的类</span><br><span class=\"line\">        static class LetterSumAgg implements AggregationBuffer &#123;</span><br><span class=\"line\">            int sum &#x3D; 0;</span><br><span class=\"line\">            void add(int num)&#123;</span><br><span class=\"line\">                sum +&#x3D; num;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg result &#x3D; new LetterSumAgg();</span><br><span class=\"line\">             return result;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public void reset(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg myagg &#x3D; new LetterSumAgg();</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException &#123;</span><br><span class=\"line\">             assert (parameters.length &#x3D;&#x3D; 1);</span><br><span class=\"line\">             if (parameters[0] !&#x3D; null) &#123;</span><br><span class=\"line\">                 LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">                 Object p1 &#x3D; ((PrimitiveObjectInspector) inputOI).getPrimitiveJavaObject(parameters[0]);</span><br><span class=\"line\">                 myagg.add(String.valueOf(p1).length());</span><br><span class=\"line\">             &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public Object terminatePartial(AggregationBuffer agg) throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">             total +&#x3D; myagg.sum;</span><br><span class=\"line\">             return total;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public void merge(AggregationBuffer agg, Object partial) throws HiveException &#123;</span><br><span class=\"line\">             if (partial !&#x3D; null) &#123;</span><br><span class=\"line\">                 LetterSumAgg myagg1 &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">                 Integer partialSum &#x3D; (Integer) integerOI.getPrimitiveJavaObject(partial);</span><br><span class=\"line\">                 LetterSumAgg myagg2 &#x3D; new LetterSumAgg();</span><br><span class=\"line\">                 myagg2.add(partialSum);</span><br><span class=\"line\">                 myagg1.add(myagg2.sum);</span><br><span class=\"line\">             &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public Object terminate(AggregationBuffer agg) throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">             total &#x3D; myagg.sum;</span><br><span class=\"line\">             return myagg.sum;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h3><p>首先准备数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; select * from users;</span><br><span class=\"line\">OK</span><br><span class=\"line\">zhangsan\t[&quot;beijing&quot;,&quot;shanghai&quot;,&quot;tianjin&quot;,&quot;hangzhou&quot;]</span><br><span class=\"line\">lisi\t[&quot;changchu&quot;,&quot;chengdu&quot;,&quot;wuhan&quot;]</span><br></pre></td></tr></table></figure>\n\n\n\n<p>然后添加jar包</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; ADD JAR &#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar;</span><br><span class=\"line\">Added [&#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar] to class path</span><br><span class=\"line\">Added resources: [&#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar]</span><br></pre></td></tr></table></figure>\n\n\n\n<p>定义函数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt;  CREATE TEMPORARY FUNCTION letters as &#39;com.will.TotalNumOfLetttersGenericUDAF&#39;;</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.049 seconds</span><br></pre></td></tr></table></figure>\n\n\n\n<p>执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; select letters(name) from users;</span><br><span class=\"line\">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class=\"line\">2020-02-23 13:25:06,087 Stage-1 map &#x3D; 0%,  reduce &#x3D; 0%</span><br><span class=\"line\">2020-02-23 13:25:11,426 Stage-1 map &#x3D; 100%,  reduce &#x3D; 0%, Cumulative CPU 2.03 sec</span><br><span class=\"line\">2020-02-23 13:25:16,607 Stage-1 map &#x3D; 100%,  reduce &#x3D; 100%, Cumulative CPU 4.01 sec</span><br><span class=\"line\">MapReduce Total cumulative CPU time: 4 seconds 10 msec</span><br><span class=\"line\">Total MapReduce CPU Time Spent: 4 seconds 10 msec</span><br><span class=\"line\">OK</span><br><span class=\"line\">12</span><br><span class=\"line\">Time taken: 23.819 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"my-own-udaf\"><a href=\"#my-own-udaf\" class=\"headerlink\" title=\"my own udaf\"></a>my own udaf</h1><h3 id=\"要解决的问题\"><a href=\"#要解决的问题\" class=\"headerlink\" title=\"要解决的问题\"></a>要解决的问题</h3><p>有一个hive表，其中两列分别代表时间戳和事件。目标是得到指定时间范围的所有事件。</p>\n<h3 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h3><p>根据上一部分的介绍，要实现聚合首先要设计如何存储，传输的问题。在这个过程中我仔细研究了hive udaf示例的histogram设计，然后设计了自己的udaf。聚合存储使用hashmap，初步解析结果使用string的list保存。</p>\n<p>代码</p>\n<p><a href=\"https://github.com/zcenao21/hive-udaf\" target=\"_blank\" rel=\"noopener\">github 演示项目</a></p>\n","site":{"data":{}},"excerpt":"<p>hive为我们定义了很多函数，大多数情况下是能够满足我们的需求的。但是在有些情况下，很有必要自己定义一些函数，这样使用起来非常方便。这就是自定义函数(udf)和自定义聚合函数(udaf,user defined aggregation function)。udf是每行返回一个结果，而udaf则是聚合的结果，多行产生一个结果。另外还有udtf，是一行产生多个结果。目前我还没有用到过udtf，就先不介绍它了。下面分别用两个示例介绍udf和udaf","more":"</p>\n<h1 id=\"udf\"><a href=\"#udf\" class=\"headerlink\" title=\"udf\"></a>udf</h1><h3 id=\"功能\"><a href=\"#功能\" class=\"headerlink\" title=\"功能\"></a>功能</h3><p>查找array中是否包含被查询值</p>\n<h3 id=\"步骤\"><a href=\"#步骤\" class=\"headerlink\" title=\"步骤\"></a>步骤</h3><ul>\n<li><p>测试数据准备</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">zhangsan        beijing,shanghai,tianjin,hangzhou</span><br><span class=\"line\">lisi    changchu,chengdu,wuhan</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>hive建表与导入</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Create table users(name string, worklocations array&lt;string&gt; ) row format delimited fields terminated by &#39;\\t&#39; collection items terminated by &#39;,&#39;; </span><br><span class=\"line\"></span><br><span class=\"line\">load data local inpath &#39;&#x2F;root&#x2F;person.txt &#39; OVERWRITE INTO TABLE users;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>udf包生成与导入</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.will;</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class=\"line\">import java.util.ArrayList;</span><br><span class=\"line\"></span><br><span class=\"line\">public class FindInArray extends UDF &#123;</span><br><span class=\"line\">    public ArrayList&lt;String&gt; evaluate(String keywords, ArrayList&lt;String&gt; column)&#123;</span><br><span class=\"line\">        &#x2F;&#x2F;参数类型使用arraylist&lt;String&gt;对应hive中的array&lt;string&gt;,而不是String[]</span><br><span class=\"line\">        if(column.contains(keywords))&#123;</span><br><span class=\"line\">            return column;</span><br><span class=\"line\">        &#125;else&#123;</span><br><span class=\"line\">            return null;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    </span><br><span class=\"line\">    public String evaluate(String keywords,ArrayList&lt;String&gt; column,String name)&#123;</span><br><span class=\"line\">        &#x2F;&#x2F;重载evaluate，另一种查询方式，返回name值</span><br><span class=\"line\">        if(column.contains(keywords))&#123;</span><br><span class=\"line\">            return name;</span><br><span class=\"line\">        &#125;else&#123;</span><br><span class=\"line\">            return null;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>使用mvn 打包</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn clean package</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>导入hive</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add jar &#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar;</span><br><span class=\"line\">create temporary function find_in_array as &#39;com.will.FindInArray&#39;;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>使用</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; select find_in_array(&#39;beijing&#39;,worklocations) from users;</span><br><span class=\"line\">OK</span><br><span class=\"line\">[&quot;beijing&quot;,&quot;shanghai&quot;,&quot;tianjin&quot;,&quot;hangzhou&quot;]</span><br><span class=\"line\">NULL</span><br><span class=\"line\">Time taken: 0.424 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<blockquote>\n<p>参考：<a href=\"https://blog.csdn.net/Nougats/article/details/71158318\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/Nougats/article/details/71158318</a></p>\n</blockquote>\n<h1 id=\"udaf\"><a href=\"#udaf\" class=\"headerlink\" title=\"udaf\"></a>udaf</h1><blockquote>\n<p>参考： </p>\n<p><a href=\"https://blog.51cto.com/xiaolanlan/2397771\" target=\"_blank\" rel=\"noopener\">https://blog.51cto.com/xiaolanlan/2397771</a></p>\n<p><a href=\"https://www.cnblogs.com/Rudd/p/5137612.html\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/Rudd/p/5137612.html</a></p>\n</blockquote>\n<h3 id=\"基础知识\"><a href=\"#基础知识\" class=\"headerlink\" title=\"基础知识\"></a>基础知识</h3><p>hive的udtf有两种，Simple和Generic。这是由于历史问题，Simple先出现，但是因为反射问题效率较低，所以现在推荐使用Generic的写法。</p>\n<ul>\n<li>Simple。即继承<code>org.apache.hadoop.hive.ql.exec.UDAF</code>类，并在派生类中以静态内部类的方式实现<code>org.apache.hadoop.hive.ql.exec.UDAFEvaluator</code>接口。在Hive源码包<code>org.apache.hadoop.hive.contrib.udaf.example</code>中包含几个示例。可以直接参阅。但是这些接口已经被注解为Deprecated，建议不要使用这种方式开发新的UDAF函数。</li>\n<li>Generic。这是Hive社区推荐的新的写法，以抽象类代替原有的接口。新的抽象类<code>org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver</code>替代老的UDAF接口，新的抽象类<code>org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator</code>替代老的UDAFEvaluator接口。</li>\n</ul>\n<p>hive是立足于hadoop之上，也就是hive基于mapreduce，hive sql最终还是会转化为mapreduce执行。为了实现mapreduce，udaf中用model来表示mapreduce各个阶段。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public static enum Mode &#123;</span><br><span class=\"line\">        PARTIAL1,</span><br><span class=\"line\">        PARTIAL2,</span><br><span class=\"line\">        FINAL,</span><br><span class=\"line\">        COMPLETE;</span><br><span class=\"line\"></span><br><span class=\"line\">        private Mode() &#123;&#125;</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合，将会调用<strong>iterate()</strong>和*<em>terminatePartial() *</em></li>\n<li>PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合，将会调用<strong>merge()</strong> 和 <strong>terminatePartial()</strong> </li>\n<li>FINAL: mapreduce的reduce阶段，从部分数据的聚合到完全聚合，将会调用<strong>merge()</strong>和*<em>terminate() *</em></li>\n</ul>\n<hr>\n<ul>\n<li><p>COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了，从原始数据直接到完全聚合，将会调用 <strong>iterate()</strong>和<strong>terminate()</strong></p>\n<p><img src=\"https://i.loli.net/2020/02/23/DlNqFRHnZ6zLuoA.png\" alt=\"image.png\"></p>\n</li>\n</ul>\n<p><img src=\"https://i.loli.net/2020/02/23/IKn3Vv57jPScxs8.png\" alt=\"image.png\"></p>\n<p>udaf骨架示例：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class GenericUDAFHistogramNumeric extends AbstractGenericUDAFResolver &#123;</span><br><span class=\"line\">  static final Log LOG &#x3D; LogFactory.getLog(GenericUDAFHistogramNumeric.class.getName());</span><br><span class=\"line\"> </span><br><span class=\"line\">  @Override</span><br><span class=\"line\">  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException &#123;</span><br><span class=\"line\">    &#x2F;&#x2F; 这里主要做类型检查</span><br><span class=\"line\"> </span><br><span class=\"line\">    return new GenericUDAFHistogramNumericEvaluator();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"> </span><br><span class=\"line\">  public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator &#123;</span><br><span class=\"line\">         &#x2F;&#x2F; 确定各个阶段输入输出参数的数据格式ObjectInspectors</span><br><span class=\"line\">         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException&#123;</span><br><span class=\"line\">             return  null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; 保存数据聚集结果的类</span><br><span class=\"line\">         public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class=\"line\">             return null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t &#x2F;&#x2F; 重置聚集结果</span><br><span class=\"line\">         public void reset(AggregationBuffer aggregationBuffer) throws HiveException &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; map阶段，迭代处理输入sql传过来的列数据 </span><br><span class=\"line\">         public void iterate(AggregationBuffer aggregationBuffer, Object[] objects) throws HiveException &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; map与combiner结束返回结果，得到部分数据聚集结果</span><br><span class=\"line\">         public Object terminatePartial(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class=\"line\">             return null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         &#x2F;&#x2F; combiner合并map返回的结果，还有reducer合并mapper或combiner返回的结果。</span><br><span class=\"line\">         public void merge(AggregationBuffer aggregationBuffer, Object o) throws HiveException &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">\t\t &#x2F;&#x2F; reducer阶段，输出最终结果 </span><br><span class=\"line\">         public Object terminate(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class=\"line\">             return null;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n<h3 id=\"功能-1\"><a href=\"#功能-1\" class=\"headerlink\" title=\"功能\"></a>功能</h3><p>统计字符数</p>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.will;</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.parse.SemanticException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;</span><br><span class=\"line\"></span><br><span class=\"line\">public class TotalNumOfLetttersGenericUDAF extends AbstractGenericUDAFResolver &#123;</span><br><span class=\"line\">    @Override</span><br><span class=\"line\">    public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        if (parameters.length !&#x3D; 1) &#123;</span><br><span class=\"line\">            throw new UDFArgumentTypeException(parameters.length - 1,&quot;Exactly one argument is expected.&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        ObjectInspector oi &#x3D; TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(parameters[0]);</span><br><span class=\"line\"></span><br><span class=\"line\">        if (oi.getCategory() !&#x3D; ObjectInspector.Category.PRIMITIVE)&#123;</span><br><span class=\"line\">            throw new UDFArgumentTypeException(0,</span><br><span class=\"line\">                    &quot;Argument must be PRIMITIVE, but &quot;</span><br><span class=\"line\">                    + oi.getCategory().name()</span><br><span class=\"line\">                    + &quot; was passed.&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        PrimitiveObjectInspector inputOI &#x3D; (PrimitiveObjectInspector) oi;</span><br><span class=\"line\">        if (inputOI.getPrimitiveCategory() !&#x3D; PrimitiveObjectInspector.PrimitiveCategory.STRING)&#123;</span><br><span class=\"line\">            throw new UDFArgumentTypeException(0, &quot;Argument must be String, but &quot;</span><br><span class=\"line\">                     + inputOI.getPrimitiveCategory().name()</span><br><span class=\"line\">                     + &quot; was passed.&quot;);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        return new TotalNumOfLettersEvaluator();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public static class TotalNumOfLettersEvaluator extends GenericUDAFEvaluator&#123;</span><br><span class=\"line\">        PrimitiveObjectInspector inputOI;</span><br><span class=\"line\">        ObjectInspector outputOI;</span><br><span class=\"line\">        PrimitiveObjectInspector integerOI;</span><br><span class=\"line\"></span><br><span class=\"line\">        int total &#x3D; 0;</span><br><span class=\"line\">        private boolean warned &#x3D; false;</span><br><span class=\"line\"></span><br><span class=\"line\">        @Override</span><br><span class=\"line\">         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException&#123;</span><br><span class=\"line\">            assert (parameters.length &#x3D;&#x3D; 1);</span><br><span class=\"line\">            super.init(m, parameters);</span><br><span class=\"line\"></span><br><span class=\"line\">            &#x2F;&#x2F;map阶段读取sql列，输入为String基础数据格式</span><br><span class=\"line\">            if (m &#x3D;&#x3D; Mode.PARTIAL1 || m &#x3D;&#x3D; Mode.COMPLETE) &#123;</span><br><span class=\"line\">                inputOI &#x3D; (PrimitiveObjectInspector) parameters[0];</span><br><span class=\"line\">            &#125; else &#123;</span><br><span class=\"line\">                &#x2F;&#x2F;其余阶段，输入为Integer基础数据格式</span><br><span class=\"line\">                integerOI &#x3D; (PrimitiveObjectInspector) parameters[0];</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">            &#x2F;&#x2F; 指定各个阶段输出数据格式都为Integer类型</span><br><span class=\"line\">            outputOI &#x3D; ObjectInspectorFactory.getReflectionObjectInspector(Integer.class,</span><br><span class=\"line\">                                        ObjectInspectorFactory.ObjectInspectorOptions.JAVA);</span><br><span class=\"line\"></span><br><span class=\"line\">            return outputOI;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        &#x2F;&#x2F;存储当前字符总数的类</span><br><span class=\"line\">        static class LetterSumAgg implements AggregationBuffer &#123;</span><br><span class=\"line\">            int sum &#x3D; 0;</span><br><span class=\"line\">            void add(int num)&#123;</span><br><span class=\"line\">                sum +&#x3D; num;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg result &#x3D; new LetterSumAgg();</span><br><span class=\"line\">             return result;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public void reset(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg myagg &#x3D; new LetterSumAgg();</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException &#123;</span><br><span class=\"line\">             assert (parameters.length &#x3D;&#x3D; 1);</span><br><span class=\"line\">             if (parameters[0] !&#x3D; null) &#123;</span><br><span class=\"line\">                 LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">                 Object p1 &#x3D; ((PrimitiveObjectInspector) inputOI).getPrimitiveJavaObject(parameters[0]);</span><br><span class=\"line\">                 myagg.add(String.valueOf(p1).length());</span><br><span class=\"line\">             &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public Object terminatePartial(AggregationBuffer agg) throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">             total +&#x3D; myagg.sum;</span><br><span class=\"line\">             return total;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public void merge(AggregationBuffer agg, Object partial) throws HiveException &#123;</span><br><span class=\"line\">             if (partial !&#x3D; null) &#123;</span><br><span class=\"line\">                 LetterSumAgg myagg1 &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">                 Integer partialSum &#x3D; (Integer) integerOI.getPrimitiveJavaObject(partial);</span><br><span class=\"line\">                 LetterSumAgg myagg2 &#x3D; new LetterSumAgg();</span><br><span class=\"line\">                 myagg2.add(partialSum);</span><br><span class=\"line\">                 myagg1.add(myagg2.sum);</span><br><span class=\"line\">             &#125;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">         public Object terminate(AggregationBuffer agg) throws HiveException &#123;</span><br><span class=\"line\">             LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class=\"line\">             total &#x3D; myagg.sum;</span><br><span class=\"line\">             return myagg.sum;</span><br><span class=\"line\">         &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h3><p>首先准备数据</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; select * from users;</span><br><span class=\"line\">OK</span><br><span class=\"line\">zhangsan\t[&quot;beijing&quot;,&quot;shanghai&quot;,&quot;tianjin&quot;,&quot;hangzhou&quot;]</span><br><span class=\"line\">lisi\t[&quot;changchu&quot;,&quot;chengdu&quot;,&quot;wuhan&quot;]</span><br></pre></td></tr></table></figure>\n\n\n\n<p>然后添加jar包</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&gt; ADD JAR &#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar;</span><br><span class=\"line\">Added [&#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar] to class path</span><br><span class=\"line\">Added resources: [&#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar]</span><br></pre></td></tr></table></figure>\n\n\n\n<p>定义函数</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt;  CREATE TEMPORARY FUNCTION letters as &#39;com.will.TotalNumOfLetttersGenericUDAF&#39;;</span><br><span class=\"line\">OK</span><br><span class=\"line\">Time taken: 0.049 seconds</span><br></pre></td></tr></table></figure>\n\n\n\n<p>执行</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hive&gt; select letters(name) from users;</span><br><span class=\"line\">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class=\"line\">2020-02-23 13:25:06,087 Stage-1 map &#x3D; 0%,  reduce &#x3D; 0%</span><br><span class=\"line\">2020-02-23 13:25:11,426 Stage-1 map &#x3D; 100%,  reduce &#x3D; 0%, Cumulative CPU 2.03 sec</span><br><span class=\"line\">2020-02-23 13:25:16,607 Stage-1 map &#x3D; 100%,  reduce &#x3D; 100%, Cumulative CPU 4.01 sec</span><br><span class=\"line\">MapReduce Total cumulative CPU time: 4 seconds 10 msec</span><br><span class=\"line\">Total MapReduce CPU Time Spent: 4 seconds 10 msec</span><br><span class=\"line\">OK</span><br><span class=\"line\">12</span><br><span class=\"line\">Time taken: 23.819 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>\n\n\n\n<h1 id=\"my-own-udaf\"><a href=\"#my-own-udaf\" class=\"headerlink\" title=\"my own udaf\"></a>my own udaf</h1><h3 id=\"要解决的问题\"><a href=\"#要解决的问题\" class=\"headerlink\" title=\"要解决的问题\"></a>要解决的问题</h3><p>有一个hive表，其中两列分别代表时间戳和事件。目标是得到指定时间范围的所有事件。</p>\n<h3 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h3><p>根据上一部分的介绍，要实现聚合首先要设计如何存储，传输的问题。在这个过程中我仔细研究了hive udaf示例的histogram设计，然后设计了自己的udaf。聚合存储使用hashmap，初步解析结果使用string的list保存。</p>\n<p>代码</p>\n<p><a href=\"https://github.com/zcenao21/hive-udaf\" target=\"_blank\" rel=\"noopener\">github 演示项目</a></p>"},{"title":"git rebase合并commits","date":"2020-02-27T14:31:21.000Z","_content":"\n# rebase作用\n\nrebase主要和merge对比。相对于merge，rebase可以合并编辑commits历史，从而让更改逻辑一目了然。<!--more-->\n\n\n# 使用示例\n\n### 首先新建项目并从github上拉下来\n\n```\n % git clone git@github.com:xxx/rebaseTest.git\n正克隆到 'rebaseTest'...\nremote: Enumerating objects: 3, done.\nremote: Counting objects: 100% (3/3), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\n接收对象中: 100% (3/3), 完成.\n检查连接... 完成。\n```\n\n\n\n###  新建分支并切换\n\n```\n % git checkout -b test\n切换到一个新分支 'test'\n```\n\n\n\n### 修改并提交多次\n\n修改一次文件后执行下面的命令一次，message替换为自定义信息\n\n```\ngit add README.md\ngit commit -m \"message\"\n```\n\n\n\n### 找到要合并的commits\n\n```\n % git log --oneline\n4254b18 11111\nd024037 00000\nf7de784 all messages\n```\n\n假设要将最上面两条合并\n\n```\ngit rebase -i f7de784\n```\n\n将pick替换为e和s，分别表示编辑新的信息，合并之前的\n\n```\ne d024037 00000\ns 4254b18 11111\n\n# Rebase f7de784..4254b18 onto f7de784 (2 command(s))\n#\n# Commands:\n# p, pick = use commit\n# r, reword = use commit, but edit the commit message\n# e, edit = use commit, but stop for amending\n# s, squash = use commit, but meld into previous commit\n# f, fixup = like \"squash\", but discard this commit's log message\n\n^G 求助       ^O Write Out  ^W 搜索       ^K 剪切文字   ^J 对齐\n^X 离开       ^R 读档       ^\\ 替换       ^U Uncut Text ^T 拼写检查\n```\n\n\n\n### 合并commit信息\n\n```\n# This is a combination of 2 commits.\n\n2 merged messages\n\n# 请为您的变更输入提交说明。以 '#' 开始的行将被忽略，而一个空的提交\n# 说明将会终止提交。\n# \n# 日期：  Thu Feb 27 22:19:08 2020 +0800\n# \n# 交互式变基操作正在进行中；至 f7de784\n# 最后一条命令已完成（2 条命令被执行）：\n#    e d024037 00000\n#    s 4254b18 11111\n# 未剩下任何命令。\n# 您在执行将分支 'test' 变基到 'f7de784' 的操作时编辑提交。\n# \n# 要提交的变更：\n#       修改：     README.md\n```\n\n\n\n### 切换到master分支并合并\n\n```\n% git checkout master\n切换到分支 'master'\n您的分支与上游分支 'origin/master' 一致。\nwill@will-Lenovo-ideapad-720S-14IKB /tmp/test/rebaseTest\n % git merge test\n更新 f7de784..687e32e\nFast-forward\n README.md | 25 ++-----------------------\n 1 file changed, 2 insertions(+), 23 deletions(-)\n```\n\n\n\n到github上可以看到master分支只有一条合并后的commit信息\n\n![rebase.png](https://i.loli.net/2020/02/27/zFCWBo4NQarsmgL.png)\n\n","source":"_posts/other/rebase合并commit.md","raw":"---\ntitle: git rebase合并commits\ndate: 2020-02-27 22:31:21\ntags:\n    - Git\n    - rebase\n    - commits合并\ncategories: \n    - other\n---\n\n# rebase作用\n\nrebase主要和merge对比。相对于merge，rebase可以合并编辑commits历史，从而让更改逻辑一目了然。<!--more-->\n\n\n# 使用示例\n\n### 首先新建项目并从github上拉下来\n\n```\n % git clone git@github.com:xxx/rebaseTest.git\n正克隆到 'rebaseTest'...\nremote: Enumerating objects: 3, done.\nremote: Counting objects: 100% (3/3), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\n接收对象中: 100% (3/3), 完成.\n检查连接... 完成。\n```\n\n\n\n###  新建分支并切换\n\n```\n % git checkout -b test\n切换到一个新分支 'test'\n```\n\n\n\n### 修改并提交多次\n\n修改一次文件后执行下面的命令一次，message替换为自定义信息\n\n```\ngit add README.md\ngit commit -m \"message\"\n```\n\n\n\n### 找到要合并的commits\n\n```\n % git log --oneline\n4254b18 11111\nd024037 00000\nf7de784 all messages\n```\n\n假设要将最上面两条合并\n\n```\ngit rebase -i f7de784\n```\n\n将pick替换为e和s，分别表示编辑新的信息，合并之前的\n\n```\ne d024037 00000\ns 4254b18 11111\n\n# Rebase f7de784..4254b18 onto f7de784 (2 command(s))\n#\n# Commands:\n# p, pick = use commit\n# r, reword = use commit, but edit the commit message\n# e, edit = use commit, but stop for amending\n# s, squash = use commit, but meld into previous commit\n# f, fixup = like \"squash\", but discard this commit's log message\n\n^G 求助       ^O Write Out  ^W 搜索       ^K 剪切文字   ^J 对齐\n^X 离开       ^R 读档       ^\\ 替换       ^U Uncut Text ^T 拼写检查\n```\n\n\n\n### 合并commit信息\n\n```\n# This is a combination of 2 commits.\n\n2 merged messages\n\n# 请为您的变更输入提交说明。以 '#' 开始的行将被忽略，而一个空的提交\n# 说明将会终止提交。\n# \n# 日期：  Thu Feb 27 22:19:08 2020 +0800\n# \n# 交互式变基操作正在进行中；至 f7de784\n# 最后一条命令已完成（2 条命令被执行）：\n#    e d024037 00000\n#    s 4254b18 11111\n# 未剩下任何命令。\n# 您在执行将分支 'test' 变基到 'f7de784' 的操作时编辑提交。\n# \n# 要提交的变更：\n#       修改：     README.md\n```\n\n\n\n### 切换到master分支并合并\n\n```\n% git checkout master\n切换到分支 'master'\n您的分支与上游分支 'origin/master' 一致。\nwill@will-Lenovo-ideapad-720S-14IKB /tmp/test/rebaseTest\n % git merge test\n更新 f7de784..687e32e\nFast-forward\n README.md | 25 ++-----------------------\n 1 file changed, 2 insertions(+), 23 deletions(-)\n```\n\n\n\n到github上可以看到master分支只有一条合并后的commit信息\n\n![rebase.png](https://i.loli.net/2020/02/27/zFCWBo4NQarsmgL.png)\n\n","slug":"other/rebase合并commit","published":1,"updated":"2020-02-27T14:45:21.906Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck7cmmo8z000cclzk7dgcafdk","content":"<h1 id=\"rebase作用\"><a href=\"#rebase作用\" class=\"headerlink\" title=\"rebase作用\"></a>rebase作用</h1><p>rebase主要和merge对比。相对于merge，rebase可以合并编辑commits历史，从而让更改逻辑一目了然。<a id=\"more\"></a></p>\n<h1 id=\"使用示例\"><a href=\"#使用示例\" class=\"headerlink\" title=\"使用示例\"></a>使用示例</h1><h3 id=\"首先新建项目并从github上拉下来\"><a href=\"#首先新建项目并从github上拉下来\" class=\"headerlink\" title=\"首先新建项目并从github上拉下来\"></a>首先新建项目并从github上拉下来</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> % git clone git@github.com:xxx&#x2F;rebaseTest.git</span><br><span class=\"line\">正克隆到 &#39;rebaseTest&#39;...</span><br><span class=\"line\">remote: Enumerating objects: 3, done.</span><br><span class=\"line\">remote: Counting objects: 100% (3&#x2F;3), done.</span><br><span class=\"line\">remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0</span><br><span class=\"line\">接收对象中: 100% (3&#x2F;3), 完成.</span><br><span class=\"line\">检查连接... 完成。</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"新建分支并切换\"><a href=\"#新建分支并切换\" class=\"headerlink\" title=\"新建分支并切换\"></a>新建分支并切换</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> % git checkout -b test</span><br><span class=\"line\">切换到一个新分支 &#39;test&#39;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"修改并提交多次\"><a href=\"#修改并提交多次\" class=\"headerlink\" title=\"修改并提交多次\"></a>修改并提交多次</h3><p>修改一次文件后执行下面的命令一次，message替换为自定义信息</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add README.md</span><br><span class=\"line\">git commit -m &quot;message&quot;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"找到要合并的commits\"><a href=\"#找到要合并的commits\" class=\"headerlink\" title=\"找到要合并的commits\"></a>找到要合并的commits</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> % git log --oneline</span><br><span class=\"line\">4254b18 11111</span><br><span class=\"line\">d024037 00000</span><br><span class=\"line\">f7de784 all messages</span><br></pre></td></tr></table></figure>\n\n<p>假设要将最上面两条合并</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rebase -i f7de784</span><br></pre></td></tr></table></figure>\n\n<p>将pick替换为e和s，分别表示编辑新的信息，合并之前的</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">e d024037 00000</span><br><span class=\"line\">s 4254b18 11111</span><br><span class=\"line\"></span><br><span class=\"line\"># Rebase f7de784..4254b18 onto f7de784 (2 command(s))</span><br><span class=\"line\">#</span><br><span class=\"line\"># Commands:</span><br><span class=\"line\"># p, pick &#x3D; use commit</span><br><span class=\"line\"># r, reword &#x3D; use commit, but edit the commit message</span><br><span class=\"line\"># e, edit &#x3D; use commit, but stop for amending</span><br><span class=\"line\"># s, squash &#x3D; use commit, but meld into previous commit</span><br><span class=\"line\"># f, fixup &#x3D; like &quot;squash&quot;, but discard this commit&#39;s log message</span><br><span class=\"line\"></span><br><span class=\"line\">^G 求助       ^O Write Out  ^W 搜索       ^K 剪切文字   ^J 对齐</span><br><span class=\"line\">^X 离开       ^R 读档       ^\\ 替换       ^U Uncut Text ^T 拼写检查</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"合并commit信息\"><a href=\"#合并commit信息\" class=\"headerlink\" title=\"合并commit信息\"></a>合并commit信息</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># This is a combination of 2 commits.</span><br><span class=\"line\"></span><br><span class=\"line\">2 merged messages</span><br><span class=\"line\"></span><br><span class=\"line\"># 请为您的变更输入提交说明。以 &#39;#&#39; 开始的行将被忽略，而一个空的提交</span><br><span class=\"line\"># 说明将会终止提交。</span><br><span class=\"line\"># </span><br><span class=\"line\"># 日期：  Thu Feb 27 22:19:08 2020 +0800</span><br><span class=\"line\"># </span><br><span class=\"line\"># 交互式变基操作正在进行中；至 f7de784</span><br><span class=\"line\"># 最后一条命令已完成（2 条命令被执行）：</span><br><span class=\"line\">#    e d024037 00000</span><br><span class=\"line\">#    s 4254b18 11111</span><br><span class=\"line\"># 未剩下任何命令。</span><br><span class=\"line\"># 您在执行将分支 &#39;test&#39; 变基到 &#39;f7de784&#39; 的操作时编辑提交。</span><br><span class=\"line\"># </span><br><span class=\"line\"># 要提交的变更：</span><br><span class=\"line\">#       修改：     README.md</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"切换到master分支并合并\"><a href=\"#切换到master分支并合并\" class=\"headerlink\" title=\"切换到master分支并合并\"></a>切换到master分支并合并</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">% git checkout master</span><br><span class=\"line\">切换到分支 &#39;master&#39;</span><br><span class=\"line\">您的分支与上游分支 &#39;origin&#x2F;master&#39; 一致。</span><br><span class=\"line\">will@will-Lenovo-ideapad-720S-14IKB &#x2F;tmp&#x2F;test&#x2F;rebaseTest</span><br><span class=\"line\"> % git merge test</span><br><span class=\"line\">更新 f7de784..687e32e</span><br><span class=\"line\">Fast-forward</span><br><span class=\"line\"> README.md | 25 ++-----------------------</span><br><span class=\"line\"> 1 file changed, 2 insertions(+), 23 deletions(-)</span><br></pre></td></tr></table></figure>\n\n\n\n<p>到github上可以看到master分支只有一条合并后的commit信息</p>\n<p><img src=\"https://i.loli.net/2020/02/27/zFCWBo4NQarsmgL.png\" alt=\"rebase.png\"></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"rebase作用\"><a href=\"#rebase作用\" class=\"headerlink\" title=\"rebase作用\"></a>rebase作用</h1><p>rebase主要和merge对比。相对于merge，rebase可以合并编辑commits历史，从而让更改逻辑一目了然。","more":"</p>\n<h1 id=\"使用示例\"><a href=\"#使用示例\" class=\"headerlink\" title=\"使用示例\"></a>使用示例</h1><h3 id=\"首先新建项目并从github上拉下来\"><a href=\"#首先新建项目并从github上拉下来\" class=\"headerlink\" title=\"首先新建项目并从github上拉下来\"></a>首先新建项目并从github上拉下来</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> % git clone git@github.com:xxx&#x2F;rebaseTest.git</span><br><span class=\"line\">正克隆到 &#39;rebaseTest&#39;...</span><br><span class=\"line\">remote: Enumerating objects: 3, done.</span><br><span class=\"line\">remote: Counting objects: 100% (3&#x2F;3), done.</span><br><span class=\"line\">remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0</span><br><span class=\"line\">接收对象中: 100% (3&#x2F;3), 完成.</span><br><span class=\"line\">检查连接... 完成。</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"新建分支并切换\"><a href=\"#新建分支并切换\" class=\"headerlink\" title=\"新建分支并切换\"></a>新建分支并切换</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> % git checkout -b test</span><br><span class=\"line\">切换到一个新分支 &#39;test&#39;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"修改并提交多次\"><a href=\"#修改并提交多次\" class=\"headerlink\" title=\"修改并提交多次\"></a>修改并提交多次</h3><p>修改一次文件后执行下面的命令一次，message替换为自定义信息</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add README.md</span><br><span class=\"line\">git commit -m &quot;message&quot;</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"找到要合并的commits\"><a href=\"#找到要合并的commits\" class=\"headerlink\" title=\"找到要合并的commits\"></a>找到要合并的commits</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> % git log --oneline</span><br><span class=\"line\">4254b18 11111</span><br><span class=\"line\">d024037 00000</span><br><span class=\"line\">f7de784 all messages</span><br></pre></td></tr></table></figure>\n\n<p>假设要将最上面两条合并</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rebase -i f7de784</span><br></pre></td></tr></table></figure>\n\n<p>将pick替换为e和s，分别表示编辑新的信息，合并之前的</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">e d024037 00000</span><br><span class=\"line\">s 4254b18 11111</span><br><span class=\"line\"></span><br><span class=\"line\"># Rebase f7de784..4254b18 onto f7de784 (2 command(s))</span><br><span class=\"line\">#</span><br><span class=\"line\"># Commands:</span><br><span class=\"line\"># p, pick &#x3D; use commit</span><br><span class=\"line\"># r, reword &#x3D; use commit, but edit the commit message</span><br><span class=\"line\"># e, edit &#x3D; use commit, but stop for amending</span><br><span class=\"line\"># s, squash &#x3D; use commit, but meld into previous commit</span><br><span class=\"line\"># f, fixup &#x3D; like &quot;squash&quot;, but discard this commit&#39;s log message</span><br><span class=\"line\"></span><br><span class=\"line\">^G 求助       ^O Write Out  ^W 搜索       ^K 剪切文字   ^J 对齐</span><br><span class=\"line\">^X 离开       ^R 读档       ^\\ 替换       ^U Uncut Text ^T 拼写检查</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"合并commit信息\"><a href=\"#合并commit信息\" class=\"headerlink\" title=\"合并commit信息\"></a>合并commit信息</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># This is a combination of 2 commits.</span><br><span class=\"line\"></span><br><span class=\"line\">2 merged messages</span><br><span class=\"line\"></span><br><span class=\"line\"># 请为您的变更输入提交说明。以 &#39;#&#39; 开始的行将被忽略，而一个空的提交</span><br><span class=\"line\"># 说明将会终止提交。</span><br><span class=\"line\"># </span><br><span class=\"line\"># 日期：  Thu Feb 27 22:19:08 2020 +0800</span><br><span class=\"line\"># </span><br><span class=\"line\"># 交互式变基操作正在进行中；至 f7de784</span><br><span class=\"line\"># 最后一条命令已完成（2 条命令被执行）：</span><br><span class=\"line\">#    e d024037 00000</span><br><span class=\"line\">#    s 4254b18 11111</span><br><span class=\"line\"># 未剩下任何命令。</span><br><span class=\"line\"># 您在执行将分支 &#39;test&#39; 变基到 &#39;f7de784&#39; 的操作时编辑提交。</span><br><span class=\"line\"># </span><br><span class=\"line\"># 要提交的变更：</span><br><span class=\"line\">#       修改：     README.md</span><br></pre></td></tr></table></figure>\n\n\n\n<h3 id=\"切换到master分支并合并\"><a href=\"#切换到master分支并合并\" class=\"headerlink\" title=\"切换到master分支并合并\"></a>切换到master分支并合并</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">% git checkout master</span><br><span class=\"line\">切换到分支 &#39;master&#39;</span><br><span class=\"line\">您的分支与上游分支 &#39;origin&#x2F;master&#39; 一致。</span><br><span class=\"line\">will@will-Lenovo-ideapad-720S-14IKB &#x2F;tmp&#x2F;test&#x2F;rebaseTest</span><br><span class=\"line\"> % git merge test</span><br><span class=\"line\">更新 f7de784..687e32e</span><br><span class=\"line\">Fast-forward</span><br><span class=\"line\"> README.md | 25 ++-----------------------</span><br><span class=\"line\"> 1 file changed, 2 insertions(+), 23 deletions(-)</span><br></pre></td></tr></table></figure>\n\n\n\n<p>到github上可以看到master分支只有一条合并后的commit信息</p>\n<p><img src=\"https://i.loli.net/2020/02/27/zFCWBo4NQarsmgL.png\" alt=\"rebase.png\"></p>"},{"title":"spark架构","date":"2019-11-02T04:43:16.000Z","_content":"\n# Spark架构\n\n![](https://i.loli.net/2019/11/02/TLMwH9aV1xStu5Y.png)\n\n<!--more-->\n\n一个sparkApplication对应 Driver程序中一个SparkContext，而SparkContext由一系列Spark job组成。一个Worker Node可以由多个Executer组成，但是Executer不能跨Worker Node。\n\n\n\n# spark数据处理系统\n\n Spark可以在仅有单个JVM的单台机器上运行，但更常与分布式存储系统和集群管理器串联组成如下数据处理系统。分布式存储系统用来存放数据，集群管理器用来协调管理集群spark任务。Spark目前支持4种集群管理器：Standalone集群管理器，Apache Mesos，Hadoop YARN，EC2。\n\n![Spark](https://i.loli.net/2019/11/02/5ijlposeYMDScIU.png)\n\n# spark生态系统\n\n![spark生态系统](https://i.loli.net/2019/11/02/B2JjgU7hczEba1r.png)\n\n\n\n","source":"_posts/spark/spark架构.md","raw":"---\ntitle: spark架构\ndate: 2019-11-02 12:43:16\ntags: \n    - Spark\n    - High Performance Spark\n    - spark架构\ncategories: \n    - Spark\n---\n\n# Spark架构\n\n![](https://i.loli.net/2019/11/02/TLMwH9aV1xStu5Y.png)\n\n<!--more-->\n\n一个sparkApplication对应 Driver程序中一个SparkContext，而SparkContext由一系列Spark job组成。一个Worker Node可以由多个Executer组成，但是Executer不能跨Worker Node。\n\n\n\n# spark数据处理系统\n\n Spark可以在仅有单个JVM的单台机器上运行，但更常与分布式存储系统和集群管理器串联组成如下数据处理系统。分布式存储系统用来存放数据，集群管理器用来协调管理集群spark任务。Spark目前支持4种集群管理器：Standalone集群管理器，Apache Mesos，Hadoop YARN，EC2。\n\n![Spark](https://i.loli.net/2019/11/02/5ijlposeYMDScIU.png)\n\n# spark生态系统\n\n![spark生态系统](https://i.loli.net/2019/11/02/B2JjgU7hczEba1r.png)\n\n\n\n","slug":"spark/spark架构","published":1,"updated":"2020-02-19T18:00:56.045Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck7cmmo90000dclzk0sxqgjxv","content":"<h1 id=\"Spark架构\"><a href=\"#Spark架构\" class=\"headerlink\" title=\"Spark架构\"></a>Spark架构</h1><p><img src=\"https://i.loli.net/2019/11/02/TLMwH9aV1xStu5Y.png\" alt=\"\"></p>\n<a id=\"more\"></a>\n\n<p>一个sparkApplication对应 Driver程序中一个SparkContext，而SparkContext由一系列Spark job组成。一个Worker Node可以由多个Executer组成，但是Executer不能跨Worker Node。</p>\n<h1 id=\"spark数据处理系统\"><a href=\"#spark数据处理系统\" class=\"headerlink\" title=\"spark数据处理系统\"></a>spark数据处理系统</h1><p> Spark可以在仅有单个JVM的单台机器上运行，但更常与分布式存储系统和集群管理器串联组成如下数据处理系统。分布式存储系统用来存放数据，集群管理器用来协调管理集群spark任务。Spark目前支持4种集群管理器：Standalone集群管理器，Apache Mesos，Hadoop YARN，EC2。</p>\n<p><img src=\"https://i.loli.net/2019/11/02/5ijlposeYMDScIU.png\" alt=\"Spark\"></p>\n<h1 id=\"spark生态系统\"><a href=\"#spark生态系统\" class=\"headerlink\" title=\"spark生态系统\"></a>spark生态系统</h1><p><img src=\"https://i.loli.net/2019/11/02/B2JjgU7hczEba1r.png\" alt=\"spark生态系统\"></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"Spark架构\"><a href=\"#Spark架构\" class=\"headerlink\" title=\"Spark架构\"></a>Spark架构</h1><p><img src=\"https://i.loli.net/2019/11/02/TLMwH9aV1xStu5Y.png\" alt=\"\"></p>","more":"<p>一个sparkApplication对应 Driver程序中一个SparkContext，而SparkContext由一系列Spark job组成。一个Worker Node可以由多个Executer组成，但是Executer不能跨Worker Node。</p>\n<h1 id=\"spark数据处理系统\"><a href=\"#spark数据处理系统\" class=\"headerlink\" title=\"spark数据处理系统\"></a>spark数据处理系统</h1><p> Spark可以在仅有单个JVM的单台机器上运行，但更常与分布式存储系统和集群管理器串联组成如下数据处理系统。分布式存储系统用来存放数据，集群管理器用来协调管理集群spark任务。Spark目前支持4种集群管理器：Standalone集群管理器，Apache Mesos，Hadoop YARN，EC2。</p>\n<p><img src=\"https://i.loli.net/2019/11/02/5ijlposeYMDScIU.png\" alt=\"Spark\"></p>\n<h1 id=\"spark生态系统\"><a href=\"#spark生态系统\" class=\"headerlink\" title=\"spark生态系统\"></a>spark生态系统</h1><p><img src=\"https://i.loli.net/2019/11/02/B2JjgU7hczEba1r.png\" alt=\"spark生态系统\"></p>"},{"title":"RDD转换","date":"2019-11-02T14:43:50.000Z","_content":"\n\n\n# RDD\n\nRDD(Resilient Distributed Datasets，弹性分布式数据集)，是一个惰性计算、静态类型的分布式数据集合，是Spark实现并发计算的基础数据结构。<!--more-->\n\nRDD包含以下特性（前3个必须有，后两个可选）：\n\n1. partitions()\n\n   返回组成分布式数据集的分区对象数组。\n\n2. itearator(p, parentIters)\n\n   为每个父分区计算分区p的iteartors。\n\n3. dependencies\n\n   返回依赖对象序列。\n\n4. partitioner()---可选\n\n   若RDD有相关元素与分区信息，则返回Scala option type的分区对象。\n\n5. prefferedLocations(p)---可选\n\n   返回数据分区的存储位置信息。\n\n\n\n针对RDD的处理有丰富的操作，包括Action算子和Transformation算子。Action操作返回对象非RDD，Transformation操作返回仍然是RDD。Action算子是触发行动的算子，**Action算子数量等于Spark Job的数量**；Transformation算子会对RDD进行变换，根据父RDD和子RDD的依赖关系不同，Transformation又分为宽依赖和窄依赖。宽依赖的示意图如下：\n\n![image.png](https://i.loli.net/2019/11/03/5EpJ6In4DhlVSxj.png)\n\n窄依赖的严格定义：**each partition of the parrent RDD is used by at most one partition of the child RDD（译：每个父RDD的分区最多被一个子RDD分区使用）**。\n\n这是区分宽窄依赖最严格的定义，还有一个并不是非常严格的说法，但是便于理解：\n\n需要进行shuffle的为宽依赖，不需要的为窄依赖。\n\n**Spark Job中的Stage个数就等于宽依赖个数。**\n\n\n\n常用的Action算子有reduce,collect,count,first,take,aggregate, fold, foreach, saveAsTextFile, countByKey等; 常用的Transformation算子有map, filter, flatmap, mapPartitions, sample, union, intersection, distinct, groupByKey, countByKey, sortBy, join, coalesce, repartition, repartitionAndSortWithinPatitions, mapValues等。\n\n\n\n\n\n# Spark Job阶段划分\n\n![](https://i.loli.net/2019/11/02/Iw4YD79qiK2h6kp.jpg)\n\n","source":"_posts/spark/RDD转换.md","raw":"---\ntitle: RDD转换\ndate: 2019-11-02 22:43:50\ntags:\n    - Spark\n    - High Performance Spark\n    - RDD\ncategories: \n    - Spark\n---\n\n\n\n# RDD\n\nRDD(Resilient Distributed Datasets，弹性分布式数据集)，是一个惰性计算、静态类型的分布式数据集合，是Spark实现并发计算的基础数据结构。<!--more-->\n\nRDD包含以下特性（前3个必须有，后两个可选）：\n\n1. partitions()\n\n   返回组成分布式数据集的分区对象数组。\n\n2. itearator(p, parentIters)\n\n   为每个父分区计算分区p的iteartors。\n\n3. dependencies\n\n   返回依赖对象序列。\n\n4. partitioner()---可选\n\n   若RDD有相关元素与分区信息，则返回Scala option type的分区对象。\n\n5. prefferedLocations(p)---可选\n\n   返回数据分区的存储位置信息。\n\n\n\n针对RDD的处理有丰富的操作，包括Action算子和Transformation算子。Action操作返回对象非RDD，Transformation操作返回仍然是RDD。Action算子是触发行动的算子，**Action算子数量等于Spark Job的数量**；Transformation算子会对RDD进行变换，根据父RDD和子RDD的依赖关系不同，Transformation又分为宽依赖和窄依赖。宽依赖的示意图如下：\n\n![image.png](https://i.loli.net/2019/11/03/5EpJ6In4DhlVSxj.png)\n\n窄依赖的严格定义：**each partition of the parrent RDD is used by at most one partition of the child RDD（译：每个父RDD的分区最多被一个子RDD分区使用）**。\n\n这是区分宽窄依赖最严格的定义，还有一个并不是非常严格的说法，但是便于理解：\n\n需要进行shuffle的为宽依赖，不需要的为窄依赖。\n\n**Spark Job中的Stage个数就等于宽依赖个数。**\n\n\n\n常用的Action算子有reduce,collect,count,first,take,aggregate, fold, foreach, saveAsTextFile, countByKey等; 常用的Transformation算子有map, filter, flatmap, mapPartitions, sample, union, intersection, distinct, groupByKey, countByKey, sortBy, join, coalesce, repartition, repartitionAndSortWithinPatitions, mapValues等。\n\n\n\n\n\n# Spark Job阶段划分\n\n![](https://i.loli.net/2019/11/02/Iw4YD79qiK2h6kp.jpg)\n\n","slug":"spark/RDD转换","published":1,"updated":"2020-02-19T18:00:56.045Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck7cmmo96000hclzkct30h12e","content":"<h1 id=\"RDD\"><a href=\"#RDD\" class=\"headerlink\" title=\"RDD\"></a>RDD</h1><p>RDD(Resilient Distributed Datasets，弹性分布式数据集)，是一个惰性计算、静态类型的分布式数据集合，是Spark实现并发计算的基础数据结构。<a id=\"more\"></a></p>\n<p>RDD包含以下特性（前3个必须有，后两个可选）：</p>\n<ol>\n<li><p>partitions()</p>\n<p>返回组成分布式数据集的分区对象数组。</p>\n</li>\n<li><p>itearator(p, parentIters)</p>\n<p>为每个父分区计算分区p的iteartors。</p>\n</li>\n<li><p>dependencies</p>\n<p>返回依赖对象序列。</p>\n</li>\n<li><p>partitioner()—可选</p>\n<p>若RDD有相关元素与分区信息，则返回Scala option type的分区对象。</p>\n</li>\n<li><p>prefferedLocations(p)—可选</p>\n<p>返回数据分区的存储位置信息。</p>\n</li>\n</ol>\n<p>针对RDD的处理有丰富的操作，包括Action算子和Transformation算子。Action操作返回对象非RDD，Transformation操作返回仍然是RDD。Action算子是触发行动的算子，<strong>Action算子数量等于Spark Job的数量</strong>；Transformation算子会对RDD进行变换，根据父RDD和子RDD的依赖关系不同，Transformation又分为宽依赖和窄依赖。宽依赖的示意图如下：</p>\n<p><img src=\"https://i.loli.net/2019/11/03/5EpJ6In4DhlVSxj.png\" alt=\"image.png\"></p>\n<p>窄依赖的严格定义：<strong>each partition of the parrent RDD is used by at most one partition of the child RDD（译：每个父RDD的分区最多被一个子RDD分区使用）</strong>。</p>\n<p>这是区分宽窄依赖最严格的定义，还有一个并不是非常严格的说法，但是便于理解：</p>\n<p>需要进行shuffle的为宽依赖，不需要的为窄依赖。</p>\n<p><strong>Spark Job中的Stage个数就等于宽依赖个数。</strong></p>\n<p>常用的Action算子有reduce,collect,count,first,take,aggregate, fold, foreach, saveAsTextFile, countByKey等; 常用的Transformation算子有map, filter, flatmap, mapPartitions, sample, union, intersection, distinct, groupByKey, countByKey, sortBy, join, coalesce, repartition, repartitionAndSortWithinPatitions, mapValues等。</p>\n<h1 id=\"Spark-Job阶段划分\"><a href=\"#Spark-Job阶段划分\" class=\"headerlink\" title=\"Spark Job阶段划分\"></a>Spark Job阶段划分</h1><p><img src=\"https://i.loli.net/2019/11/02/Iw4YD79qiK2h6kp.jpg\" alt=\"\"></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"RDD\"><a href=\"#RDD\" class=\"headerlink\" title=\"RDD\"></a>RDD</h1><p>RDD(Resilient Distributed Datasets，弹性分布式数据集)，是一个惰性计算、静态类型的分布式数据集合，是Spark实现并发计算的基础数据结构。","more":"</p>\n<p>RDD包含以下特性（前3个必须有，后两个可选）：</p>\n<ol>\n<li><p>partitions()</p>\n<p>返回组成分布式数据集的分区对象数组。</p>\n</li>\n<li><p>itearator(p, parentIters)</p>\n<p>为每个父分区计算分区p的iteartors。</p>\n</li>\n<li><p>dependencies</p>\n<p>返回依赖对象序列。</p>\n</li>\n<li><p>partitioner()—可选</p>\n<p>若RDD有相关元素与分区信息，则返回Scala option type的分区对象。</p>\n</li>\n<li><p>prefferedLocations(p)—可选</p>\n<p>返回数据分区的存储位置信息。</p>\n</li>\n</ol>\n<p>针对RDD的处理有丰富的操作，包括Action算子和Transformation算子。Action操作返回对象非RDD，Transformation操作返回仍然是RDD。Action算子是触发行动的算子，<strong>Action算子数量等于Spark Job的数量</strong>；Transformation算子会对RDD进行变换，根据父RDD和子RDD的依赖关系不同，Transformation又分为宽依赖和窄依赖。宽依赖的示意图如下：</p>\n<p><img src=\"https://i.loli.net/2019/11/03/5EpJ6In4DhlVSxj.png\" alt=\"image.png\"></p>\n<p>窄依赖的严格定义：<strong>each partition of the parrent RDD is used by at most one partition of the child RDD（译：每个父RDD的分区最多被一个子RDD分区使用）</strong>。</p>\n<p>这是区分宽窄依赖最严格的定义，还有一个并不是非常严格的说法，但是便于理解：</p>\n<p>需要进行shuffle的为宽依赖，不需要的为窄依赖。</p>\n<p><strong>Spark Job中的Stage个数就等于宽依赖个数。</strong></p>\n<p>常用的Action算子有reduce,collect,count,first,take,aggregate, fold, foreach, saveAsTextFile, countByKey等; 常用的Transformation算子有map, filter, flatmap, mapPartitions, sample, union, intersection, distinct, groupByKey, countByKey, sortBy, join, coalesce, repartition, repartitionAndSortWithinPatitions, mapValues等。</p>\n<h1 id=\"Spark-Job阶段划分\"><a href=\"#Spark-Job阶段划分\" class=\"headerlink\" title=\"Spark Job阶段划分\"></a>Spark Job阶段划分</h1><p><img src=\"https://i.loli.net/2019/11/02/Iw4YD79qiK2h6kp.jpg\" alt=\"\"></p>"},{"title":"github个人网站替换自定义域名","date":"2020-02-24T12:29:16.000Z","_content":"\n首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。\ngithub个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。\n<!--more-->\n所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。\n\n\n### 域名申请\n经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。\n接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。\n**这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。**第三次终于申请成功了。\n\n接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。\n\n首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了\n\n![选区_017.png](https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png)\n\n替换后如上图\n\n等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了\n\n![选区_018.png](https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png)\n\n现在再进去添加两条纪录\n\n![选区_020.png](https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png)\n\n上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值\n\n```\nping xxx.github.io\n```\n\n最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。\n\n访问will21.cn，搞定！","source":"_posts/other/github个人网站更改域名.md","raw":"---\ntitle: github个人网站替换自定义域名\ndate: 2020-02-24 20:29:16\ntags: \n    - github个人网站\n    - 域名修改\ncategories: \n    - other\n---\n\n首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。\ngithub个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。\n<!--more-->\n所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。\n\n\n### 域名申请\n经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。\n接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。\n**这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。**第三次终于申请成功了。\n\n接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。\n\n首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了\n\n![选区_017.png](https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png)\n\n替换后如上图\n\n等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了\n\n![选区_018.png](https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png)\n\n现在再进去添加两条纪录\n\n![选区_020.png](https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png)\n\n上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值\n\n```\nping xxx.github.io\n```\n\n最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。\n\n访问will21.cn，搞定！","slug":"other/github个人网站更改域名","published":1,"updated":"2020-02-24T12:34:53.103Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck7cmmo99000jclzkdsujasq0","content":"<p>首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。<br>github个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。</p>\n<a id=\"more\"></a>\n<p>所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。</p>\n<h3 id=\"域名申请\"><a href=\"#域名申请\" class=\"headerlink\" title=\"域名申请\"></a>域名申请</h3><p>经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。<br>接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。<br><strong>这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。</strong>第三次终于申请成功了。</p>\n<p>接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。</p>\n<p>首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了</p>\n<p><img src=\"https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png\" alt=\"选区_017.png\"></p>\n<p>替换后如上图</p>\n<p>等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了</p>\n<p><img src=\"https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png\" alt=\"选区_018.png\"></p>\n<p>现在再进去添加两条纪录</p>\n<p><img src=\"https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png\" alt=\"选区_020.png\"></p>\n<p>上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ping xxx.github.io</span><br></pre></td></tr></table></figure>\n\n<p>最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。</p>\n<p>访问will21.cn，搞定！</p>\n","site":{"data":{}},"excerpt":"<p>首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。<br>github个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。</p>","more":"<p>所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。</p>\n<h3 id=\"域名申请\"><a href=\"#域名申请\" class=\"headerlink\" title=\"域名申请\"></a>域名申请</h3><p>经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。<br>接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。<br><strong>这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。</strong>第三次终于申请成功了。</p>\n<p>接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。</p>\n<p>首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了</p>\n<p><img src=\"https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png\" alt=\"选区_017.png\"></p>\n<p>替换后如上图</p>\n<p>等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了</p>\n<p><img src=\"https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png\" alt=\"选区_018.png\"></p>\n<p>现在再进去添加两条纪录</p>\n<p><img src=\"https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png\" alt=\"选区_020.png\"></p>\n<p>上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ping xxx.github.io</span><br></pre></td></tr></table></figure>\n\n<p>最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。</p>\n<p>访问will21.cn，搞定！</p>"},{"title":"spark---目录","date":"2019-11-02T04:50:06.000Z","top":true,"_content":"\nSpark作为一个开源大数据分析引擎，分布式、计算速度快是其显著的优势，特别适用于机器学习（ML）等计算中需要迭代的算法。下面将主要针对Spark Core部分，分为以下一系列文章进行介绍：\n\n- {% post_link spark/spark Spark绪论%}\n- {% post_link spark/spark架构 Spark架构%}\n- {% post_link spark/RDD转换 RDD转换%}\n- 键值对处理\n\n\n\n> 参考书目：high performance spark, Holden karau & Rachel Warren","source":"_posts/spark/spark目录.md","raw":"---\ntitle: spark---目录\ndate: 2019-11-02 12:50:06\ntags: spark目录\ncategories: \n    - 目录\ntop: true\n---\n\nSpark作为一个开源大数据分析引擎，分布式、计算速度快是其显著的优势，特别适用于机器学习（ML）等计算中需要迭代的算法。下面将主要针对Spark Core部分，分为以下一系列文章进行介绍：\n\n- {% post_link spark/spark Spark绪论%}\n- {% post_link spark/spark架构 Spark架构%}\n- {% post_link spark/RDD转换 RDD转换%}\n- 键值对处理\n\n\n\n> 参考书目：high performance spark, Holden karau & Rachel Warren","slug":"spark/spark目录","published":1,"updated":"2020-02-24T12:41:40.095Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck7cmmo9c000nclzk46p0f6bm","content":"<p>Spark作为一个开源大数据分析引擎，分布式、计算速度快是其显著的优势，特别适用于机器学习（ML）等计算中需要迭代的算法。下面将主要针对Spark Core部分，分为以下一系列文章进行介绍：</p>\n<ul>\n<li><a href=\"/2019/10/24/spark/spark/\" title=\"Spark绪论\">Spark绪论</a></li>\n<li><a href=\"/2019/11/02/spark/spark%E6%9E%B6%E6%9E%84/\" title=\"Spark架构\">Spark架构</a></li>\n<li><a href=\"/2019/11/02/spark/RDD%E8%BD%AC%E6%8D%A2/\" title=\"RDD转换\">RDD转换</a></li>\n<li>键值对处理</li>\n</ul>\n<blockquote>\n<p>参考书目：high performance spark, Holden karau &amp; Rachel Warren</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>Spark作为一个开源大数据分析引擎，分布式、计算速度快是其显著的优势，特别适用于机器学习（ML）等计算中需要迭代的算法。下面将主要针对Spark Core部分，分为以下一系列文章进行介绍：</p>\n<ul>\n<li><a href=\"/2019/10/24/spark/spark/\" title=\"Spark绪论\">Spark绪论</a></li>\n<li><a href=\"/2019/11/02/spark/spark%E6%9E%B6%E6%9E%84/\" title=\"Spark架构\">Spark架构</a></li>\n<li><a href=\"/2019/11/02/spark/RDD%E8%BD%AC%E6%8D%A2/\" title=\"RDD转换\">RDD转换</a></li>\n<li>键值对处理</li>\n</ul>\n<blockquote>\n<p>参考书目：high performance spark, Holden karau &amp; Rachel Warren</p>\n</blockquote>\n"},{"title":"Spark绪论","date":"2019-10-24T08:33:13.000Z","_content":"\n\n# 前言\n\n- 为什么会有spark\n\n  现在的计算机性能越来越高，单台机器处理M级别的数据简直小菜一碟。但是如果数据量更大，达到G、T甚至P、E级别，这时单台机器的性能可能就有点不够了。也许你会反驳，目前世界上最快的超级计算<!--more--> 机“Summit”浮点运算速度峰值可达20亿亿次/秒，这相当快了吧！确实很快，但是它的使用和维护费用都很高呀，一般人或者说公司根本负担不起。所以聪明的人类就发明了大数据工具。虽然单台机器的计算性能可能并不是很好，但是如果多台机器联合起来一起做计算，那么不就可以实现相对廉价的机器实现超大数据规模的计算吗？所以诞生了大数据工具Hadoop。Hadoop基于HDFS和Map-Reduce奠定了大数据领域的霸主地位。随后各种工具层出不穷。Spark针对Hadoop计算中慢的缺点，即每次计算结果都要读写到磁盘导致速度变慢的设计，作了如下改进：\n\n  1. 计算中间结果保存在内存中。Hadoop每次的计算结果都会放回HDFS，这种设计严重影响了计算速度。spark基于内存的设计则改进了这一点。\n  2. 惰性计算。Spark使用有向无环图（DAG）调度机制，遇到action操作算子才进行实际计算，所有就有了优化空间。\n\n  > 可参考对比：https://www.zhihu.com/question/26568496\n\n- Spark是什么\n\n  官方定义：**Apache Spark™** is a unified analytics engine for large-scale data processing。翻译过来就是：用于大规模数据处理的分析引擎。再直白一点就是说：spark是用于快速处理大量数据的工具。\n\n  <img src=\"https://ericfu.me/images/2018/06/spark-banner.png\" width=\"700\" hegiht=\"113\" align=center />\n\n\n\n# 和其他工具对比\n\n> 引用自：https://www.boxuegu.com/news/458.html\n\n\n\n- **Hadoop框架**\n\n  提起大数据，第一个想起的肯定是Hadoop，因为Hadoop是目前世界上应用最广泛的大数据工具，它凭借极高的容错率和极低的硬件价格，在大数据市场上风生水起。Hadoop还是第一个在开源社区上引发高度关注的批处理框架，他提出的Map和Reduce的计算模式简洁而优雅。迄今为止，Hadoop已经成为了一个广阔的生态圈，实现了大量算法和组件。由于Hadoop的计算任务需要在集群的多个节点上多次读写，因此在速度上会稍显劣势，但是其吞吐量也同样是其他框架所不能匹敌的。\n\n\n\n- **Storm框架**\n  与Hadoop的批处理模式不同，Storm采用的是流计算框架，由Twitter开源并且托管在GitHub上。与Hadoop类似的是，Storm也提出了两个计算角色，分别为Spout和Bolt。如果说Hadoop是水桶，只能一桶一桶的去井里扛，那么Storm就是水龙头，只要打开就可以源源不断的出水。Storm支持的语言也比较多，Java、Ruby、Python等语言都能很好的支持。由于Storm是流计算框架，因此使用的是内存，延迟上有极大的优势，但是Storm不会持久化数据。\n\n \n\n- **Samza框架**\n  Smaza也是一种流计算框架，但他目前只支持JVM语言，灵活度上略显不足，并且Samza必须和Kafka共同使用。但是响应的，其也继承了Kafka的低延时、分区、避免回压等优势。对于已经有Hadoop+Kafka工作环境的团队来说，Samza是一个不错的选择，并且Samza在多个团队使用的时候能体现良好的性能。\n\n \n\n- **Spark框架**\n  Spark属于前两种框架形式的集合体，是一种混合式的计算框架。它既有自带的实时流处理工具，也可以和Hadoop集成，代替其中的MapReduce，甚至Spark还可以单独拿出来部署集群，但是还得借助HDFS等分布式存储系统。Spark的强大之处在于其运算速度，与Storm类似，Spark也是基于内存的，并且在内存满负载的时候，硬盘也能运算，运算结果表示，Spark的速度大约为Hadoop的一百倍，并且其成本可能比Hadoop更低。但是Spark目前还没有像Hadoop哪有拥有上万级别的集群，因此现阶段的Spark和Hadoop搭配起来使用更加合适。\n\n \n\n- **Flink框架**\n  Flink也是一种混合式的计算框架，但是在设计初始，Fink的侧重点在于处理流式数据，这与Spark的设计初衷恰恰相反，而在市场需求的驱使下，两者都在朝着更多的兼容性发展。Flink目前不是很成熟，更多情况下Flink还是起到一个借鉴的作用。\n\n\n","source":"_posts/spark/spark.md","raw":"---\ntitle: Spark绪论\ndate: 2019-10-24 16:33:13\ntags: \n    - Spark\n    - High Performance Spark\n    - 大数据工具\ncategories: \n    - Spark\n---\n\n\n# 前言\n\n- 为什么会有spark\n\n  现在的计算机性能越来越高，单台机器处理M级别的数据简直小菜一碟。但是如果数据量更大，达到G、T甚至P、E级别，这时单台机器的性能可能就有点不够了。也许你会反驳，目前世界上最快的超级计算<!--more--> 机“Summit”浮点运算速度峰值可达20亿亿次/秒，这相当快了吧！确实很快，但是它的使用和维护费用都很高呀，一般人或者说公司根本负担不起。所以聪明的人类就发明了大数据工具。虽然单台机器的计算性能可能并不是很好，但是如果多台机器联合起来一起做计算，那么不就可以实现相对廉价的机器实现超大数据规模的计算吗？所以诞生了大数据工具Hadoop。Hadoop基于HDFS和Map-Reduce奠定了大数据领域的霸主地位。随后各种工具层出不穷。Spark针对Hadoop计算中慢的缺点，即每次计算结果都要读写到磁盘导致速度变慢的设计，作了如下改进：\n\n  1. 计算中间结果保存在内存中。Hadoop每次的计算结果都会放回HDFS，这种设计严重影响了计算速度。spark基于内存的设计则改进了这一点。\n  2. 惰性计算。Spark使用有向无环图（DAG）调度机制，遇到action操作算子才进行实际计算，所有就有了优化空间。\n\n  > 可参考对比：https://www.zhihu.com/question/26568496\n\n- Spark是什么\n\n  官方定义：**Apache Spark™** is a unified analytics engine for large-scale data processing。翻译过来就是：用于大规模数据处理的分析引擎。再直白一点就是说：spark是用于快速处理大量数据的工具。\n\n  <img src=\"https://ericfu.me/images/2018/06/spark-banner.png\" width=\"700\" hegiht=\"113\" align=center />\n\n\n\n# 和其他工具对比\n\n> 引用自：https://www.boxuegu.com/news/458.html\n\n\n\n- **Hadoop框架**\n\n  提起大数据，第一个想起的肯定是Hadoop，因为Hadoop是目前世界上应用最广泛的大数据工具，它凭借极高的容错率和极低的硬件价格，在大数据市场上风生水起。Hadoop还是第一个在开源社区上引发高度关注的批处理框架，他提出的Map和Reduce的计算模式简洁而优雅。迄今为止，Hadoop已经成为了一个广阔的生态圈，实现了大量算法和组件。由于Hadoop的计算任务需要在集群的多个节点上多次读写，因此在速度上会稍显劣势，但是其吞吐量也同样是其他框架所不能匹敌的。\n\n\n\n- **Storm框架**\n  与Hadoop的批处理模式不同，Storm采用的是流计算框架，由Twitter开源并且托管在GitHub上。与Hadoop类似的是，Storm也提出了两个计算角色，分别为Spout和Bolt。如果说Hadoop是水桶，只能一桶一桶的去井里扛，那么Storm就是水龙头，只要打开就可以源源不断的出水。Storm支持的语言也比较多，Java、Ruby、Python等语言都能很好的支持。由于Storm是流计算框架，因此使用的是内存，延迟上有极大的优势，但是Storm不会持久化数据。\n\n \n\n- **Samza框架**\n  Smaza也是一种流计算框架，但他目前只支持JVM语言，灵活度上略显不足，并且Samza必须和Kafka共同使用。但是响应的，其也继承了Kafka的低延时、分区、避免回压等优势。对于已经有Hadoop+Kafka工作环境的团队来说，Samza是一个不错的选择，并且Samza在多个团队使用的时候能体现良好的性能。\n\n \n\n- **Spark框架**\n  Spark属于前两种框架形式的集合体，是一种混合式的计算框架。它既有自带的实时流处理工具，也可以和Hadoop集成，代替其中的MapReduce，甚至Spark还可以单独拿出来部署集群，但是还得借助HDFS等分布式存储系统。Spark的强大之处在于其运算速度，与Storm类似，Spark也是基于内存的，并且在内存满负载的时候，硬盘也能运算，运算结果表示，Spark的速度大约为Hadoop的一百倍，并且其成本可能比Hadoop更低。但是Spark目前还没有像Hadoop哪有拥有上万级别的集群，因此现阶段的Spark和Hadoop搭配起来使用更加合适。\n\n \n\n- **Flink框架**\n  Flink也是一种混合式的计算框架，但是在设计初始，Fink的侧重点在于处理流式数据，这与Spark的设计初衷恰恰相反，而在市场需求的驱使下，两者都在朝着更多的兼容性发展。Flink目前不是很成熟，更多情况下Flink还是起到一个借鉴的作用。\n\n\n","slug":"spark/spark","published":1,"updated":"2020-02-19T18:00:56.045Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck7cmmo9d000pclzk5pgj0pf7","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><ul>\n<li><p>为什么会有spark</p>\n<p>现在的计算机性能越来越高，单台机器处理M级别的数据简直小菜一碟。但是如果数据量更大，达到G、T甚至P、E级别，这时单台机器的性能可能就有点不够了。也许你会反驳，目前世界上最快的超级计算<a id=\"more\"></a> 机“Summit”浮点运算速度峰值可达20亿亿次/秒，这相当快了吧！确实很快，但是它的使用和维护费用都很高呀，一般人或者说公司根本负担不起。所以聪明的人类就发明了大数据工具。虽然单台机器的计算性能可能并不是很好，但是如果多台机器联合起来一起做计算，那么不就可以实现相对廉价的机器实现超大数据规模的计算吗？所以诞生了大数据工具Hadoop。Hadoop基于HDFS和Map-Reduce奠定了大数据领域的霸主地位。随后各种工具层出不穷。Spark针对Hadoop计算中慢的缺点，即每次计算结果都要读写到磁盘导致速度变慢的设计，作了如下改进：</p>\n<ol>\n<li>计算中间结果保存在内存中。Hadoop每次的计算结果都会放回HDFS，这种设计严重影响了计算速度。spark基于内存的设计则改进了这一点。</li>\n<li>惰性计算。Spark使用有向无环图（DAG）调度机制，遇到action操作算子才进行实际计算，所有就有了优化空间。</li>\n</ol>\n<blockquote>\n<p>可参考对比：<a href=\"https://www.zhihu.com/question/26568496\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/26568496</a></p>\n</blockquote>\n</li>\n<li><p>Spark是什么</p>\n<p>官方定义：<strong>Apache Spark™</strong> is a unified analytics engine for large-scale data processing。翻译过来就是：用于大规模数据处理的分析引擎。再直白一点就是说：spark是用于快速处理大量数据的工具。</p>\n<img src=\"https://ericfu.me/images/2018/06/spark-banner.png\" width=\"700\" hegiht=\"113\" align=center />\n\n\n\n</li>\n</ul>\n<h1 id=\"和其他工具对比\"><a href=\"#和其他工具对比\" class=\"headerlink\" title=\"和其他工具对比\"></a>和其他工具对比</h1><blockquote>\n<p>引用自：<a href=\"https://www.boxuegu.com/news/458.html\" target=\"_blank\" rel=\"noopener\">https://www.boxuegu.com/news/458.html</a></p>\n</blockquote>\n<ul>\n<li><p><strong>Hadoop框架</strong></p>\n<p>提起大数据，第一个想起的肯定是Hadoop，因为Hadoop是目前世界上应用最广泛的大数据工具，它凭借极高的容错率和极低的硬件价格，在大数据市场上风生水起。Hadoop还是第一个在开源社区上引发高度关注的批处理框架，他提出的Map和Reduce的计算模式简洁而优雅。迄今为止，Hadoop已经成为了一个广阔的生态圈，实现了大量算法和组件。由于Hadoop的计算任务需要在集群的多个节点上多次读写，因此在速度上会稍显劣势，但是其吞吐量也同样是其他框架所不能匹敌的。</p>\n</li>\n</ul>\n<ul>\n<li><strong>Storm框架</strong><br>与Hadoop的批处理模式不同，Storm采用的是流计算框架，由Twitter开源并且托管在GitHub上。与Hadoop类似的是，Storm也提出了两个计算角色，分别为Spout和Bolt。如果说Hadoop是水桶，只能一桶一桶的去井里扛，那么Storm就是水龙头，只要打开就可以源源不断的出水。Storm支持的语言也比较多，Java、Ruby、Python等语言都能很好的支持。由于Storm是流计算框架，因此使用的是内存，延迟上有极大的优势，但是Storm不会持久化数据。</li>\n</ul>\n<ul>\n<li><strong>Samza框架</strong><br>Smaza也是一种流计算框架，但他目前只支持JVM语言，灵活度上略显不足，并且Samza必须和Kafka共同使用。但是响应的，其也继承了Kafka的低延时、分区、避免回压等优势。对于已经有Hadoop+Kafka工作环境的团队来说，Samza是一个不错的选择，并且Samza在多个团队使用的时候能体现良好的性能。</li>\n</ul>\n<ul>\n<li><strong>Spark框架</strong><br>Spark属于前两种框架形式的集合体，是一种混合式的计算框架。它既有自带的实时流处理工具，也可以和Hadoop集成，代替其中的MapReduce，甚至Spark还可以单独拿出来部署集群，但是还得借助HDFS等分布式存储系统。Spark的强大之处在于其运算速度，与Storm类似，Spark也是基于内存的，并且在内存满负载的时候，硬盘也能运算，运算结果表示，Spark的速度大约为Hadoop的一百倍，并且其成本可能比Hadoop更低。但是Spark目前还没有像Hadoop哪有拥有上万级别的集群，因此现阶段的Spark和Hadoop搭配起来使用更加合适。</li>\n</ul>\n<ul>\n<li><strong>Flink框架</strong><br>Flink也是一种混合式的计算框架，但是在设计初始，Fink的侧重点在于处理流式数据，这与Spark的设计初衷恰恰相反，而在市场需求的驱使下，两者都在朝着更多的兼容性发展。Flink目前不是很成熟，更多情况下Flink还是起到一个借鉴的作用。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><ul>\n<li><p>为什么会有spark</p>\n<p>现在的计算机性能越来越高，单台机器处理M级别的数据简直小菜一碟。但是如果数据量更大，达到G、T甚至P、E级别，这时单台机器的性能可能就有点不够了。也许你会反驳，目前世界上最快的超级计算","more":"机“Summit”浮点运算速度峰值可达20亿亿次/秒，这相当快了吧！确实很快，但是它的使用和维护费用都很高呀，一般人或者说公司根本负担不起。所以聪明的人类就发明了大数据工具。虽然单台机器的计算性能可能并不是很好，但是如果多台机器联合起来一起做计算，那么不就可以实现相对廉价的机器实现超大数据规模的计算吗？所以诞生了大数据工具Hadoop。Hadoop基于HDFS和Map-Reduce奠定了大数据领域的霸主地位。随后各种工具层出不穷。Spark针对Hadoop计算中慢的缺点，即每次计算结果都要读写到磁盘导致速度变慢的设计，作了如下改进：</p>\n<ol>\n<li>计算中间结果保存在内存中。Hadoop每次的计算结果都会放回HDFS，这种设计严重影响了计算速度。spark基于内存的设计则改进了这一点。</li>\n<li>惰性计算。Spark使用有向无环图（DAG）调度机制，遇到action操作算子才进行实际计算，所有就有了优化空间。</li>\n</ol>\n<blockquote>\n<p>可参考对比：<a href=\"https://www.zhihu.com/question/26568496\" target=\"_blank\" rel=\"noopener\">https://www.zhihu.com/question/26568496</a></p>\n</blockquote>\n</li>\n<li><p>Spark是什么</p>\n<p>官方定义：<strong>Apache Spark™</strong> is a unified analytics engine for large-scale data processing。翻译过来就是：用于大规模数据处理的分析引擎。再直白一点就是说：spark是用于快速处理大量数据的工具。</p>\n<img src=\"https://ericfu.me/images/2018/06/spark-banner.png\" width=\"700\" hegiht=\"113\" align=center />\n\n\n\n</li>\n</ul>\n<h1 id=\"和其他工具对比\"><a href=\"#和其他工具对比\" class=\"headerlink\" title=\"和其他工具对比\"></a>和其他工具对比</h1><blockquote>\n<p>引用自：<a href=\"https://www.boxuegu.com/news/458.html\" target=\"_blank\" rel=\"noopener\">https://www.boxuegu.com/news/458.html</a></p>\n</blockquote>\n<ul>\n<li><p><strong>Hadoop框架</strong></p>\n<p>提起大数据，第一个想起的肯定是Hadoop，因为Hadoop是目前世界上应用最广泛的大数据工具，它凭借极高的容错率和极低的硬件价格，在大数据市场上风生水起。Hadoop还是第一个在开源社区上引发高度关注的批处理框架，他提出的Map和Reduce的计算模式简洁而优雅。迄今为止，Hadoop已经成为了一个广阔的生态圈，实现了大量算法和组件。由于Hadoop的计算任务需要在集群的多个节点上多次读写，因此在速度上会稍显劣势，但是其吞吐量也同样是其他框架所不能匹敌的。</p>\n</li>\n</ul>\n<ul>\n<li><strong>Storm框架</strong><br>与Hadoop的批处理模式不同，Storm采用的是流计算框架，由Twitter开源并且托管在GitHub上。与Hadoop类似的是，Storm也提出了两个计算角色，分别为Spout和Bolt。如果说Hadoop是水桶，只能一桶一桶的去井里扛，那么Storm就是水龙头，只要打开就可以源源不断的出水。Storm支持的语言也比较多，Java、Ruby、Python等语言都能很好的支持。由于Storm是流计算框架，因此使用的是内存，延迟上有极大的优势，但是Storm不会持久化数据。</li>\n</ul>\n<ul>\n<li><strong>Samza框架</strong><br>Smaza也是一种流计算框架，但他目前只支持JVM语言，灵活度上略显不足，并且Samza必须和Kafka共同使用。但是响应的，其也继承了Kafka的低延时、分区、避免回压等优势。对于已经有Hadoop+Kafka工作环境的团队来说，Samza是一个不错的选择，并且Samza在多个团队使用的时候能体现良好的性能。</li>\n</ul>\n<ul>\n<li><strong>Spark框架</strong><br>Spark属于前两种框架形式的集合体，是一种混合式的计算框架。它既有自带的实时流处理工具，也可以和Hadoop集成，代替其中的MapReduce，甚至Spark还可以单独拿出来部署集群，但是还得借助HDFS等分布式存储系统。Spark的强大之处在于其运算速度，与Storm类似，Spark也是基于内存的，并且在内存满负载的时候，硬盘也能运算，运算结果表示，Spark的速度大约为Hadoop的一百倍，并且其成本可能比Hadoop更低。但是Spark目前还没有像Hadoop哪有拥有上万级别的集群，因此现阶段的Spark和Hadoop搭配起来使用更加合适。</li>\n</ul>\n<ul>\n<li><strong>Flink框架</strong><br>Flink也是一种混合式的计算框架，但是在设计初始，Fink的侧重点在于处理流式数据，这与Spark的设计初衷恰恰相反，而在市场需求的驱使下，两者都在朝着更多的兼容性发展。Flink目前不是很成熟，更多情况下Flink还是起到一个借鉴的作用。</li>\n</ul>"},{"title":"计算机联网","date":"2020-03-05T04:19:21.000Z","_content":"\n本科计算机网络学的不扎实，一直对ip寻址一知半解。最近稍微研究了一下，有了新的认识，小结一下。\n<!--more-->\n搜索引擎是我们经常用到的，就以百度为例。当我们在本机上输入www.baidu.com, 回车，就能在浏览器看到百度的搜索首页。看似一个简单的过程，包括了很多知识。\n\n\n\n\n### 域名解析\nTCP/IP是计算机通信的协议，计算机间是通过IP来进行连接的。IP（仅介绍IPV4，IPV6可以自己了解）的定义是32位来表示的，每个位可以为0或者1。由于二进制不好写且不好记，所以表示的时候8位为一组表示。有过一定计算机基础都知道8位表示为十进制，能表达的范围为0～255（无符号），所以32位IP可以表示为192.168.143.252这样的一串数字，比32位更方便书写和记忆了。但是对于人来说还是不容易记住那么多的IP地址，怎么办？解决办法是用我们容易记住的网址来替换IP，比如www.baidu.com，我们最终还是通过IP来访问百度服务器的。\n\n这个对应关系存在哪里呢？它们存在域名解析服务器上，国际上有很多的域名解析服务器，而且是分等级的，最高级的是根域名解析服务器，各国家或组织来维护，这些可以自己了解。有了域名解析服务器，我们就可以将网址转化为对应的IP了。\n\n\n\n### 子网掩码\n\n32位的IP供全球的人使用，最多可用个数是2的32次方，差不多42亿左右，现在全球人数早就超过了这个数字;而且申请IP不是个人来申请的，而是代理商或组织申请一段连续的IP，这样就更显得不够用了。怎么办？这就涉及到了子网的概念。比如一个小办公楼，如果就分配一个IP，作为统一的通信入口和出口。对于外面的计算机来说，只要负责找到这个入口，其他的事情交给办公楼内部解决。内部可以以代号来表示其中的计算机，这样就可以节省很多IP。这个代号在IP协议里也规定了，也是用IP表示，不过有范围：\n\nA类地址：10.0.0.0 - 10.255.255.255 \nB类地址：172.16.0.0 - 172.31.255.255 C类地址：192.168.0.0 -192.168.255.255 \n\n关于ABC类可以自己了解。这样内部的IP表示也解决了。但是还有一个问题，如何识别一个一个IP是要访问内部还是外部？这就是子网掩码发挥作用的时候了。之前也说了，IP本质上还是按位来表示的，要么0要么1，所以可以按位与，与的定义是如果都是1那么与的结果是1，其他情况都是0。这样与1就表示保持原来的位不变，与0就表示置0。用公式来表示：\n\n1&\\*=\\*\n\n0&\\*=0\n\n默认的子网掩码是255.255.255.0，这个子网掩码的意思是保持32位的前24位不变，后面的置0。如果本机IP和目标IP都和子网掩码做与操作，如果结果相同说明在同一个子网内","source":"_posts/other/计算机网络之联网.md","raw":"---\ntitle: 计算机联网\ndate: 2020-03-05 12:19:21\ntags: \n    - ip寻址\n    - 上网\n    - 计算机网络\ncategories: \n    - other\n---\n\n本科计算机网络学的不扎实，一直对ip寻址一知半解。最近稍微研究了一下，有了新的认识，小结一下。\n<!--more-->\n搜索引擎是我们经常用到的，就以百度为例。当我们在本机上输入www.baidu.com, 回车，就能在浏览器看到百度的搜索首页。看似一个简单的过程，包括了很多知识。\n\n\n\n\n### 域名解析\nTCP/IP是计算机通信的协议，计算机间是通过IP来进行连接的。IP（仅介绍IPV4，IPV6可以自己了解）的定义是32位来表示的，每个位可以为0或者1。由于二进制不好写且不好记，所以表示的时候8位为一组表示。有过一定计算机基础都知道8位表示为十进制，能表达的范围为0～255（无符号），所以32位IP可以表示为192.168.143.252这样的一串数字，比32位更方便书写和记忆了。但是对于人来说还是不容易记住那么多的IP地址，怎么办？解决办法是用我们容易记住的网址来替换IP，比如www.baidu.com，我们最终还是通过IP来访问百度服务器的。\n\n这个对应关系存在哪里呢？它们存在域名解析服务器上，国际上有很多的域名解析服务器，而且是分等级的，最高级的是根域名解析服务器，各国家或组织来维护，这些可以自己了解。有了域名解析服务器，我们就可以将网址转化为对应的IP了。\n\n\n\n### 子网掩码\n\n32位的IP供全球的人使用，最多可用个数是2的32次方，差不多42亿左右，现在全球人数早就超过了这个数字;而且申请IP不是个人来申请的，而是代理商或组织申请一段连续的IP，这样就更显得不够用了。怎么办？这就涉及到了子网的概念。比如一个小办公楼，如果就分配一个IP，作为统一的通信入口和出口。对于外面的计算机来说，只要负责找到这个入口，其他的事情交给办公楼内部解决。内部可以以代号来表示其中的计算机，这样就可以节省很多IP。这个代号在IP协议里也规定了，也是用IP表示，不过有范围：\n\nA类地址：10.0.0.0 - 10.255.255.255 \nB类地址：172.16.0.0 - 172.31.255.255 C类地址：192.168.0.0 -192.168.255.255 \n\n关于ABC类可以自己了解。这样内部的IP表示也解决了。但是还有一个问题，如何识别一个一个IP是要访问内部还是外部？这就是子网掩码发挥作用的时候了。之前也说了，IP本质上还是按位来表示的，要么0要么1，所以可以按位与，与的定义是如果都是1那么与的结果是1，其他情况都是0。这样与1就表示保持原来的位不变，与0就表示置0。用公式来表示：\n\n1&\\*=\\*\n\n0&\\*=0\n\n默认的子网掩码是255.255.255.0，这个子网掩码的意思是保持32位的前24位不变，后面的置0。如果本机IP和目标IP都和子网掩码做与操作，如果结果相同说明在同一个子网内","slug":"other/计算机网络之联网","published":1,"updated":"2020-03-08T07:57:43.266Z","_id":"ck7gdrx470000vazk7e969voy","comments":1,"layout":"post","photos":[],"link":"","content":"<p>本科计算机网络学的不扎实，一直对ip寻址一知半解。最近稍微研究了一下，有了新的认识，小结一下。</p>\n<a id=\"more\"></a>\n<p>搜索引擎是我们经常用到的，就以百度为例。当我们在本机上输入<a href=\"http://www.baidu.com\" target=\"_blank\" rel=\"noopener\">www.baidu.com</a>, 回车，就能在浏览器看到百度的搜索首页。看似一个简单的过程，包括了很多知识。</p>\n<h3 id=\"域名解析\"><a href=\"#域名解析\" class=\"headerlink\" title=\"域名解析\"></a>域名解析</h3><p>TCP/IP是计算机通信的协议，计算机间是通过IP来进行连接的。IP（仅介绍IPV4，IPV6可以自己了解）的定义是32位来表示的，每个位可以为0或者1。由于二进制不好写且不好记，所以表示的时候8位为一组表示。有过一定计算机基础都知道8位表示为十进制，能表达的范围为0～255（无符号），所以32位IP可以表示为192.168.143.252这样的一串数字，比32位更方便书写和记忆了。但是对于人来说还是不容易记住那么多的IP地址，怎么办？解决办法是用我们容易记住的网址来替换IP，比如<a href=\"http://www.baidu.com，我们最终还是通过IP来访问百度服务器的。\" target=\"_blank\" rel=\"noopener\">www.baidu.com，我们最终还是通过IP来访问百度服务器的。</a></p>\n<p>这个对应关系存在哪里呢？它们存在域名解析服务器上，国际上有很多的域名解析服务器，而且是分等级的，最高级的是根域名解析服务器，各国家或组织来维护，这些可以自己了解。有了域名解析服务器，我们就可以将网址转化为对应的IP了。</p>\n<h3 id=\"子网掩码\"><a href=\"#子网掩码\" class=\"headerlink\" title=\"子网掩码\"></a>子网掩码</h3><p>32位的IP供全球的人使用，最多可用个数是2的32次方，差不多42亿左右，现在全球人数早就超过了这个数字;而且申请IP不是个人来申请的，而是代理商或组织申请一段连续的IP，这样就更显得不够用了。怎么办？这就涉及到了子网的概念。比如一个小办公楼，如果就分配一个IP，作为统一的通信入口和出口。对于外面的计算机来说，只要负责找到这个入口，其他的事情交给办公楼内部解决。内部可以以代号来表示其中的计算机，这样就可以节省很多IP。这个代号在IP协议里也规定了，也是用IP表示，不过有范围：</p>\n<p>A类地址：10.0.0.0 - 10.255.255.255<br>B类地址：172.16.0.0 - 172.31.255.255 C类地址：192.168.0.0 -192.168.255.255 </p>\n<p>关于ABC类可以自己了解。这样内部的IP表示也解决了。但是还有一个问题，如何识别一个一个IP是要访问内部还是外部？这就是子网掩码发挥作用的时候了。之前也说了，IP本质上还是按位来表示的，要么0要么1，所以可以按位与，与的定义是如果都是1那么与的结果是1，其他情况都是0。这样与1就表示保持原来的位不变，与0就表示置0。用公式来表示：</p>\n<p>1&amp;*=*</p>\n<p>0&amp;*=0</p>\n<p>默认的子网掩码是255.255.255.0，这个子网掩码的意思是保持32位的前24位不变，后面的置0。如果本机IP和目标IP都和子网掩码做与操作，如果结果相同说明在同一个子网内</p>\n","site":{"data":{}},"excerpt":"<p>本科计算机网络学的不扎实，一直对ip寻址一知半解。最近稍微研究了一下，有了新的认识，小结一下。</p>","more":"<p>搜索引擎是我们经常用到的，就以百度为例。当我们在本机上输入<a href=\"http://www.baidu.com\" target=\"_blank\" rel=\"noopener\">www.baidu.com</a>, 回车，就能在浏览器看到百度的搜索首页。看似一个简单的过程，包括了很多知识。</p>\n<h3 id=\"域名解析\"><a href=\"#域名解析\" class=\"headerlink\" title=\"域名解析\"></a>域名解析</h3><p>TCP/IP是计算机通信的协议，计算机间是通过IP来进行连接的。IP（仅介绍IPV4，IPV6可以自己了解）的定义是32位来表示的，每个位可以为0或者1。由于二进制不好写且不好记，所以表示的时候8位为一组表示。有过一定计算机基础都知道8位表示为十进制，能表达的范围为0～255（无符号），所以32位IP可以表示为192.168.143.252这样的一串数字，比32位更方便书写和记忆了。但是对于人来说还是不容易记住那么多的IP地址，怎么办？解决办法是用我们容易记住的网址来替换IP，比如<a href=\"http://www.baidu.com，我们最终还是通过IP来访问百度服务器的。\" target=\"_blank\" rel=\"noopener\">www.baidu.com，我们最终还是通过IP来访问百度服务器的。</a></p>\n<p>这个对应关系存在哪里呢？它们存在域名解析服务器上，国际上有很多的域名解析服务器，而且是分等级的，最高级的是根域名解析服务器，各国家或组织来维护，这些可以自己了解。有了域名解析服务器，我们就可以将网址转化为对应的IP了。</p>\n<h3 id=\"子网掩码\"><a href=\"#子网掩码\" class=\"headerlink\" title=\"子网掩码\"></a>子网掩码</h3><p>32位的IP供全球的人使用，最多可用个数是2的32次方，差不多42亿左右，现在全球人数早就超过了这个数字;而且申请IP不是个人来申请的，而是代理商或组织申请一段连续的IP，这样就更显得不够用了。怎么办？这就涉及到了子网的概念。比如一个小办公楼，如果就分配一个IP，作为统一的通信入口和出口。对于外面的计算机来说，只要负责找到这个入口，其他的事情交给办公楼内部解决。内部可以以代号来表示其中的计算机，这样就可以节省很多IP。这个代号在IP协议里也规定了，也是用IP表示，不过有范围：</p>\n<p>A类地址：10.0.0.0 - 10.255.255.255<br>B类地址：172.16.0.0 - 172.31.255.255 C类地址：192.168.0.0 -192.168.255.255 </p>\n<p>关于ABC类可以自己了解。这样内部的IP表示也解决了。但是还有一个问题，如何识别一个一个IP是要访问内部还是外部？这就是子网掩码发挥作用的时候了。之前也说了，IP本质上还是按位来表示的，要么0要么1，所以可以按位与，与的定义是如果都是1那么与的结果是1，其他情况都是0。这样与1就表示保持原来的位不变，与0就表示置0。用公式来表示：</p>\n<p>1&amp;*=*</p>\n<p>0&amp;*=0</p>\n<p>默认的子网掩码是255.255.255.0，这个子网掩码的意思是保持32位的前24位不变，后面的置0。如果本机IP和目标IP都和子网掩码做与操作，如果结果相同说明在同一个子网内</p>"},{"title":"文件及用户权限","date":"2020-03-09T16:31:21.000Z","_content":"\n\n\n# 概述\n\n计算机最重要的两大部分：存储和计算。存储分永久性存储（例如文件）和短暂的存储（例如内存）。永久性存储我们接触最多的就是文件了。<!--more-->大多数人都用过word，肯定都有过word没保存，工作白干了这种尴尬的事情，这就是临时的修改没有保存到文件中的缘故。文件作为重要信息载体，安全性、共享性都非常重要。我们是否可以通过linux的文件系统做到以下事情？\n\n1.不同用户对同一文件有不同权限\n\n2.是否可以分配某个权限给一群人\n\n3.为了文件安全性，是否可以设置文件只能添加或者其他权限如不能删除改名\n\n4.只让用户做指定的事情\n\n带着这些问题，我们将分3个部分进行介绍。\n\n\n\n# 基础权限\n\n网上盗一张图：\n\n![image.png](https://i.loli.net/2020/03/12/Nwa34uJdEZFgW2H.png)\n\nlinux列出文件的命令`ls -l`执行一下，得到如下结果\n\n![](https://i.loli.net/2020/03/12/HvxWYgweRpmGf6l.png)\n\n<center>图1</center>\n\n表示当前目录下只有一个test.txt文件。最前面的含义可以用第一张图来解释。第一个数字表示第几个符号\n\n`-rw-rw-r--`\n\n1: -表示文件类型是文件。文件夹用d表示，l表示链接文件。最常用的就是这三个。接下来每3个一组\n\n2: r对于will用户可读（`ll`显示的结果第三列表示所有者用户）\n\n3: w对于will用户可写\n\n4: -对于will用户不可执行\n\n接下来的3个表示对于用户组will可读可写不可执行(`ll`显示的结果第四列表示所在用户组)\n\n最后的3个表示对于其他用户可读不可写不可执行\n\n> 第一个问题就解决了。当以will用户登陆使用linux时，对于test.txt文件就有两个权限读和写; 若非will用户组的其他用户登陆linux时，对于test.txt文件就只有读的权限。\n\n>第二个问题也解决了。如何给一群人一个权限？拉到一个用户组就可以了。\n\n对于文件和文件夹，rwx的权限含义是不一样的，如下表\n\n|      | r            | w            | x            |\n| ---- | ------------ | ------------ | ------------ |\n| 文件 | 读取文件内容 | 修改文件内容 | 执行文件内容 |\n| 目录 | 读到文件名   | 修改文件名   | 进入该目录   |\n\n对于目录权限的理解：若没有x权限，就无法进入该目录;若没有r权限，则文件夹中内容不可见。可以将文件夹理解成一个盒子，x权限相当于我们有了钥匙，但是是在黑夜中打开，看不见里面的内容，r权限就是一道光，照亮盒子，让我们看到里面的小盒子（子文件夹）和小糖果、小文具（文件）。","source":"_posts/linux/linux-文件权限.md","raw":"---\ntitle: 文件及用户权限\ndate: 2020-03-10 00:31:21\ntags:\n    - Linux\n    - 文件\n    - 权限\ncategories: \n    - Linux\n---\n\n\n\n# 概述\n\n计算机最重要的两大部分：存储和计算。存储分永久性存储（例如文件）和短暂的存储（例如内存）。永久性存储我们接触最多的就是文件了。<!--more-->大多数人都用过word，肯定都有过word没保存，工作白干了这种尴尬的事情，这就是临时的修改没有保存到文件中的缘故。文件作为重要信息载体，安全性、共享性都非常重要。我们是否可以通过linux的文件系统做到以下事情？\n\n1.不同用户对同一文件有不同权限\n\n2.是否可以分配某个权限给一群人\n\n3.为了文件安全性，是否可以设置文件只能添加或者其他权限如不能删除改名\n\n4.只让用户做指定的事情\n\n带着这些问题，我们将分3个部分进行介绍。\n\n\n\n# 基础权限\n\n网上盗一张图：\n\n![image.png](https://i.loli.net/2020/03/12/Nwa34uJdEZFgW2H.png)\n\nlinux列出文件的命令`ls -l`执行一下，得到如下结果\n\n![](https://i.loli.net/2020/03/12/HvxWYgweRpmGf6l.png)\n\n<center>图1</center>\n\n表示当前目录下只有一个test.txt文件。最前面的含义可以用第一张图来解释。第一个数字表示第几个符号\n\n`-rw-rw-r--`\n\n1: -表示文件类型是文件。文件夹用d表示，l表示链接文件。最常用的就是这三个。接下来每3个一组\n\n2: r对于will用户可读（`ll`显示的结果第三列表示所有者用户）\n\n3: w对于will用户可写\n\n4: -对于will用户不可执行\n\n接下来的3个表示对于用户组will可读可写不可执行(`ll`显示的结果第四列表示所在用户组)\n\n最后的3个表示对于其他用户可读不可写不可执行\n\n> 第一个问题就解决了。当以will用户登陆使用linux时，对于test.txt文件就有两个权限读和写; 若非will用户组的其他用户登陆linux时，对于test.txt文件就只有读的权限。\n\n>第二个问题也解决了。如何给一群人一个权限？拉到一个用户组就可以了。\n\n对于文件和文件夹，rwx的权限含义是不一样的，如下表\n\n|      | r            | w            | x            |\n| ---- | ------------ | ------------ | ------------ |\n| 文件 | 读取文件内容 | 修改文件内容 | 执行文件内容 |\n| 目录 | 读到文件名   | 修改文件名   | 进入该目录   |\n\n对于目录权限的理解：若没有x权限，就无法进入该目录;若没有r权限，则文件夹中内容不可见。可以将文件夹理解成一个盒子，x权限相当于我们有了钥匙，但是是在黑夜中打开，看不见里面的内容，r权限就是一道光，照亮盒子，让我们看到里面的小盒子（子文件夹）和小糖果、小文具（文件）。","slug":"linux/linux-文件权限","published":1,"updated":"2020-03-17T16:32:02.971Z","_id":"ck7kp9cu80000txzkayechfzf","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h1><p>计算机最重要的两大部分：存储和计算。存储分永久性存储（例如文件）和短暂的存储（例如内存）。永久性存储我们接触最多的就是文件了。<a id=\"more\"></a>大多数人都用过word，肯定都有过word没保存，工作白干了这种尴尬的事情，这就是临时的修改没有保存到文件中的缘故。文件作为重要信息载体，安全性、共享性都非常重要。我们是否可以通过linux的文件系统做到以下事情？</p>\n<p>1.不同用户对同一文件有不同权限</p>\n<p>2.是否可以分配某个权限给一群人</p>\n<p>3.为了文件安全性，是否可以设置文件只能添加或者其他权限如不能删除改名</p>\n<p>4.只让用户做指定的事情</p>\n<p>带着这些问题，我们将分3个部分进行介绍。</p>\n<h1 id=\"基础权限\"><a href=\"#基础权限\" class=\"headerlink\" title=\"基础权限\"></a>基础权限</h1><p>网上盗一张图：</p>\n<p><img src=\"https://i.loli.net/2020/03/12/Nwa34uJdEZFgW2H.png\" alt=\"image.png\"></p>\n<p>linux列出文件的命令<code>ls -l</code>执行一下，得到如下结果</p>\n<p><img src=\"https://i.loli.net/2020/03/12/HvxWYgweRpmGf6l.png\" alt=\"\"></p>\n<center>图1</center>\n\n<p>表示当前目录下只有一个test.txt文件。最前面的含义可以用第一张图来解释。第一个数字表示第几个符号</p>\n<p><code>-rw-rw-r--</code></p>\n<p>1: -表示文件类型是文件。文件夹用d表示，l表示链接文件。最常用的就是这三个。接下来每3个一组</p>\n<p>2: r对于will用户可读（<code>ll</code>显示的结果第三列表示所有者用户）</p>\n<p>3: w对于will用户可写</p>\n<p>4: -对于will用户不可执行</p>\n<p>接下来的3个表示对于用户组will可读可写不可执行(<code>ll</code>显示的结果第四列表示所在用户组)</p>\n<p>最后的3个表示对于其他用户可读不可写不可执行</p>\n<blockquote>\n<p>第一个问题就解决了。当以will用户登陆使用linux时，对于test.txt文件就有两个权限读和写; 若非will用户组的其他用户登陆linux时，对于test.txt文件就只有读的权限。</p>\n</blockquote>\n<blockquote>\n<p>第二个问题也解决了。如何给一群人一个权限？拉到一个用户组就可以了。</p>\n</blockquote>\n<p>对于文件和文件夹，rwx的权限含义是不一样的，如下表</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>r</th>\n<th>w</th>\n<th>x</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>文件</td>\n<td>读取文件内容</td>\n<td>修改文件内容</td>\n<td>执行文件内容</td>\n</tr>\n<tr>\n<td>目录</td>\n<td>读到文件名</td>\n<td>修改文件名</td>\n<td>进入该目录</td>\n</tr>\n</tbody></table>\n<p>对于目录权限的理解：若没有x权限，就无法进入该目录;若没有r权限，则文件夹中内容不可见。可以将文件夹理解成一个盒子，x权限相当于我们有了钥匙，但是是在黑夜中打开，看不见里面的内容，r权限就是一道光，照亮盒子，让我们看到里面的小盒子（子文件夹）和小糖果、小文具（文件）。</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h1><p>计算机最重要的两大部分：存储和计算。存储分永久性存储（例如文件）和短暂的存储（例如内存）。永久性存储我们接触最多的就是文件了。","more":"大多数人都用过word，肯定都有过word没保存，工作白干了这种尴尬的事情，这就是临时的修改没有保存到文件中的缘故。文件作为重要信息载体，安全性、共享性都非常重要。我们是否可以通过linux的文件系统做到以下事情？</p>\n<p>1.不同用户对同一文件有不同权限</p>\n<p>2.是否可以分配某个权限给一群人</p>\n<p>3.为了文件安全性，是否可以设置文件只能添加或者其他权限如不能删除改名</p>\n<p>4.只让用户做指定的事情</p>\n<p>带着这些问题，我们将分3个部分进行介绍。</p>\n<h1 id=\"基础权限\"><a href=\"#基础权限\" class=\"headerlink\" title=\"基础权限\"></a>基础权限</h1><p>网上盗一张图：</p>\n<p><img src=\"https://i.loli.net/2020/03/12/Nwa34uJdEZFgW2H.png\" alt=\"image.png\"></p>\n<p>linux列出文件的命令<code>ls -l</code>执行一下，得到如下结果</p>\n<p><img src=\"https://i.loli.net/2020/03/12/HvxWYgweRpmGf6l.png\" alt=\"\"></p>\n<center>图1</center>\n\n<p>表示当前目录下只有一个test.txt文件。最前面的含义可以用第一张图来解释。第一个数字表示第几个符号</p>\n<p><code>-rw-rw-r--</code></p>\n<p>1: -表示文件类型是文件。文件夹用d表示，l表示链接文件。最常用的就是这三个。接下来每3个一组</p>\n<p>2: r对于will用户可读（<code>ll</code>显示的结果第三列表示所有者用户）</p>\n<p>3: w对于will用户可写</p>\n<p>4: -对于will用户不可执行</p>\n<p>接下来的3个表示对于用户组will可读可写不可执行(<code>ll</code>显示的结果第四列表示所在用户组)</p>\n<p>最后的3个表示对于其他用户可读不可写不可执行</p>\n<blockquote>\n<p>第一个问题就解决了。当以will用户登陆使用linux时，对于test.txt文件就有两个权限读和写; 若非will用户组的其他用户登陆linux时，对于test.txt文件就只有读的权限。</p>\n</blockquote>\n<blockquote>\n<p>第二个问题也解决了。如何给一群人一个权限？拉到一个用户组就可以了。</p>\n</blockquote>\n<p>对于文件和文件夹，rwx的权限含义是不一样的，如下表</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>r</th>\n<th>w</th>\n<th>x</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>文件</td>\n<td>读取文件内容</td>\n<td>修改文件内容</td>\n<td>执行文件内容</td>\n</tr>\n<tr>\n<td>目录</td>\n<td>读到文件名</td>\n<td>修改文件名</td>\n<td>进入该目录</td>\n</tr>\n</tbody></table>\n<p>对于目录权限的理解：若没有x权限，就无法进入该目录;若没有r权限，则文件夹中内容不可见。可以将文件夹理解成一个盒子，x权限相当于我们有了钥匙，但是是在黑夜中打开，看不见里面的内容，r权限就是一道光，照亮盒子，让我们看到里面的小盒子（子文件夹）和小糖果、小文具（文件）。</p>"}],"PostAsset":[],"PostCategory":[{"post_id":"ck7cmmo8q0003clzkf3p196cg","category_id":"ck7cmmo8u0005clzkbilzgd03","_id":"ck7cmmo95000eclzke6z01a1d"},{"post_id":"ck7cmmo8y0009clzkefw42vl3","category_id":"ck7cmmo8u0005clzkbilzgd03","_id":"ck7cmmo97000iclzk4e4d4ayh"},{"post_id":"ck7cmmo8s0004clzkgmbj2cvt","category_id":"ck7cmmo8y000aclzk56gfcbk3","_id":"ck7cmmo9a000kclzk4jpgfcj9"},{"post_id":"ck7cmmo8w0007clzkdi6jc0xq","category_id":"ck7cmmo96000fclzk3tqbdowf","_id":"ck7cmmo9e000rclzkap4re7xn"},{"post_id":"ck7cmmo9c000nclzk46p0f6bm","category_id":"ck7cmmo8y000aclzk56gfcbk3","_id":"ck7cmmo9f000vclzk7gzt5546"},{"post_id":"ck7cmmo8x0008clzk57cahwwp","category_id":"ck7cmmo9a000lclzk0t313ahl","_id":"ck7cmmo9g000xclzkflwu5752"},{"post_id":"ck7cmmo8z000cclzk7dgcafdk","category_id":"ck7cmmo9e000sclzk1bvw6oiv","_id":"ck7cmmo9g0010clzk7bth28p1"},{"post_id":"ck7cmmo90000dclzk0sxqgjxv","category_id":"ck7cmmo9g000yclzkh6miada4","_id":"ck7cmmo9i0014clzka1ti90mz"},{"post_id":"ck7cmmo96000hclzkct30h12e","category_id":"ck7cmmo9g000yclzkh6miada4","_id":"ck7cmmo9j0018clzk16nlhbz8"},{"post_id":"ck7cmmo99000jclzkdsujasq0","category_id":"ck7cmmo9e000sclzk1bvw6oiv","_id":"ck7cmmo9k001cclzkc3fag6b0"},{"post_id":"ck7cmmo9d000pclzk5pgj0pf7","category_id":"ck7cmmo9g000yclzkh6miada4","_id":"ck7cmmo9k001fclzkfrte4493"},{"post_id":"ck7gdrx470000vazk7e969voy","category_id":"ck7cmmo9e000sclzk1bvw6oiv","_id":"ck7gdrx4n0002vazkcn70bzjx"},{"post_id":"ck7kp9cu80000txzkayechfzf","category_id":"ck7cmmo9a000lclzk0t313ahl","_id":"ck7o9bqrq0002kgzkdgut9dlc"}],"PostTag":[{"post_id":"ck7cmmo8q0003clzkf3p196cg","tag_id":"ck7cmmo8v0006clzk4ml3gqj4","_id":"ck7cmmo9d000oclzke9e98wwv"},{"post_id":"ck7cmmo8q0003clzkf3p196cg","tag_id":"ck7cmmo8z000bclzkdsk46uh0","_id":"ck7cmmo9e000qclzked5s5h1d"},{"post_id":"ck7cmmo8q0003clzkf3p196cg","tag_id":"ck7cmmo96000gclzk1gfs9xbe","_id":"ck7cmmo9f000uclzkg9pyh3nv"},{"post_id":"ck7cmmo8s0004clzkgmbj2cvt","tag_id":"ck7cmmo9b000mclzk8ezcgic0","_id":"ck7cmmo9f000wclzk98ql0jpi"},{"post_id":"ck7cmmo8w0007clzkdi6jc0xq","tag_id":"ck7cmmo9f000tclzkew102is5","_id":"ck7cmmo9j0016clzk6ihm5g8o"},{"post_id":"ck7cmmo8w0007clzkdi6jc0xq","tag_id":"ck7cmmo9g000zclzk525e2sr7","_id":"ck7cmmo9j0019clzk1q0bhcqo"},{"post_id":"ck7cmmo8w0007clzkdi6jc0xq","tag_id":"ck7cmmo9g0012clzk3p7kee91","_id":"ck7cmmo9k001bclzkde03aw2b"},{"post_id":"ck7cmmo8x0008clzk57cahwwp","tag_id":"ck7cmmo9i0015clzkcn9shvcq","_id":"ck7cmmo9k001eclzk53xe0sye"},{"post_id":"ck7cmmo8x0008clzk57cahwwp","tag_id":"ck7cmmo9j001aclzk14q438aj","_id":"ck7cmmo9k001gclzk0fe934xp"},{"post_id":"ck7cmmo8y0009clzkefw42vl3","tag_id":"ck7cmmo8v0006clzk4ml3gqj4","_id":"ck7cmmo9l001jclzk3hsudpkr"},{"post_id":"ck7cmmo8y0009clzkefw42vl3","tag_id":"ck7cmmo9k001dclzk0vh3gzgj","_id":"ck7cmmo9l001kclzk221l5mps"},{"post_id":"ck7cmmo8y0009clzkefw42vl3","tag_id":"ck7cmmo9k001hclzkhyj9fyys","_id":"ck7cmmo9l001mclzkd37xdkay"},{"post_id":"ck7cmmo8z000cclzk7dgcafdk","tag_id":"ck7cmmo9l001iclzkd0jeeg0o","_id":"ck7cmmo9m001pclzk211l1vga"},{"post_id":"ck7cmmo8z000cclzk7dgcafdk","tag_id":"ck7cmmo9l001lclzk7sxu5qjh","_id":"ck7cmmo9m001qclzkfxita45t"},{"post_id":"ck7cmmo8z000cclzk7dgcafdk","tag_id":"ck7cmmo9l001nclzk6tbtbefl","_id":"ck7cmmo9o001sclzk8v87be9b"},{"post_id":"ck7cmmo90000dclzk0sxqgjxv","tag_id":"ck7cmmo9m001oclzk0q616oyw","_id":"ck7cmmo9q001vclzkh0g1ara9"},{"post_id":"ck7cmmo90000dclzk0sxqgjxv","tag_id":"ck7cmmo9m001rclzk49ckb9iw","_id":"ck7cmmo9q001wclzk0703fes0"},{"post_id":"ck7cmmo90000dclzk0sxqgjxv","tag_id":"ck7cmmo9p001tclzk7es9bfe8","_id":"ck7cmmo9q001yclzk80tg8cvf"},{"post_id":"ck7cmmo96000hclzkct30h12e","tag_id":"ck7cmmo9m001oclzk0q616oyw","_id":"ck7cmmo9r0021clzk0k40fk09"},{"post_id":"ck7cmmo96000hclzkct30h12e","tag_id":"ck7cmmo9m001rclzk49ckb9iw","_id":"ck7cmmo9s0022clzkdjuihazl"},{"post_id":"ck7cmmo96000hclzkct30h12e","tag_id":"ck7cmmo9r001zclzk2017226l","_id":"ck7cmmo9s0024clzk6c1mf6xt"},{"post_id":"ck7cmmo99000jclzkdsujasq0","tag_id":"ck7cmmo9r0020clzk8dg44i5d","_id":"ck7cmmo9t0026clzk0u523j2s"},{"post_id":"ck7cmmo99000jclzkdsujasq0","tag_id":"ck7cmmo9s0023clzk1guvaq0f","_id":"ck7cmmo9t0027clzk9yuc7qs4"},{"post_id":"ck7cmmo9c000nclzk46p0f6bm","tag_id":"ck7cmmo9s0025clzk8rv0bwqr","_id":"ck7cmmo9t0029clzk1z9ge10w"},{"post_id":"ck7cmmo9d000pclzk5pgj0pf7","tag_id":"ck7cmmo9m001oclzk0q616oyw","_id":"ck7cmmo9v002cclzk6cjddu83"},{"post_id":"ck7cmmo9d000pclzk5pgj0pf7","tag_id":"ck7cmmo9m001rclzk49ckb9iw","_id":"ck7cmmo9v002dclzkephl0yml"},{"post_id":"ck7cmmo9d000pclzk5pgj0pf7","tag_id":"ck7cmmo9u002bclzk0npr2r5s","_id":"ck7cmmo9v002eclzk8pia2k75"},{"post_id":"ck7gdrx470000vazk7e969voy","tag_id":"ck7gdrx4g0001vazk9aosf2ft","_id":"ck7gdrx4p0005vazkbc9c95sw"},{"post_id":"ck7gdrx470000vazk7e969voy","tag_id":"ck7gdrx4n0003vazkgjrfe5cb","_id":"ck7gdrx4p0006vazkbtd0gn06"},{"post_id":"ck7gdrx470000vazk7e969voy","tag_id":"ck7gdrx4o0004vazkengy4flm","_id":"ck7gdrx4p0007vazk592j1dl1"},{"post_id":"ck7kp9cu80000txzkayechfzf","tag_id":"ck7cmmo9i0015clzkcn9shvcq","_id":"ck7o9bqrp0000kgzk9r7ehs82"},{"post_id":"ck7kp9cu80000txzkayechfzf","tag_id":"ck7kp9cuf0001txzkddded3mr","_id":"ck7o9bqrq0001kgzk5oicbr27"},{"post_id":"ck7kp9cu80000txzkayechfzf","tag_id":"ck7kp9cuj0003txzkd8a17te0","_id":"ck7o9bqrq0003kgzk71w71z36"}],"Tag":[{"name":"hive","_id":"ck7cmmo8v0006clzk4ml3gqj4"},{"name":"源码","_id":"ck7cmmo8z000bclzkdsk46uh0"},{"name":"开发","_id":"ck7cmmo96000gclzk1gfs9xbe"},{"name":"linux目录","_id":"ck7cmmo9b000mclzk8ezcgic0"},{"name":"hadoop","_id":"ck7cmmo9f000tclzkew102is5"},{"name":"hadoop安装","_id":"ck7cmmo9g000zclzk525e2sr7"},{"name":"windows 10","_id":"ck7cmmo9g0012clzk3p7kee91"},{"name":"Linux","_id":"ck7cmmo9i0015clzkcn9shvcq"},{"name":"基础知识","_id":"ck7cmmo9j001aclzk14q438aj"},{"name":"udf","_id":"ck7cmmo9k001dclzk0vh3gzgj"},{"name":"udaf","_id":"ck7cmmo9k001hclzkhyj9fyys"},{"name":"Git","_id":"ck7cmmo9l001iclzkd0jeeg0o"},{"name":"rebase","_id":"ck7cmmo9l001lclzk7sxu5qjh"},{"name":"commits合并","_id":"ck7cmmo9l001nclzk6tbtbefl"},{"name":"Spark","_id":"ck7cmmo9m001oclzk0q616oyw"},{"name":"High Performance Spark","_id":"ck7cmmo9m001rclzk49ckb9iw"},{"name":"spark架构","_id":"ck7cmmo9p001tclzk7es9bfe8"},{"name":"RDD","_id":"ck7cmmo9r001zclzk2017226l"},{"name":"github个人网站","_id":"ck7cmmo9r0020clzk8dg44i5d"},{"name":"域名修改","_id":"ck7cmmo9s0023clzk1guvaq0f"},{"name":"spark目录","_id":"ck7cmmo9s0025clzk8rv0bwqr"},{"name":"大数据工具","_id":"ck7cmmo9u002bclzk0npr2r5s"},{"name":"ip寻址","_id":"ck7gdrx4g0001vazk9aosf2ft"},{"name":"上网","_id":"ck7gdrx4n0003vazkgjrfe5cb"},{"name":"计算机网络","_id":"ck7gdrx4o0004vazkengy4flm"},{"name":"文件","_id":"ck7kp9cuf0001txzkddded3mr"},{"name":"权限","_id":"ck7kp9cuj0003txzkd8a17te0"}]}}